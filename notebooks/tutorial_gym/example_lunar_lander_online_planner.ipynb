{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment and Dependencies\n",
    "Import required libraries including NumPy, sklearn with Intel extension, and pi_optimal utilities. Configure warning suppression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Environment and Dependencies\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "#from sklearnex import patch_sklearn\n",
    "import warnings\n",
    "\n",
    "# Change directory to the parent directory\n",
    "os.chdir(\"..\")\n",
    "\n",
    "# Apply Intel extension to sklearn\n",
    "#patch_sklearn()\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import pi_optimal utilities\n",
    "from pi_optimal.utils.data_generators.gym_data_generator import GymDataGenerator\n",
    "from pi_optimal.datasets.timeseries_dataset import TimeseriesDataset\n",
    "from pi_optimal.models.sklearn.random_forest_model import RandomForest\n",
    "from pi_optimal.models.sklearn.mlp import NeuralNetwork\n",
    "from pi_optimal.evaluators.base_evaluator import BaseEvaluator\n",
    "from pi_optimal.evaluators.plotting import plot_n_step_evaluation, plot_n_step_episode_rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Gym Data Generator\n",
    "Initialize GymDataGenerator with LunarLander environment and collect training and test data with specified parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 49769.26it/s]\n",
      "Collecting steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 48232.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create Gym Data Generator\n",
    "\n",
    "# Initialize GymDataGenerator with LunarLander environment\n",
    "data_collector = GymDataGenerator(env_name=\"LunarLander-v3\")\n",
    "\n",
    "# Collect training data\n",
    "df_train = data_collector.collect(n_steps=10000, max_steps_per_episode=200, env_seed=None, action_seed=None)\n",
    "df_test = data_collector.collect(n_steps=5000, max_steps_per_episode=200, env_seed=None, action_seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Dataset Parameters\n",
    "Set up dataset configuration dictionary defining features, processors, and evaluation metrics for states, actions, and rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Dataset Parameters\n",
    "\n",
    "# Define dataset configuration dictionary\n",
    "dataset_config = {\n",
    "    \"episode_column\": \"episode\",\n",
    "    \"timestep_column\": \"step\",\n",
    "    \"states\": {\n",
    "        0: {\"name\": \"state_0\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        1: {\"name\": \"state_1\", \"type\": \"numerical\", \"processor\": {\"name\": \"RobustScaler\", \"params\": {\"quantile_range\": (5.0, 95.0)}}, \"evaluation_metric\": \"mae\"},\n",
    "        2: {\"name\": \"state_2\", \"type\": \"numerical\", \"processor\": {\"name\": \"RobustScaler\", \"params\": {\"quantile_range\": (5.0, 95.0)}}, \"evaluation_metric\": \"mae\"},\n",
    "        3: {\"name\": \"state_3\", \"type\": \"numerical\", \"processor\": {\"name\": \"RobustScaler\", \"params\": {\"quantile_range\": (5.0, 95.0)}}, \"evaluation_metric\": \"mae\"},\n",
    "        4: {\"name\": \"state_4\", \"type\": \"numerical\", \"processor\": {\"name\": \"RobustScaler\", \"params\": {\"quantile_range\": (5.0, 95.0)}}, \"evaluation_metric\": \"mae\"},\n",
    "        5: {\"name\": \"state_5\", \"type\": \"numerical\", \"processor\": {\"name\": \"RobustScaler\", \"params\": {\"quantile_range\": (5.0, 95.0)}}, \"evaluation_metric\": \"mae\"},\n",
    "        6: {\"name\": \"state_6\", \"type\": \"binary\", \"processor\": None, \"evaluation_metric\": \"f1_binary\"},\n",
    "        7: {\"name\": \"state_7\", \"type\": \"binary\", \"processor\": None, \"evaluation_metric\": \"f1_binary\"},\n",
    "        8: {\"name\": \"done\", \"type\": \"binary\", \"processor\": None, \"evaluation_metric\": \"f1_binary\"},\n",
    "        9: {\"name\": \"reward\", \"type\": \"numerical\", \"processor\": {\"name\": \"RobustScaler\", \"params\": {\"quantile_range\": (5.0, 95.0)}}, \"evaluation_metric\": \"mae\"},\n",
    "    },\n",
    "    \"actions\": {\n",
    "        0: {\"name\": \"action_0\", \"type\": \"categorial\", \"processor\": {\"name\": \"OneHotEncoder\"}},\n",
    "    },\n",
    "    \"reward_feature_idx\": 9,\n",
    "    \"reward_vector_idx\": 9,\n",
    "    \"reward_column\": \"reward\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training and Test Datasets\n",
    "Initialize TimeseriesDataset objects with collected data, applying the configuration and setting lookback/forecast windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id='logger-container'>\n",
       "            <div class=\"logger-entry\" data-indent=\"0\" style=\"margin-left: 0px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚öôÔ∏è</span>\n",
       "                <span class=\"logger-message\">Initializing new dataset...</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">üìù</span>\n",
       "                <span class=\"logger-message\">Dataset has 10000 rows and 13 columns.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">üìù</span>\n",
       "                <span class=\"logger-message\">Dataset has 109 episodes.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">üìù</span>\n",
       "                <span class=\"logger-message\">Dataset has 10 state features and 1 actions.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚öôÔ∏è</span>\n",
       "                <span class=\"logger-message\">Fitting feature processors...</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Processors created and fitted</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚öôÔ∏è</span>\n",
       "                <span class=\"logger-message\">Transforming features...</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_0' using preprocessor 'StandardScaler() with mean -0.01 and std 0.27'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_1' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_2' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_3' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_4' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_5' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_6' using preprocessor 'None '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_7' using preprocessor 'None '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'done' using preprocessor 'None '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'reward' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed actions feature 'action_0' using preprocessor 'OneHotEncoder(sparse_output=False) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"0\" style=\"margin-left: 0px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚ú®</span>\n",
       "                <span class=\"logger-message\">Dataset was created successfully!</span>\n",
       "            </div>\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            /* Logger Container */\n",
       "            #logger-container {\n",
       "                max-height: 600px;\n",
       "                overflow-y: auto;\n",
       "                font-family: 'Segoe UI Emoji', 'Segoe UI Symbol', monospace;\n",
       "                padding: 5px;\n",
       "                background-color: #ffffff;\n",
       "                font-size: 0.9em; /* Increased font size */\n",
       "                position: relative;\n",
       "            }\n",
       "\n",
       "            /* Individual Log Entry */\n",
       "            .logger-entry {\n",
       "                display: flex;\n",
       "                align-items: center;\n",
       "                padding: 1px 5px;\n",
       "                margin-bottom: 4px;\n",
       "                background-color: #ffffff;\n",
       "                transition: background-color 0.2s, box-shadow 0.2s;\n",
       "                white-space: pre-wrap; /* Preserve whitespace for symbols */\n",
       "                position: relative;\n",
       "            }\n",
       "\n",
       "            .logger-entry:hover {\n",
       "                background-color: #f1f3f5;\n",
       "                box-shadow: 0 2px 6px rgba(0,0,0,0.1);\n",
       "            }\n",
       "\n",
       "        \n",
       "\n",
       "            /* Connector Line styling based on indent */\n",
       "            .logger-entry[data-indent=\"1\"]::before {\n",
       "                left: 0;\n",
       "            }\n",
       "            .logger-entry[data-indent=\"2\"]::before {\n",
       "                left: 0;\n",
       "            }\n",
       "            /* Add more as needed for higher indent levels */\n",
       "\n",
       "            /* Emoji */\n",
       "            .logger-emoji {\n",
       "                margin-right: 10px;\n",
       "                font-size: 1em; /* Larger emoji size */\n",
       "                flex-shrink: 0;\n",
       "                color: inherit; /* Inherit color from parent */\n",
       "            }\n",
       "\n",
       "            /* Message */\n",
       "            .logger-message {\n",
       "                flex-grow: 1;\n",
       "                color: #212529;\n",
       "                word-break: break-word;\n",
       "            }\n",
       "        </style>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id='logger-container'>\n",
       "            <div class=\"logger-entry\" data-indent=\"0\" style=\"margin-left: 0px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚öôÔ∏è</span>\n",
       "                <span class=\"logger-message\">Initializing new dataset...</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">üìù</span>\n",
       "                <span class=\"logger-message\">Dataset has 5000 rows and 13 columns.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">üìù</span>\n",
       "                <span class=\"logger-message\">Dataset has 53 episodes.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">üìù</span>\n",
       "                <span class=\"logger-message\">Dataset has 10 state features and 1 actions.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">üìù</span>\n",
       "                <span class=\"logger-message\">Using processors provided in the dataset_configuration.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚öôÔ∏è</span>\n",
       "                <span class=\"logger-message\">Transforming features...</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_0' using preprocessor 'StandardScaler() with mean -0.01 and std 0.27'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_1' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_2' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_3' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_4' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_5' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_6' using preprocessor 'None '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_7' using preprocessor 'None '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'done' using preprocessor 'None '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'reward' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed actions feature 'action_0' using preprocessor 'OneHotEncoder(sparse_output=False) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"0\" style=\"margin-left: 0px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚ú®</span>\n",
       "                <span class=\"logger-message\">Dataset was created successfully!</span>\n",
       "            </div>\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            /* Logger Container */\n",
       "            #logger-container {\n",
       "                max-height: 600px;\n",
       "                overflow-y: auto;\n",
       "                font-family: 'Segoe UI Emoji', 'Segoe UI Symbol', monospace;\n",
       "                padding: 5px;\n",
       "                background-color: #ffffff;\n",
       "                font-size: 0.9em; /* Increased font size */\n",
       "                position: relative;\n",
       "            }\n",
       "\n",
       "            /* Individual Log Entry */\n",
       "            .logger-entry {\n",
       "                display: flex;\n",
       "                align-items: center;\n",
       "                padding: 1px 5px;\n",
       "                margin-bottom: 4px;\n",
       "                background-color: #ffffff;\n",
       "                transition: background-color 0.2s, box-shadow 0.2s;\n",
       "                white-space: pre-wrap; /* Preserve whitespace for symbols */\n",
       "                position: relative;\n",
       "            }\n",
       "\n",
       "            .logger-entry:hover {\n",
       "                background-color: #f1f3f5;\n",
       "                box-shadow: 0 2px 6px rgba(0,0,0,0.1);\n",
       "            }\n",
       "\n",
       "        \n",
       "\n",
       "            /* Connector Line styling based on indent */\n",
       "            .logger-entry[data-indent=\"1\"]::before {\n",
       "                left: 0;\n",
       "            }\n",
       "            .logger-entry[data-indent=\"2\"]::before {\n",
       "                left: 0;\n",
       "            }\n",
       "            /* Add more as needed for higher indent levels */\n",
       "\n",
       "            /* Emoji */\n",
       "            .logger-emoji {\n",
       "                margin-right: 10px;\n",
       "                font-size: 1em; /* Larger emoji size */\n",
       "                flex-shrink: 0;\n",
       "                color: inherit; /* Inherit color from parent */\n",
       "            }\n",
       "\n",
       "            /* Message */\n",
       "            .logger-message {\n",
       "                flex-grow: 1;\n",
       "                color: #212529;\n",
       "                word-break: break-word;\n",
       "            }\n",
       "        </style>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Training and Test Datasets\n",
    "\n",
    "# Define lookback and forecast timesteps\n",
    "LOOKBACK_TIMESTEPS = 10\n",
    "FORECAST_TIMESTEPS = 1\n",
    "\n",
    "# Initialize TimeseriesDataset objects for training and test data\n",
    "dataset_train = TimeseriesDataset(\n",
    "    df=df_train,\n",
    "    dataset_config=dataset_config,\n",
    "    lookback_timesteps=LOOKBACK_TIMESTEPS,\n",
    "    forecast_timesteps=FORECAST_TIMESTEPS,\n",
    "    train_processors=True\n",
    ")\n",
    "\n",
    "\n",
    "dataset_test = TimeseriesDataset(\n",
    "    df=df_test,\n",
    "    dataset_config=dataset_config,\n",
    "    lookback_timesteps=LOOKBACK_TIMESTEPS,\n",
    "    forecast_timesteps=FORECAST_TIMESTEPS,\n",
    "    train_processors=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Network Model\n",
    "Create and train a Neural Network model with specified hyperparameters on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5bcdd3468a416e846df5162e459add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training models...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Initialize Neural Network model with specified hyperparameters\n",
    "nn_model1 = NeuralNetwork(params={\n",
    "    \"hidden_layer_sizes\": (128, 128),\n",
    "    \"alpha\": 0.01,\n",
    "    \"learning_rate_init\": 0.001,}\n",
    ")\n",
    "    \n",
    "# Train the Neural Network model on the first training dataset\n",
    "nn_model1.fit(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f6c3fced9b483c9779ed8cae42b221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training models...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Initialize Neural Network model with specified hyperparameters\n",
    "nn_model2 = NeuralNetwork(params={\n",
    "    \"hidden_layer_sizes\": (128, 128),\n",
    "    \"alpha\": 0.01,\n",
    "    \"learning_rate_init\": 0.001,}\n",
    ")\n",
    "    \n",
    "# Train the Neural Network model on the first training dataset\n",
    "nn_model2.fit(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373506bf27e54a32b2eecd6bcebb2531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training models...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Initialize Neural Network model with specified hyperparameters\n",
    "nn_model3 = NeuralNetwork(params={\n",
    "    \"hidden_layer_sizes\": (128, 128),\n",
    "    \"alpha\": 0.01,\n",
    "    \"learning_rate_init\": 0.001}\n",
    ")\n",
    "    \n",
    "# Train the Neural Network model on the first training dataset\n",
    "nn_model3.fit(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea9ee2e1076485e8cdd06d0459ed2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training models...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Initialize Neural Network model with specified hyperparameters\n",
    "nn_model4 = NeuralNetwork(params={\n",
    "    \"hidden_layer_sizes\": (128, 128),\n",
    "    \"alpha\": 0.01,\n",
    "    \"learning_rate_init\": 0.001}\n",
    ")\n",
    "    \n",
    "# Train the Neural Network model on the first training dataset\n",
    "nn_model4.fit(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4ef67634b0454a84fbc81236adc0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training models...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Initialize Neural Network model with specified hyperparameters\n",
    "nn_model5 = NeuralNetwork(params={\n",
    "    \"hidden_layer_sizes\": (128, 128),\n",
    "    \"alpha\": 0.01,\n",
    "    \"learning_rate_init\": 0.001}\n",
    ")\n",
    "    \n",
    "# Train the Neural Network model on the first training dataset\n",
    "nn_model5.fit(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e74848854c50447c83fa0ed81086ebdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training models...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Initialize Neural Network model with specified hyperparameters\n",
    "nn_model6 = NeuralNetwork(params={\n",
    "    \"hidden_layer_sizes\": (128, 128),\n",
    "    \"alpha\": 0.01,\n",
    "    \"learning_rate_init\": 0.001}\n",
    ")\n",
    "    \n",
    "# Train the Neural Network model on the first training dataset\n",
    "nn_model6.fit(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02709b35df54488c8647ecc63e72111a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training models...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Initialize Neural Network model with specified hyperparameters\n",
    "nn_model7 = NeuralNetwork(params={\n",
    "    \"hidden_layer_sizes\": (128, 128),\n",
    "    \"alpha\": 0.01,\n",
    "    \"learning_rate_init\": 0.001}\n",
    ")\n",
    "    \n",
    "# Train the Neural Network model on the first training dataset\n",
    "nn_model7.fit(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0092651c9dd415aa191ca7cb0bb4bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training models...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Initialize Neural Network model with specified hyperparameters\n",
    "nn_model8 = NeuralNetwork(params={\n",
    "    \"hidden_layer_sizes\": (128, 128),\n",
    "    \"alpha\": 0.01,\n",
    "    \"learning_rate_init\": 0.001}\n",
    ")\n",
    "    \n",
    "# Train the Neural Network model on the first training dataset\n",
    "nn_model8.fit(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3fa421442f4711ba1fdd7ba239fedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training models...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Initialize Neural Network model with specified hyperparameters\n",
    "nn_model9 = NeuralNetwork(params={\n",
    "    \"hidden_layer_sizes\": (128, 128),\n",
    "    \"alpha\": 0.01,\n",
    "    \"learning_rate_init\": 0.001}\n",
    ")\n",
    "    \n",
    "# Train the Neural Network model on the first training dataset\n",
    "nn_model9.fit(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f61d1a5981f43f6941dccb99b03e187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training models...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Initialize Neural Network model with specified hyperparameters\n",
    "nn_model10 = NeuralNetwork(params={\n",
    "    \"hidden_layer_sizes\": (128, 128),\n",
    "    \"alpha\": 0.01,\n",
    "    \"learning_rate_init\": 0.001}\n",
    ")\n",
    "    \n",
    "# Train the Neural Network model on the first training dataset\n",
    "nn_model10.fit(dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low Level Workflow\n",
    "\n",
    "Here you could see how it works under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pi_optimal.utils.gym_wrapper.model_based_env import ModelBasedEnv\n",
    "\n",
    "sim_env = ModelBasedEnv(models=[nn_model1, nn_model2, nn_model3, nn_model4, nn_model5, nn_model6, nn_model7, nn_model8, nn_model9, nn_model10], dataset=dataset_train, max_episode_steps=200, use_start_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 12, State history: [-0.00470073  1.34486113 -0.07679896 -0.38653565  0.05215874  0.08994773\n",
      "  0.          0.        ]\n",
      "Step: 13, State history: [-0.01410913  1.32018666 -0.04364895 -0.41056232  0.05746433  0.1645839\n",
      "  0.          0.        ]\n",
      "Step: 14, State history: [-0.00573451  1.34256692 -0.03274342 -0.41341624  0.05369352  0.1382038\n",
      "  0.          0.        ]\n",
      "Step: 15, State history: [-0.00135175  1.31640741 -0.05056884 -0.43816871  0.06746983  0.12415782\n",
      "  0.          0.        ]\n",
      "Step: 16, State history: [-0.00618959  1.30754704 -0.0563117  -0.44799178  0.05307124  0.10002305\n",
      "  0.          0.        ]\n",
      "Step: 17, State history: [-0.01331905  1.29178258 -0.07776237 -0.42933553  0.06300507  0.12323796\n",
      "  0.          0.        ]\n",
      "Step: 18, State history: [-0.00783595  1.28365262 -0.04696333 -0.46053702  0.07305327  0.14354231\n",
      "  0.          0.        ]\n",
      "Step: 19, State history: [-0.0084333   1.27871331 -0.05872541 -0.47863985  0.09516768  0.13996049\n",
      "  0.          0.        ]\n",
      "Step: 20, State history: [-0.01041328  1.2496906  -0.08754052 -0.42670747  0.09396291  0.11932978\n",
      "  0.          0.        ]\n",
      "Step: 21, State history: [-0.01483484  1.24587219 -0.07110311 -0.40499646  0.07282512  0.08509522\n",
      "  0.          0.        ]\n",
      "Step: 22, State history: [-0.01305763  1.22146158 -0.12070692 -0.44261747  0.09481469  0.08802297\n",
      "  0.          0.        ]\n",
      "Step: 23, State history: [-0.0094498   1.23108682 -0.08340619 -0.45642755  0.09021196  0.18258195\n",
      "  0.          0.        ]\n",
      "Step: 24, State history: [-0.01588906  1.18362514 -0.10548425 -0.47511801  0.12025205  0.21948128\n",
      "  0.          0.        ]\n",
      "Step: 25, State history: [-0.0085192   1.22093853 -0.10216547 -0.50946448  0.10358807  0.22550664\n",
      "  0.          0.        ]\n",
      "Step: 26, State history: [-0.02229622  1.19270152 -0.09398808 -0.51605838  0.10177884  0.23293869\n",
      "  0.          0.        ]\n",
      "Step: 27, State history: [-0.01739046  1.18145968 -0.08451114 -0.50511348  0.15307696  0.20286865\n",
      "  0.          0.        ]\n",
      "Step: 28, State history: [-0.01572059  1.1819521  -0.06506232 -0.54006565  0.15675395  0.17898589\n",
      "  0.          0.        ]\n",
      "Step: 29, State history: [-0.02076919  1.16120234 -0.0908942  -0.57541521  0.1605422   0.17063112\n",
      "  0.          0.        ]\n",
      "Step: 30, State history: [-0.01906765  1.13316768 -0.0613734  -0.61385772  0.16867533  0.14684616\n",
      "  0.          0.        ]\n",
      "Step: 31, State history: [-0.01767869  1.17539816 -0.07178796 -0.62559517  0.16537732  0.09291853\n",
      "  0.          0.        ]\n",
      "Step: 32, State history: [-0.01088993  1.13111765 -0.02837601 -0.66222601  0.19879842  0.08465654\n",
      "  0.          0.        ]\n",
      "Step: 33, State history: [-0.0187277   1.10403423 -0.05764477 -0.70436729  0.16684286  0.11219311\n",
      "  0.          0.        ]\n",
      "Step: 34, State history: [-0.01990142  1.05227392 -0.03707431 -0.70839172  0.16101074  0.06538016\n",
      "  0.          0.        ]\n",
      "Step: 35, State history: [-0.01919644  1.09171308 -0.01234983 -0.7237579   0.17823322  0.08418576\n",
      "  0.          0.        ]\n",
      "Step: 36, State history: [-0.02323692  1.06500771 -0.05068055 -0.75825222  0.18986065  0.10922089\n",
      "  0.          0.        ]\n",
      "Step: 37, State history: [-0.0206022   1.04463082 -0.01668573 -0.7911512   0.17756658  0.13794282\n",
      "  0.          0.        ]\n",
      "Step: 38, State history: [-0.01642693  1.01155791 -0.04848645 -0.83433275  0.19958257  0.10518585\n",
      "  0.          0.        ]\n",
      "Step: 39, State history: [-0.01637506  1.01550531 -0.03764599 -0.8359138   0.1826331   0.13841568\n",
      "  0.          0.        ]\n",
      "Step: 40, State history: [-0.030526    0.98229059 -0.04314493 -0.88722055  0.20328128  0.10351402\n",
      "  0.          0.        ]\n",
      "Step: 41, State history: [-0.01883387  0.96073819 -0.07752732 -0.90377504  0.21636538  0.17368589\n",
      "  0.          0.        ]\n",
      "Step: 42, State history: [-0.02225192  0.92466403 -0.09512316 -0.86670117  0.21114167  0.1486879\n",
      "  0.          0.        ]\n",
      "Step: 43, State history: [-0.02103093  0.96495571 -0.08480259 -0.83880332  0.23071926  0.15453001\n",
      "  0.          0.        ]\n",
      "Step: 44, State history: [-0.03163671  0.95711982 -0.09670074 -0.81280617  0.19473482  0.15763324\n",
      "  0.          0.        ]\n",
      "Step: 45, State history: [-0.02724636  0.87113852 -0.08293478 -0.8916317   0.24824962  0.15034642\n",
      "  0.          0.        ]\n",
      "Step: 46, State history: [-0.02304714  0.85584117 -0.08442731 -0.90660495  0.24261293  0.18706567\n",
      "  0.          0.        ]\n",
      "Step: 47, State history: [-0.02627249  0.86271225 -0.08626157 -0.91419017  0.24713709  0.09709048\n",
      "  0.          0.        ]\n",
      "Step: 48, State history: [-0.0236109   0.80147813 -0.07876563 -0.97481522  0.28591703  0.16160144\n",
      "  0.          0.        ]\n",
      "Step: 49, State history: [-0.026663    0.78721758 -0.07408268 -0.95288108  0.26436579  0.14282782\n",
      "  0.          0.        ]\n",
      "Step: 50, State history: [-0.0282753   0.75342739 -0.10864341 -0.98770045  0.26635804  0.18962867\n",
      "  0.          0.        ]\n",
      "Step: 51, State history: [-0.02370298  0.75170564 -0.08929506 -1.04601598  0.26493331  0.19218653\n",
      "  0.          0.        ]\n",
      "Step: 52, State history: [-0.03448238  0.72098813 -0.11805333 -1.10005973  0.33823388  0.19965833\n",
      "  0.          0.        ]\n",
      "Step: 53, State history: [-0.0339458   0.71655649 -0.12272211 -1.10848514  0.28205177  0.20861928\n",
      "  0.          0.        ]\n",
      "Step: 54, State history: [-0.03657346  0.68195321 -0.12081803 -1.08074425  0.31140107  0.18877782\n",
      "  0.          0.        ]\n",
      "Step: 55, State history: [-0.03704492  0.62620484 -0.10657349 -1.12639667  0.30288042  0.16803185\n",
      "  0.          0.        ]\n",
      "Step: 56, State history: [-0.04089883  0.61314689 -0.13621678 -1.16434152  0.32535888  0.11979255\n",
      "  0.          0.        ]\n",
      "Step: 57, State history: [-0.02829062  0.60754383 -0.13225967 -1.16057096  0.32516183  0.04989408\n",
      "  0.          0.        ]\n",
      "Step: 58, State history: [-0.04433838  0.55800003 -0.11298619 -1.19098329  0.30461278  0.06458737\n",
      "  0.          0.        ]\n",
      "Step: 59, State history: [-0.04213485  0.54648489 -0.10519806 -1.20872888  0.32813212  0.06620161\n",
      "  0.          0.        ]\n",
      "Step: 60, State history: [-3.83292581e-02  5.16566755e-01 -1.02727159e-01 -1.25048349e+00\n",
      "  3.16314008e-01 -1.04258349e-04  0.00000000e+00  0.00000000e+00]\n",
      "Step: 61, State history: [-0.05071217  0.4945377  -0.13334773 -1.22626899  0.33178513  0.02568667\n",
      "  0.          0.        ]\n",
      "Step: 62, State history: [-0.04018201  0.42600589 -0.12075096 -1.25697551  0.32738394  0.06962729\n",
      "  0.          0.        ]\n",
      "Step: 63, State history: [-0.05300053  0.43316813 -0.17698334 -1.21936979  0.35496166  0.04496417\n",
      "  0.          0.        ]\n",
      "Step: 64, State history: [-0.05255711  0.37530956 -0.13903151 -1.2622237   0.31679697  0.08631483\n",
      "  0.          0.        ]\n",
      "Step: 65, State history: [-0.05409201  0.36460143 -0.17134753 -1.29125474  0.3464175   0.17818864\n",
      "  0.          0.        ]\n",
      "Step: 66, State history: [-0.05470905  0.34065543 -0.19701284 -1.3446225   0.32997339  0.07380276\n",
      "  0.          0.        ]\n",
      "Step: 67, State history: [-0.04134306  0.31646477 -0.22248013 -1.27300407  0.32361132  0.0799308\n",
      "  0.          0.        ]\n",
      "Step: 68, State history: [-0.05414282  0.27825306 -0.18286692 -1.29283723  0.35010497  0.0930183\n",
      "  0.          0.        ]\n",
      "Step: 69, State history: [-0.06158785  0.23604681 -0.21235224 -1.36585779  0.34686353  0.09469159\n",
      "  0.          0.        ]\n",
      "Step: 70, State history: [-0.06881558  0.19204081 -0.25059946 -1.29779754  0.31625452  0.12943043\n",
      "  0.          0.        ]\n",
      "Step: 71, State history: [-0.0696912   0.164074   -0.23305109 -1.30677977  0.34881289  0.14816622\n",
      "  0.          0.        ]\n",
      "Step: 72, State history: [-0.06502926  0.1613981  -0.26431933 -1.37776535  0.3954552   0.14375932\n",
      "  0.          0.        ]\n",
      "Step: 73, State history: [-0.07139529  0.1445386  -0.23404856 -1.34448632  0.39743947  0.16877995\n",
      "  0.          0.        ]\n",
      "Step: 74, State history: [-0.06328288  0.10231563 -0.25707407 -1.38907909  0.37651835  0.1430093\n",
      "  0.          0.        ]\n",
      "Step: 75, State history: [-0.07597253  0.09889693 -0.25341988 -1.37155236  0.36986083  0.0874624\n",
      "  0.          0.        ]\n",
      "Step: 76, State history: [-0.07749525  0.04102205 -0.32796891 -1.34486969  0.36800334  0.14765374\n",
      "  0.          0.        ]\n",
      "Step: 77, State history: [-0.08950451  0.04345331 -0.30454783 -1.4046825   0.38407219  0.10504332\n",
      "  0.          1.        ]\n",
      "Step: 78, State history: [-0.08261926 -0.02412205 -0.13763988 -1.24577473  0.47865657 -0.17695606\n",
      "  0.          1.        ]\n",
      "Step: 79, State history: [-0.08193875 -0.02994139 -0.0946337  -0.81767143  0.30182916 -3.0397124\n",
      "  0.          1.        ]\n",
      "-105.3994454073834\n"
     ]
    }
   ],
   "source": [
    "obs, _ = sim_env.reset()\n",
    "total_reward = 0\n",
    "for _ in range(200):\n",
    "    action = sim_env.action_space.sample()\n",
    "    obs, reward, done, done, info = sim_env.step(action)\n",
    "    total_reward += reward\n",
    "    sim_env.render(\"human\")\n",
    "    if done:\n",
    "        break\n",
    "sim_env.close()\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.8     |\n",
      "|    ep_rew_mean     | -171     |\n",
      "| time/              |          |\n",
      "|    fps             | 187      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 88.3        |\n",
      "|    ep_rew_mean          | -377        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 172         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014061442 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | -0.000606   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 309         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00667    |\n",
      "|    value_loss           | 1.07e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-198.38 +/- 109.77\n",
      "Episode length: 96.02 +/- 18.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 96           |\n",
      "|    mean_reward          | -198         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012468555 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.999        |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | -0.0176      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.05e+05     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    value_loss           | 1.55e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.9     |\n",
      "|    ep_rew_mean     | -298     |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 95.5        |\n",
      "|    ep_rew_mean          | -288        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 174         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090694934 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | -0.0154     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 238         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    value_loss           | 807         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-242.90 +/- 120.97\n",
      "Episode length: 134.06 +/- 36.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 134         |\n",
      "|    mean_reward          | -243        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051037528 |\n",
      "|    clip_fraction        | 0.000146    |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | -0.00153    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 342         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00848    |\n",
      "|    value_loss           | 1.08e+03    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 97.3     |\n",
      "|    ep_rew_mean     | -274     |\n",
      "| time/              |          |\n",
      "|    fps             | 173      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 59       |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 108         |\n",
      "|    ep_rew_mean          | -642        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 174         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.107732356 |\n",
      "|    clip_fraction        | 0.0146      |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | -0.00354    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 361         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 727         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 117          |\n",
      "|    ep_rew_mean          | -723         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 175          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 81           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026022617 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.999        |\n",
      "|    entropy_loss         | -0.851       |\n",
      "|    explained_variance   | -0.00684     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.32e+05     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    value_loss           | 5.42e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-372.43 +/- 150.66\n",
      "Episode length: 137.76 +/- 44.08\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 138          |\n",
      "|    mean_reward          | -372         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031249211 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.999        |\n",
      "|    entropy_loss         | -0.995       |\n",
      "|    explained_variance   | 0.000718     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.45e+04     |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00151     |\n",
      "|    value_loss           | 1.84e+05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 126      |\n",
      "|    ep_rew_mean     | -855     |\n",
      "| time/              |          |\n",
      "|    fps             | 172      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 95       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 137        |\n",
      "|    ep_rew_mean          | -860       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 172        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 106        |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11930993 |\n",
      "|    clip_fraction        | 0.0289     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -1.01      |\n",
      "|    explained_variance   | 0.000123   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.95e+04   |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0315    |\n",
      "|    value_loss           | 6.66e+04   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-94.30 +/- 80.11\n",
      "Episode length: 141.34 +/- 38.96\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 141        |\n",
      "|    mean_reward          | -94.3      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.24069896 |\n",
      "|    clip_fraction        | 0.11       |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -1.17      |\n",
      "|    explained_variance   | -0.000214  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 197        |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0664    |\n",
      "|    value_loss           | 409        |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 142      |\n",
      "|    ep_rew_mean     | -851     |\n",
      "| time/              |          |\n",
      "|    fps             | 172      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 118      |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 147        |\n",
      "|    ep_rew_mean          | -829       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 129        |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04330781 |\n",
      "|    clip_fraction        | 0.000342   |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -1.13      |\n",
      "|    explained_variance   | -0.00153   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 597        |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.00626   |\n",
      "|    value_loss           | 839        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 150         |\n",
      "|    ep_rew_mean          | -820        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 175         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 139         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.069263846 |\n",
      "|    clip_fraction        | 0.00977     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -0.012      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 197         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 510         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-39.86 +/- 70.31\n",
      "Episode length: 174.10 +/- 170.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 174         |\n",
      "|    mean_reward          | -39.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030018616 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | -0.00469    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 636         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00579    |\n",
      "|    value_loss           | 851         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 153      |\n",
      "|    ep_rew_mean     | -521     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 151      |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 153         |\n",
      "|    ep_rew_mean          | -409        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 176         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 162         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028598836 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.00344     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 118         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00488    |\n",
      "|    value_loss           | 466         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-42.42 +/- 71.22\n",
      "Episode length: 448.10 +/- 371.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 448          |\n",
      "|    mean_reward          | -42.4        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025424063 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.999        |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.00336      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 362          |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00261     |\n",
      "|    value_loss           | 702          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 156      |\n",
      "|    ep_rew_mean     | -170     |\n",
      "| time/              |          |\n",
      "|    fps             | 173      |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 176      |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 155         |\n",
      "|    ep_rew_mean          | -81.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 190         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040082604 |\n",
      "|    clip_fraction        | 0.000879    |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.0184      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 156         |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00487    |\n",
      "|    value_loss           | 200         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 156         |\n",
      "|    ep_rew_mean          | -64.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 203         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022882996 |\n",
      "|    clip_fraction        | 0.000439    |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.0261      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 379         |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00431    |\n",
      "|    value_loss           | 646         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-78.94 +/- 102.20\n",
      "Episode length: 405.42 +/- 335.78\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 405        |\n",
      "|    mean_reward          | -78.9      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 35000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10205357 |\n",
      "|    clip_fraction        | 4.88e-05   |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.959     |\n",
      "|    explained_variance   | 0.00146    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 154        |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.01      |\n",
      "|    value_loss           | 402        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 163      |\n",
      "|    ep_rew_mean     | -67.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 168      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 219      |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 166        |\n",
      "|    ep_rew_mean          | -64.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 230        |\n",
      "|    total_timesteps      | 38912      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03856289 |\n",
      "|    clip_fraction        | 0.00576    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -1.02      |\n",
      "|    explained_variance   | 0.025      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 81.5       |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.00425   |\n",
      "|    value_loss           | 509        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-121.52 +/- 88.60\n",
      "Episode length: 192.94 +/- 132.95\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 193        |\n",
      "|    mean_reward          | -122       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 40000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14498669 |\n",
      "|    clip_fraction        | 0.0419     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -1.02      |\n",
      "|    explained_variance   | 0.0673     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 94.5       |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0332    |\n",
      "|    value_loss           | 346        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 167      |\n",
      "|    ep_rew_mean     | -57.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 168      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 243      |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 169        |\n",
      "|    ep_rew_mean          | -49.4      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 254        |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09255865 |\n",
      "|    clip_fraction        | 0.00947    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.978     |\n",
      "|    explained_variance   | 0.0299     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 329        |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0173    |\n",
      "|    value_loss           | 447        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-11.09 +/- 67.67\n",
      "Episode length: 548.64 +/- 362.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 549         |\n",
      "|    mean_reward          | -11.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058053844 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.995      |\n",
      "|    explained_variance   | 0.0826      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 79.9        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 185         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 172      |\n",
      "|    ep_rew_mean     | -57.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 168      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 268      |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 170        |\n",
      "|    ep_rew_mean          | -49.4      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 278        |\n",
      "|    total_timesteps      | 47104      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16087627 |\n",
      "|    clip_fraction        | 0.0269     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -1.14      |\n",
      "|    explained_variance   | 0.434      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 36.6       |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    value_loss           | 130        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 172         |\n",
      "|    ep_rew_mean          | -41.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 289         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011188116 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.995      |\n",
      "|    explained_variance   | 0.186       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 309         |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00442    |\n",
      "|    value_loss           | 566         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-48.94 +/- 49.50\n",
      "Episode length: 355.98 +/- 346.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 356         |\n",
      "|    mean_reward          | -48.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041806094 |\n",
      "|    clip_fraction        | 0.000537    |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.465       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 169         |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0054     |\n",
      "|    value_loss           | 356         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 173      |\n",
      "|    ep_rew_mean     | -42.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 168      |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 303      |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 175          |\n",
      "|    ep_rew_mean          | -30.7        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 168          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 315          |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019023796 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.999        |\n",
      "|    entropy_loss         | -1           |\n",
      "|    explained_variance   | 0.485        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 640          |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    value_loss           | 700          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-91.33 +/- 58.14\n",
      "Episode length: 151.94 +/- 126.77\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 152        |\n",
      "|    mean_reward          | -91.3      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 55000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05823594 |\n",
      "|    clip_fraction        | 0.00762    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -1.05      |\n",
      "|    explained_variance   | 0.731      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 71.1       |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.00763   |\n",
      "|    value_loss           | 148        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 172      |\n",
      "|    ep_rew_mean     | -25.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 327      |\n",
      "|    total_timesteps | 55296    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 177         |\n",
      "|    ep_rew_mean          | -21.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 338         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027279777 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.966      |\n",
      "|    explained_variance   | 0.565       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 256         |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0049     |\n",
      "|    value_loss           | 597         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 177         |\n",
      "|    ep_rew_mean          | -27.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 350         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053975813 |\n",
      "|    clip_fraction        | 0.00215     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.723       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 63.5        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00893    |\n",
      "|    value_loss           | 247         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-176.71 +/- 101.30\n",
      "Episode length: 183.32 +/- 66.13\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 183        |\n",
      "|    mean_reward          | -177       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 60000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08616817 |\n",
      "|    clip_fraction        | 0.00317    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.88      |\n",
      "|    explained_variance   | 0.75       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 145        |\n",
      "|    n_updates            | 290        |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    value_loss           | 538        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 178      |\n",
      "|    ep_rew_mean     | -19      |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 362      |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 178        |\n",
      "|    ep_rew_mean          | -1.54      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 373        |\n",
      "|    total_timesteps      | 63488      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11946868 |\n",
      "|    clip_fraction        | 0.0137     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.931     |\n",
      "|    explained_variance   | 0.564      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 56.5       |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.022     |\n",
      "|    value_loss           | 184        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-3.35 +/- 63.32\n",
      "Episode length: 314.78 +/- 344.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 315         |\n",
      "|    mean_reward          | -3.35       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 65000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032934032 |\n",
      "|    clip_fraction        | 0.000195    |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.995      |\n",
      "|    explained_variance   | 0.469       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 430         |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00691    |\n",
      "|    value_loss           | 489         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 175      |\n",
      "|    ep_rew_mean     | 3.1      |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 387      |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 172         |\n",
      "|    ep_rew_mean          | -3.82       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 398         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025522836 |\n",
      "|    clip_fraction        | 0.000684    |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.506       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 54.6        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00663    |\n",
      "|    value_loss           | 381         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 170         |\n",
      "|    ep_rew_mean          | -2.23       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 409         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007390675 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.42        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 51.1        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00261    |\n",
      "|    value_loss           | 574         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-91.35 +/- 101.49\n",
      "Episode length: 346.50 +/- 329.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 346         |\n",
      "|    mean_reward          | -91.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.092987046 |\n",
      "|    clip_fraction        | 0.00537     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.528       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 320         |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 443         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 169      |\n",
      "|    ep_rew_mean     | 5.44     |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 422      |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 167         |\n",
      "|    ep_rew_mean          | 17          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 433         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037250657 |\n",
      "|    clip_fraction        | 0.00474     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 59.1        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00768    |\n",
      "|    value_loss           | 161         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-63.99 +/- 86.31\n",
      "Episode length: 332.06 +/- 338.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 332         |\n",
      "|    mean_reward          | -64         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 75000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017215375 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.743       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 54.4        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00343    |\n",
      "|    value_loss           | 108         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 169      |\n",
      "|    ep_rew_mean     | 28.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 446      |\n",
      "|    total_timesteps | 75776    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 170         |\n",
      "|    ep_rew_mean          | 23.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 456         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060482588 |\n",
      "|    clip_fraction        | 9.77e-05    |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.853      |\n",
      "|    explained_variance   | 0.697       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 107         |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00574    |\n",
      "|    value_loss           | 292         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 170        |\n",
      "|    ep_rew_mean          | 11.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 39         |\n",
      "|    time_elapsed         | 470        |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11163472 |\n",
      "|    clip_fraction        | 0.0104     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -1.09      |\n",
      "|    explained_variance   | 0.815      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 47.5       |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.0093    |\n",
      "|    value_loss           | 140        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=0.38 +/- 43.32\n",
      "Episode length: 331.36 +/- 362.34\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 331        |\n",
      "|    mean_reward          | 0.383      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 80000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10109495 |\n",
      "|    clip_fraction        | 0.0115     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.908     |\n",
      "|    explained_variance   | 0.904      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 28.3       |\n",
      "|    n_updates            | 390        |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    value_loss           | 125        |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 168      |\n",
      "|    ep_rew_mean     | 7.42     |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 482      |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 173        |\n",
      "|    ep_rew_mean          | 13.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 41         |\n",
      "|    time_elapsed         | 492        |\n",
      "|    total_timesteps      | 83968      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09328563 |\n",
      "|    clip_fraction        | 0.000635   |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.829     |\n",
      "|    explained_variance   | 0.779      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 152        |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0135    |\n",
      "|    value_loss           | 370        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-85.08 +/- 76.31\n",
      "Episode length: 481.92 +/- 354.75\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 482        |\n",
      "|    mean_reward          | -85.1      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 85000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05354681 |\n",
      "|    clip_fraction        | 0.00552    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.961     |\n",
      "|    explained_variance   | 0.844      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 26.9       |\n",
      "|    n_updates            | 410        |\n",
      "|    policy_gradient_loss | -0.00378   |\n",
      "|    value_loss           | 192        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 176      |\n",
      "|    ep_rew_mean     | 19.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 506      |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 177        |\n",
      "|    ep_rew_mean          | 27.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 43         |\n",
      "|    time_elapsed         | 517        |\n",
      "|    total_timesteps      | 88064      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09858666 |\n",
      "|    clip_fraction        | 0.0223     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.864     |\n",
      "|    explained_variance   | 0.817      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 132        |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    value_loss           | 428        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=75.41 +/- 89.56\n",
      "Episode length: 524.98 +/- 384.37\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 525        |\n",
      "|    mean_reward          | 75.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 90000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06989086 |\n",
      "|    clip_fraction        | 0.00264    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.905     |\n",
      "|    explained_variance   | 0.847      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 66.8       |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.00719   |\n",
      "|    value_loss           | 205        |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 181      |\n",
      "|    ep_rew_mean     | 28.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 168      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 534      |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 183         |\n",
      "|    ep_rew_mean          | 25          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 546         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079462245 |\n",
      "|    clip_fraction        | 0.00811     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.613       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 45.2        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    value_loss           | 150         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 183        |\n",
      "|    ep_rew_mean          | 28.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 46         |\n",
      "|    time_elapsed         | 558        |\n",
      "|    total_timesteps      | 94208      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.21423702 |\n",
      "|    clip_fraction        | 0.00957    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.765     |\n",
      "|    explained_variance   | 0.911      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 73.3       |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | -0.0222    |\n",
      "|    value_loss           | 89.6       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=39.48 +/- 91.17\n",
      "Episode length: 923.68 +/- 204.82\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 924        |\n",
      "|    mean_reward          | 39.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 95000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12457635 |\n",
      "|    clip_fraction        | 0.0348     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -1.01      |\n",
      "|    explained_variance   | 0.874      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 11.5       |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0282    |\n",
      "|    value_loss           | 48         |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 185      |\n",
      "|    ep_rew_mean     | 32.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 167      |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 574      |\n",
      "|    total_timesteps | 96256    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 183        |\n",
      "|    ep_rew_mean          | 46.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 48         |\n",
      "|    time_elapsed         | 585        |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08316889 |\n",
      "|    clip_fraction        | 0.0173     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -1.08      |\n",
      "|    explained_variance   | 0.672      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 40.1       |\n",
      "|    n_updates            | 470        |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    value_loss           | 171        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=162.00 +/- 106.31\n",
      "Episode length: 609.96 +/- 324.45\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 610        |\n",
      "|    mean_reward          | 162        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 100000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05651784 |\n",
      "|    clip_fraction        | 0.00215    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.896     |\n",
      "|    explained_variance   | 0.832      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 12.3       |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    value_loss           | 78.5       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 187      |\n",
      "|    ep_rew_mean     | 52.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 166      |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 602      |\n",
      "|    total_timesteps | 100352   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 189        |\n",
      "|    ep_rew_mean          | 54.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 50         |\n",
      "|    time_elapsed         | 615        |\n",
      "|    total_timesteps      | 102400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05691602 |\n",
      "|    clip_fraction        | 0.0083     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.863     |\n",
      "|    explained_variance   | 0.921      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 27.1       |\n",
      "|    n_updates            | 490        |\n",
      "|    policy_gradient_loss | -0.00519   |\n",
      "|    value_loss           | 51.9       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 188        |\n",
      "|    ep_rew_mean          | 58.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 51         |\n",
      "|    time_elapsed         | 627        |\n",
      "|    total_timesteps      | 104448     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06237865 |\n",
      "|    clip_fraction        | 0.00859    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.947     |\n",
      "|    explained_variance   | 0.627      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 83.5       |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.00816   |\n",
      "|    value_loss           | 494        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=64.18 +/- 128.62\n",
      "Episode length: 232.58 +/- 106.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 233         |\n",
      "|    mean_reward          | 64.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 105000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051268358 |\n",
      "|    clip_fraction        | 0.0084      |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.965      |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.2        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.00603    |\n",
      "|    value_loss           | 155         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 186      |\n",
      "|    ep_rew_mean     | 46.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 166      |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 639      |\n",
      "|    total_timesteps | 106496   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 187         |\n",
      "|    ep_rew_mean          | 50.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 651         |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081469685 |\n",
      "|    clip_fraction        | 0.00278     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.781      |\n",
      "|    explained_variance   | 0.738       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 237         |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    value_loss           | 295         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=73.36 +/- 92.79\n",
      "Episode length: 687.92 +/- 360.54\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 688        |\n",
      "|    mean_reward          | 73.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 110000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06078071 |\n",
      "|    clip_fraction        | 0.00332    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.908     |\n",
      "|    explained_variance   | 0.636      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 230        |\n",
      "|    n_updates            | 530        |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    value_loss           | 653        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 187      |\n",
      "|    ep_rew_mean     | 56.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 165      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 668      |\n",
      "|    total_timesteps | 110592   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 188         |\n",
      "|    ep_rew_mean          | 58.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 680         |\n",
      "|    total_timesteps      | 112640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059568614 |\n",
      "|    clip_fraction        | 0.00474     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.958      |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 51.8        |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00669    |\n",
      "|    value_loss           | 255         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 188       |\n",
      "|    ep_rew_mean          | 54        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 165       |\n",
      "|    iterations           | 56        |\n",
      "|    time_elapsed         | 692       |\n",
      "|    total_timesteps      | 114688    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6125517 |\n",
      "|    clip_fraction        | 0.0326    |\n",
      "|    clip_range           | 0.999     |\n",
      "|    entropy_loss         | -0.51     |\n",
      "|    explained_variance   | 0.923     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 12        |\n",
      "|    n_updates            | 550       |\n",
      "|    policy_gradient_loss | -0.029    |\n",
      "|    value_loss           | 27.7      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=-51.09 +/- 20.46\n",
      "Episode length: 993.80 +/- 43.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 994        |\n",
      "|    mean_reward          | -51.1      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 115000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06610582 |\n",
      "|    clip_fraction        | 0.00918    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.949     |\n",
      "|    explained_variance   | 0.932      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.11       |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.00744   |\n",
      "|    value_loss           | 24.1       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 190      |\n",
      "|    ep_rew_mean     | 49       |\n",
      "| time/              |          |\n",
      "|    fps             | 164      |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 710      |\n",
      "|    total_timesteps | 116736   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 191        |\n",
      "|    ep_rew_mean          | 57.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 164        |\n",
      "|    iterations           | 58         |\n",
      "|    time_elapsed         | 721        |\n",
      "|    total_timesteps      | 118784     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09355265 |\n",
      "|    clip_fraction        | 0.0144     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.846     |\n",
      "|    explained_variance   | 0.964      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.86       |\n",
      "|    n_updates            | 570        |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    value_loss           | 28.7       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-50.51 +/- 20.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -50.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043857623 |\n",
      "|    clip_fraction        | 0.00103     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.91       |\n",
      "|    explained_variance   | 0.705       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 68.8        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.00645    |\n",
      "|    value_loss           | 415         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 191      |\n",
      "|    ep_rew_mean     | 62       |\n",
      "| time/              |          |\n",
      "|    fps             | 163      |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 739      |\n",
      "|    total_timesteps | 120832   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 191         |\n",
      "|    ep_rew_mean          | 57.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 750         |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053999227 |\n",
      "|    clip_fraction        | 0.00283     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.805      |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 84.2        |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.00993    |\n",
      "|    value_loss           | 319         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 193        |\n",
      "|    ep_rew_mean          | 55.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 163        |\n",
      "|    iterations           | 61         |\n",
      "|    time_elapsed         | 763        |\n",
      "|    total_timesteps      | 124928     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02880416 |\n",
      "|    clip_fraction        | 0.00107    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.953     |\n",
      "|    explained_variance   | 0.74       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 90.5       |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.00367   |\n",
      "|    value_loss           | 321        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=-50.36 +/- 19.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -50.4      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 125000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08388546 |\n",
      "|    clip_fraction        | 0.0158     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -1.03      |\n",
      "|    explained_variance   | 0.831      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 72.2       |\n",
      "|    n_updates            | 610        |\n",
      "|    policy_gradient_loss | -0.0239    |\n",
      "|    value_loss           | 137        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 192      |\n",
      "|    ep_rew_mean     | 56.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 162      |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 781      |\n",
      "|    total_timesteps | 126976   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 192        |\n",
      "|    ep_rew_mean          | 55.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 63         |\n",
      "|    time_elapsed         | 793        |\n",
      "|    total_timesteps      | 129024     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09468272 |\n",
      "|    clip_fraction        | 0.00903    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.99      |\n",
      "|    explained_variance   | 0.847      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 77.2       |\n",
      "|    n_updates            | 620        |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    value_loss           | 386        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-64.34 +/- 28.60\n",
      "Episode length: 936.10 +/- 217.70\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 936        |\n",
      "|    mean_reward          | -64.3      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 130000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07032778 |\n",
      "|    clip_fraction        | 0.00391    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.865     |\n",
      "|    explained_variance   | 0.921      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 94         |\n",
      "|    n_updates            | 630        |\n",
      "|    policy_gradient_loss | -0.0065    |\n",
      "|    value_loss           | 155        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 189      |\n",
      "|    ep_rew_mean     | 54.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 161      |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 810      |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 186        |\n",
      "|    ep_rew_mean          | 57.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 65         |\n",
      "|    time_elapsed         | 821        |\n",
      "|    total_timesteps      | 133120     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14842945 |\n",
      "|    clip_fraction        | 0.0196     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.904     |\n",
      "|    explained_variance   | 0.919      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 24.9       |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.018     |\n",
      "|    value_loss           | 107        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=-83.05 +/- 26.41\n",
      "Episode length: 883.92 +/- 275.17\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 884        |\n",
      "|    mean_reward          | -83        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 135000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08519223 |\n",
      "|    clip_fraction        | 0.0231     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.983     |\n",
      "|    explained_variance   | 0.884      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 47.1       |\n",
      "|    n_updates            | 650        |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    value_loss           | 106        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 185      |\n",
      "|    ep_rew_mean     | 52.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 161      |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 837      |\n",
      "|    total_timesteps | 135168   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 185         |\n",
      "|    ep_rew_mean          | 45.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 850         |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038952194 |\n",
      "|    clip_fraction        | 0.00996     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.827      |\n",
      "|    explained_variance   | 0.726       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 66.6        |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.00899    |\n",
      "|    value_loss           | 135         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 182        |\n",
      "|    ep_rew_mean          | 36.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 161        |\n",
      "|    iterations           | 68         |\n",
      "|    time_elapsed         | 861        |\n",
      "|    total_timesteps      | 139264     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12612304 |\n",
      "|    clip_fraction        | 0.0275     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.755     |\n",
      "|    explained_variance   | 0.807      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 28.6       |\n",
      "|    n_updates            | 670        |\n",
      "|    policy_gradient_loss | -0.00959   |\n",
      "|    value_loss           | 51.6       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-59.09 +/- 29.99\n",
      "Episode length: 815.56 +/- 352.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 816         |\n",
      "|    mean_reward          | -59.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 140000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055826552 |\n",
      "|    clip_fraction        | 0.00356     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.874      |\n",
      "|    explained_variance   | 0.762       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.3        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 134         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 180      |\n",
      "|    ep_rew_mean     | 41.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 160      |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 877      |\n",
      "|    total_timesteps | 141312   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 180       |\n",
      "|    ep_rew_mean          | 44.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 161       |\n",
      "|    iterations           | 70        |\n",
      "|    time_elapsed         | 889       |\n",
      "|    total_timesteps      | 143360    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1929102 |\n",
      "|    clip_fraction        | 0.0146    |\n",
      "|    clip_range           | 0.999     |\n",
      "|    entropy_loss         | -0.685    |\n",
      "|    explained_variance   | 0.822     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 12.7      |\n",
      "|    n_updates            | 690       |\n",
      "|    policy_gradient_loss | -0.0219   |\n",
      "|    value_loss           | 53        |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=-53.51 +/- 23.81\n",
      "Episode length: 984.82 +/- 106.26\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 985        |\n",
      "|    mean_reward          | -53.5      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 145000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.22954354 |\n",
      "|    clip_fraction        | 0.0269     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.746     |\n",
      "|    explained_variance   | 0.919      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 13.8       |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    value_loss           | 49.5       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 180      |\n",
      "|    ep_rew_mean     | 32.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 160      |\n",
      "|    iterations      | 71       |\n",
      "|    time_elapsed    | 907      |\n",
      "|    total_timesteps | 145408   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 179        |\n",
      "|    ep_rew_mean          | 29.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 160        |\n",
      "|    iterations           | 72         |\n",
      "|    time_elapsed         | 919        |\n",
      "|    total_timesteps      | 147456     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07235034 |\n",
      "|    clip_fraction        | 0.0174     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.898     |\n",
      "|    explained_variance   | 0.904      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 21.8       |\n",
      "|    n_updates            | 710        |\n",
      "|    policy_gradient_loss | -0.0078    |\n",
      "|    value_loss           | 81.1       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 180       |\n",
      "|    ep_rew_mean          | 32.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 160       |\n",
      "|    iterations           | 73        |\n",
      "|    time_elapsed         | 930       |\n",
      "|    total_timesteps      | 149504    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0496768 |\n",
      "|    clip_fraction        | 0.00547   |\n",
      "|    clip_range           | 0.999     |\n",
      "|    entropy_loss         | -0.846    |\n",
      "|    explained_variance   | 0.643     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 105       |\n",
      "|    n_updates            | 720       |\n",
      "|    policy_gradient_loss | -0.00696  |\n",
      "|    value_loss           | 603       |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-32.04 +/- 48.26\n",
      "Episode length: 924.18 +/- 212.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 924         |\n",
      "|    mean_reward          | -32         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 150000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019113446 |\n",
      "|    clip_fraction        | 0.000488    |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.934      |\n",
      "|    explained_variance   | 0.683       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.4        |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.00181    |\n",
      "|    value_loss           | 141         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 182      |\n",
      "|    ep_rew_mean     | 30.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 159      |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 948      |\n",
      "|    total_timesteps | 151552   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 182        |\n",
      "|    ep_rew_mean          | 37.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 75         |\n",
      "|    time_elapsed         | 960        |\n",
      "|    total_timesteps      | 153600     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11213435 |\n",
      "|    clip_fraction        | 0.021      |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.832     |\n",
      "|    explained_variance   | 0.91       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 13.3       |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    value_loss           | 24.9       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=5.65 +/- 108.75\n",
      "Episode length: 842.44 +/- 270.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 842         |\n",
      "|    mean_reward          | 5.65        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 155000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058820046 |\n",
      "|    clip_fraction        | 0.00337     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.916      |\n",
      "|    explained_variance   | 0.657       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 47.5        |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.00721    |\n",
      "|    value_loss           | 214         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 180      |\n",
      "|    ep_rew_mean     | 41       |\n",
      "| time/              |          |\n",
      "|    fps             | 159      |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 977      |\n",
      "|    total_timesteps | 155648   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 183         |\n",
      "|    ep_rew_mean          | 46.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 990         |\n",
      "|    total_timesteps      | 157696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008159611 |\n",
      "|    clip_fraction        | 0.000146    |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.849      |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 99.2        |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.00174    |\n",
      "|    value_loss           | 183         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 186        |\n",
      "|    ep_rew_mean          | 42.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 78         |\n",
      "|    time_elapsed         | 1002       |\n",
      "|    total_timesteps      | 159744     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09516369 |\n",
      "|    clip_fraction        | 0.00728    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.841     |\n",
      "|    explained_variance   | 0.901      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 89.8       |\n",
      "|    n_updates            | 770        |\n",
      "|    policy_gradient_loss | -0.00535   |\n",
      "|    value_loss           | 62.8       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=25.75 +/- 103.36\n",
      "Episode length: 745.84 +/- 319.59\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 746        |\n",
      "|    mean_reward          | 25.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 160000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.24760628 |\n",
      "|    clip_fraction        | 0.0694     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.933     |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.34       |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.029     |\n",
      "|    value_loss           | 21.5       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 187      |\n",
      "|    ep_rew_mean     | 41.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 158      |\n",
      "|    iterations      | 79       |\n",
      "|    time_elapsed    | 1019     |\n",
      "|    total_timesteps | 161792   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 186        |\n",
      "|    ep_rew_mean          | 50.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 158        |\n",
      "|    iterations           | 80         |\n",
      "|    time_elapsed         | 1030       |\n",
      "|    total_timesteps      | 163840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09276159 |\n",
      "|    clip_fraction        | 0.00464    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.708     |\n",
      "|    explained_variance   | 0.946      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 19.9       |\n",
      "|    n_updates            | 790        |\n",
      "|    policy_gradient_loss | -0.0099    |\n",
      "|    value_loss           | 53.3       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=36.01 +/- 134.61\n",
      "Episode length: 658.40 +/- 331.92\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 658        |\n",
      "|    mean_reward          | 36         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 165000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02024848 |\n",
      "|    clip_fraction        | 0.00171    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.746     |\n",
      "|    explained_variance   | 0.881      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 65.4       |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.00586   |\n",
      "|    value_loss           | 243        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 187      |\n",
      "|    ep_rew_mean     | 51.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 158      |\n",
      "|    iterations      | 81       |\n",
      "|    time_elapsed    | 1045     |\n",
      "|    total_timesteps | 165888   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 185         |\n",
      "|    ep_rew_mean          | 50.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 1058        |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058203034 |\n",
      "|    clip_fraction        | 0.00552     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.85       |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.9        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00637    |\n",
      "|    value_loss           | 103         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 185         |\n",
      "|    ep_rew_mean          | 53.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 1070        |\n",
      "|    total_timesteps      | 169984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031059869 |\n",
      "|    clip_fraction        | 0.00317     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.757      |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 110         |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.00484    |\n",
      "|    value_loss           | 155         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-1.80 +/- 120.71\n",
      "Episode length: 601.34 +/- 267.95\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 601        |\n",
      "|    mean_reward          | -1.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 170000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05921617 |\n",
      "|    clip_fraction        | 0.00322    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.741     |\n",
      "|    explained_variance   | 0.938      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 14.9       |\n",
      "|    n_updates            | 830        |\n",
      "|    policy_gradient_loss | -0.00692   |\n",
      "|    value_loss           | 79.9       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 185      |\n",
      "|    ep_rew_mean     | 52.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 158      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 1085     |\n",
      "|    total_timesteps | 172032   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 187        |\n",
      "|    ep_rew_mean          | 44.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 158        |\n",
      "|    iterations           | 85         |\n",
      "|    time_elapsed         | 1097       |\n",
      "|    total_timesteps      | 174080     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06272267 |\n",
      "|    clip_fraction        | 0.00845    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.727     |\n",
      "|    explained_variance   | 0.955      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 16.7       |\n",
      "|    n_updates            | 840        |\n",
      "|    policy_gradient_loss | -0.0126    |\n",
      "|    value_loss           | 56         |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=-78.47 +/- 34.55\n",
      "Episode length: 928.72 +/- 201.10\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 929        |\n",
      "|    mean_reward          | -78.5      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 175000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08635719 |\n",
      "|    clip_fraction        | 0.00894    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.542     |\n",
      "|    explained_variance   | 0.947      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 14.6       |\n",
      "|    n_updates            | 850        |\n",
      "|    policy_gradient_loss | -0.00969   |\n",
      "|    value_loss           | 59.8       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 189      |\n",
      "|    ep_rew_mean     | 37.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 157      |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 1117     |\n",
      "|    total_timesteps | 176128   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 188        |\n",
      "|    ep_rew_mean          | 39.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 157        |\n",
      "|    iterations           | 87         |\n",
      "|    time_elapsed         | 1129       |\n",
      "|    total_timesteps      | 178176     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05178811 |\n",
      "|    clip_fraction        | 0.0138     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.743     |\n",
      "|    explained_variance   | 0.96       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 19.9       |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.00776   |\n",
      "|    value_loss           | 32.8       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-122.95 +/- 75.79\n",
      "Episode length: 804.40 +/- 218.99\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 804        |\n",
      "|    mean_reward          | -123       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 180000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06236082 |\n",
      "|    clip_fraction        | 0.0206     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.87      |\n",
      "|    explained_variance   | 0.605      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 11.8       |\n",
      "|    n_updates            | 870        |\n",
      "|    policy_gradient_loss | -0.00259   |\n",
      "|    value_loss           | 662        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 188      |\n",
      "|    ep_rew_mean     | 42.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 157      |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 1146     |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 190         |\n",
      "|    ep_rew_mean          | 39.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 1157        |\n",
      "|    total_timesteps      | 182272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030451449 |\n",
      "|    clip_fraction        | 0.000488    |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.723      |\n",
      "|    explained_variance   | 0.828       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 45.7        |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.00657    |\n",
      "|    value_loss           | 148         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 191        |\n",
      "|    ep_rew_mean          | 39.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 157        |\n",
      "|    iterations           | 90         |\n",
      "|    time_elapsed         | 1168       |\n",
      "|    total_timesteps      | 184320     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03305471 |\n",
      "|    clip_fraction        | 0.00264    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.717     |\n",
      "|    explained_variance   | 0.895      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 15.4       |\n",
      "|    n_updates            | 890        |\n",
      "|    policy_gradient_loss | -0.00526   |\n",
      "|    value_loss           | 109        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=-143.27 +/- 46.24\n",
      "Episode length: 726.98 +/- 199.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 727         |\n",
      "|    mean_reward          | -143        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 185000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043746177 |\n",
      "|    clip_fraction        | 0.00747     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.613      |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 70.7        |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.00551    |\n",
      "|    value_loss           | 85.7        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 192      |\n",
      "|    ep_rew_mean     | 42.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 157      |\n",
      "|    iterations      | 91       |\n",
      "|    time_elapsed    | 1184     |\n",
      "|    total_timesteps | 186368   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 195        |\n",
      "|    ep_rew_mean          | 46.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 157        |\n",
      "|    iterations           | 92         |\n",
      "|    time_elapsed         | 1195       |\n",
      "|    total_timesteps      | 188416     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.24727812 |\n",
      "|    clip_fraction        | 0.0225     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.667     |\n",
      "|    explained_variance   | 0.952      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 12.8       |\n",
      "|    n_updates            | 910        |\n",
      "|    policy_gradient_loss | -0.0261    |\n",
      "|    value_loss           | 37.8       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=-79.01 +/- 108.34\n",
      "Episode length: 441.48 +/- 147.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 441         |\n",
      "|    mean_reward          | -79         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 190000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.087416634 |\n",
      "|    clip_fraction        | 0.0112      |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.777      |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.42        |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 39.3        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 196      |\n",
      "|    ep_rew_mean     | 48.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 157      |\n",
      "|    iterations      | 93       |\n",
      "|    time_elapsed    | 1208     |\n",
      "|    total_timesteps | 190464   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 196        |\n",
      "|    ep_rew_mean          | 52.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 157        |\n",
      "|    iterations           | 94         |\n",
      "|    time_elapsed         | 1219       |\n",
      "|    total_timesteps      | 192512     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14807664 |\n",
      "|    clip_fraction        | 0.0302     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.756     |\n",
      "|    explained_variance   | 0.925      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 26.4       |\n",
      "|    n_updates            | 930        |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    value_loss           | 59.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 193         |\n",
      "|    ep_rew_mean          | 54.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 1231        |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.071243964 |\n",
      "|    clip_fraction        | 0.0181      |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.951      |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.65        |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 46.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=-77.73 +/- 76.49\n",
      "Episode length: 463.74 +/- 220.75\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 464        |\n",
      "|    mean_reward          | -77.7      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 195000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02125818 |\n",
      "|    clip_fraction        | 9.77e-05   |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.741     |\n",
      "|    explained_variance   | 0.719      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 43.9       |\n",
      "|    n_updates            | 950        |\n",
      "|    policy_gradient_loss | -0.00387   |\n",
      "|    value_loss           | 573        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 193      |\n",
      "|    ep_rew_mean     | 59       |\n",
      "| time/              |          |\n",
      "|    fps             | 157      |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 1244     |\n",
      "|    total_timesteps | 196608   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 193        |\n",
      "|    ep_rew_mean          | 59.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 158        |\n",
      "|    iterations           | 97         |\n",
      "|    time_elapsed         | 1254       |\n",
      "|    total_timesteps      | 198656     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05947735 |\n",
      "|    clip_fraction        | 0.00903    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.815     |\n",
      "|    explained_variance   | 0.652      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 12         |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | -0.00861   |\n",
      "|    value_loss           | 175        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-48.49 +/- 86.92\n",
      "Episode length: 366.80 +/- 247.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 367         |\n",
      "|    mean_reward          | -48.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 200000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034315236 |\n",
      "|    clip_fraction        | 0.00752     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.741      |\n",
      "|    explained_variance   | 0.828       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 59.4        |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0038     |\n",
      "|    value_loss           | 151         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 193      |\n",
      "|    ep_rew_mean     | 63.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 158      |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 1268     |\n",
      "|    total_timesteps | 200704   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 192        |\n",
      "|    ep_rew_mean          | 59.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 158        |\n",
      "|    iterations           | 99         |\n",
      "|    time_elapsed         | 1279       |\n",
      "|    total_timesteps      | 202752     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07785636 |\n",
      "|    clip_fraction        | 0.00771    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.726     |\n",
      "|    explained_variance   | 0.87       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 22.9       |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    value_loss           | 72.4       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 189        |\n",
      "|    ep_rew_mean          | 60.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 158        |\n",
      "|    iterations           | 100        |\n",
      "|    time_elapsed         | 1293       |\n",
      "|    total_timesteps      | 204800     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07806361 |\n",
      "|    clip_fraction        | 0.0135     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.6       |\n",
      "|    explained_variance   | 0.945      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 33.1       |\n",
      "|    n_updates            | 990        |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    value_loss           | 66.2       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=205000, episode_reward=-54.98 +/- 107.15\n",
      "Episode length: 455.02 +/- 222.39\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 455        |\n",
      "|    mean_reward          | -55        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 205000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10819811 |\n",
      "|    clip_fraction        | 0.0107     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.623     |\n",
      "|    explained_variance   | 0.909      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 12.9       |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | -0.0172    |\n",
      "|    value_loss           | 93.3       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 187      |\n",
      "|    ep_rew_mean     | 51.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 157      |\n",
      "|    iterations      | 101      |\n",
      "|    time_elapsed    | 1309     |\n",
      "|    total_timesteps | 206848   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 184         |\n",
      "|    ep_rew_mean          | 43.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 102         |\n",
      "|    time_elapsed         | 1322        |\n",
      "|    total_timesteps      | 208896      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021508502 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.659      |\n",
      "|    explained_variance   | 0.526       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 526         |\n",
      "|    n_updates            | 1010        |\n",
      "|    policy_gradient_loss | -0.00503    |\n",
      "|    value_loss           | 1.21e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=-111.26 +/- 100.59\n",
      "Episode length: 526.08 +/- 195.48\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 526        |\n",
      "|    mean_reward          | -111       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 210000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04349161 |\n",
      "|    clip_fraction        | 0.00171    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.687     |\n",
      "|    explained_variance   | 0.611      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 808        |\n",
      "|    n_updates            | 1020       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    value_loss           | 1.69e+03   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 185      |\n",
      "|    ep_rew_mean     | 36.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 157      |\n",
      "|    iterations      | 103      |\n",
      "|    time_elapsed    | 1337     |\n",
      "|    total_timesteps | 210944   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 185         |\n",
      "|    ep_rew_mean          | 40.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 104         |\n",
      "|    time_elapsed         | 1349        |\n",
      "|    total_timesteps      | 212992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025691677 |\n",
      "|    clip_fraction        | 0.00132     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.677      |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 56.6        |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | -0.00666    |\n",
      "|    value_loss           | 295         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=215000, episode_reward=-158.70 +/- 47.14\n",
      "Episode length: 626.16 +/- 145.06\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 626        |\n",
      "|    mean_reward          | -159       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 215000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05852741 |\n",
      "|    clip_fraction        | 0.004      |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.722     |\n",
      "|    explained_variance   | 0.843      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 36.8       |\n",
      "|    n_updates            | 1040       |\n",
      "|    policy_gradient_loss | -0.00848   |\n",
      "|    value_loss           | 124        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 184      |\n",
      "|    ep_rew_mean     | 32.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 157      |\n",
      "|    iterations      | 105      |\n",
      "|    time_elapsed    | 1364     |\n",
      "|    total_timesteps | 215040   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 184         |\n",
      "|    ep_rew_mean          | 35.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 106         |\n",
      "|    time_elapsed         | 1376        |\n",
      "|    total_timesteps      | 217088      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054259755 |\n",
      "|    clip_fraction        | 0.0183      |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.676      |\n",
      "|    explained_variance   | 0.711       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 218         |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | -0.00801    |\n",
      "|    value_loss           | 837         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 182        |\n",
      "|    ep_rew_mean          | 34.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 158        |\n",
      "|    iterations           | 107        |\n",
      "|    time_elapsed         | 1386       |\n",
      "|    total_timesteps      | 219136     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16998196 |\n",
      "|    clip_fraction        | 0.0241     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.733     |\n",
      "|    explained_variance   | 0.573      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 210        |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | -0.00889   |\n",
      "|    value_loss           | 362        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-3.39 +/- 100.82\n",
      "Episode length: 700.86 +/- 358.31\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 701        |\n",
      "|    mean_reward          | -3.39      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 220000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12128906 |\n",
      "|    clip_fraction        | 0.0064     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.64      |\n",
      "|    explained_variance   | 0.706      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 20.4       |\n",
      "|    n_updates            | 1070       |\n",
      "|    policy_gradient_loss | -0.00595   |\n",
      "|    value_loss           | 363        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 182      |\n",
      "|    ep_rew_mean     | 28.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 157      |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 1401     |\n",
      "|    total_timesteps | 221184   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 184          |\n",
      "|    ep_rew_mean          | 33.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 158          |\n",
      "|    iterations           | 109          |\n",
      "|    time_elapsed         | 1412         |\n",
      "|    total_timesteps      | 223232       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055837436 |\n",
      "|    clip_fraction        | 0.000195     |\n",
      "|    clip_range           | 0.999        |\n",
      "|    entropy_loss         | -0.77        |\n",
      "|    explained_variance   | 0.663        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 184          |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.00345     |\n",
      "|    value_loss           | 695          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=225000, episode_reward=27.13 +/- 118.82\n",
      "Episode length: 616.68 +/- 361.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 617         |\n",
      "|    mean_reward          | 27.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 225000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036025725 |\n",
      "|    clip_fraction        | 0.00269     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.748      |\n",
      "|    explained_variance   | 0.737       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 57.3        |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | -0.00256    |\n",
      "|    value_loss           | 311         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 184      |\n",
      "|    ep_rew_mean     | 46.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 157      |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 1426     |\n",
      "|    total_timesteps | 225280   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 186        |\n",
      "|    ep_rew_mean          | 50.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 158        |\n",
      "|    iterations           | 111        |\n",
      "|    time_elapsed         | 1437       |\n",
      "|    total_timesteps      | 227328     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11472268 |\n",
      "|    clip_fraction        | 0.00596    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.676     |\n",
      "|    explained_variance   | 0.817      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 237        |\n",
      "|    n_updates            | 1100       |\n",
      "|    policy_gradient_loss | -0.00818   |\n",
      "|    value_loss           | 162        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 184        |\n",
      "|    ep_rew_mean          | 58         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 158        |\n",
      "|    iterations           | 112        |\n",
      "|    time_elapsed         | 1448       |\n",
      "|    total_timesteps      | 229376     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06717868 |\n",
      "|    clip_fraction        | 0.00986    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.834     |\n",
      "|    explained_variance   | 0.499      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 239        |\n",
      "|    n_updates            | 1110       |\n",
      "|    policy_gradient_loss | -0.00615   |\n",
      "|    value_loss           | 1.21e+03   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=102.22 +/- 115.58\n",
      "Episode length: 488.70 +/- 319.31\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 489        |\n",
      "|    mean_reward          | 102        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 230000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07740754 |\n",
      "|    clip_fraction        | 0.0221     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.801     |\n",
      "|    explained_variance   | 0.837      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 123        |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    value_loss           | 179        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 184      |\n",
      "|    ep_rew_mean     | 59.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 158      |\n",
      "|    iterations      | 113      |\n",
      "|    time_elapsed    | 1461     |\n",
      "|    total_timesteps | 231424   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 185         |\n",
      "|    ep_rew_mean          | 66.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 114         |\n",
      "|    time_elapsed         | 1476        |\n",
      "|    total_timesteps      | 233472      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.110990584 |\n",
      "|    clip_fraction        | 0.0121      |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.615      |\n",
      "|    explained_variance   | 0.788       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.3        |\n",
      "|    n_updates            | 1130        |\n",
      "|    policy_gradient_loss | -0.0068     |\n",
      "|    value_loss           | 195         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=235000, episode_reward=40.12 +/- 113.67\n",
      "Episode length: 522.82 +/- 360.79\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 523        |\n",
      "|    mean_reward          | 40.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 235000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05985061 |\n",
      "|    clip_fraction        | 0.017      |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.861     |\n",
      "|    explained_variance   | 0.861      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 37.9       |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0122    |\n",
      "|    value_loss           | 98.3       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 183      |\n",
      "|    ep_rew_mean     | 62.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 157      |\n",
      "|    iterations      | 115      |\n",
      "|    time_elapsed    | 1494     |\n",
      "|    total_timesteps | 235520   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 185         |\n",
      "|    ep_rew_mean          | 63.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 116         |\n",
      "|    time_elapsed         | 1510        |\n",
      "|    total_timesteps      | 237568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051049966 |\n",
      "|    clip_fraction        | 0.00352     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.801      |\n",
      "|    explained_variance   | 0.721       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41          |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | -0.00377    |\n",
      "|    value_loss           | 533         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 185        |\n",
      "|    ep_rew_mean          | 77.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 157        |\n",
      "|    iterations           | 117        |\n",
      "|    time_elapsed         | 1526       |\n",
      "|    total_timesteps      | 239616     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06528373 |\n",
      "|    clip_fraction        | 0.00693    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.74      |\n",
      "|    explained_variance   | 0.823      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 31.1       |\n",
      "|    n_updates            | 1160       |\n",
      "|    policy_gradient_loss | -0.00896   |\n",
      "|    value_loss           | 70.5       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=28.57 +/- 124.18\n",
      "Episode length: 584.52 +/- 360.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 585         |\n",
      "|    mean_reward          | 28.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 240000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057543218 |\n",
      "|    clip_fraction        | 0.0082      |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.775      |\n",
      "|    explained_variance   | 0.724       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 508         |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | -0.00589    |\n",
      "|    value_loss           | 599         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 182      |\n",
      "|    ep_rew_mean     | 80.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 156      |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 1544     |\n",
      "|    total_timesteps | 241664   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 184        |\n",
      "|    ep_rew_mean          | 69.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 156        |\n",
      "|    iterations           | 119        |\n",
      "|    time_elapsed         | 1557       |\n",
      "|    total_timesteps      | 243712     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07090818 |\n",
      "|    clip_fraction        | 0.0128     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.828     |\n",
      "|    explained_variance   | 0.807      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 129        |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.00758   |\n",
      "|    value_loss           | 251        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=245000, episode_reward=70.71 +/- 119.61\n",
      "Episode length: 535.78 +/- 336.49\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 536        |\n",
      "|    mean_reward          | 70.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 245000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05877452 |\n",
      "|    clip_fraction        | 0.00908    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.666     |\n",
      "|    explained_variance   | 0.824      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.37       |\n",
      "|    n_updates            | 1190       |\n",
      "|    policy_gradient_loss | -0.0056    |\n",
      "|    value_loss           | 99.3       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 184      |\n",
      "|    ep_rew_mean     | 66.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 156      |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 1572     |\n",
      "|    total_timesteps | 245760   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 184        |\n",
      "|    ep_rew_mean          | 60.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 156        |\n",
      "|    iterations           | 121        |\n",
      "|    time_elapsed         | 1584       |\n",
      "|    total_timesteps      | 247808     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08370276 |\n",
      "|    clip_fraction        | 0.0193     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.784     |\n",
      "|    explained_variance   | 0.765      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 148        |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.00887   |\n",
      "|    value_loss           | 315        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 180        |\n",
      "|    ep_rew_mean          | 64.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 156        |\n",
      "|    iterations           | 122        |\n",
      "|    time_elapsed         | 1594       |\n",
      "|    total_timesteps      | 249856     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05204446 |\n",
      "|    clip_fraction        | 0.00601    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.692     |\n",
      "|    explained_variance   | 0.685      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 72.9       |\n",
      "|    n_updates            | 1210       |\n",
      "|    policy_gradient_loss | -0.01      |\n",
      "|    value_loss           | 212        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=-8.68 +/- 109.93\n",
      "Episode length: 588.16 +/- 376.85\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 588        |\n",
      "|    mean_reward          | -8.68      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 250000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13000178 |\n",
      "|    clip_fraction        | 0.025      |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -0.802     |\n",
      "|    explained_variance   | 0.767      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 59         |\n",
      "|    n_updates            | 1220       |\n",
      "|    policy_gradient_loss | -0.0152    |\n",
      "|    value_loss           | 341        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 181      |\n",
      "|    ep_rew_mean     | 61.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 156      |\n",
      "|    iterations      | 123      |\n",
      "|    time_elapsed    | 1609     |\n",
      "|    total_timesteps | 251904   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 183         |\n",
      "|    ep_rew_mean          | 58.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 124         |\n",
      "|    time_elapsed         | 1621        |\n",
      "|    total_timesteps      | 253952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062467687 |\n",
      "|    clip_fraction        | 0.00444     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.741      |\n",
      "|    explained_variance   | 0.613       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.6        |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | -0.00369    |\n",
      "|    value_loss           | 332         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=255000, episode_reward=-86.73 +/- 42.12\n",
      "Episode length: 691.10 +/- 382.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 691         |\n",
      "|    mean_reward          | -86.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 255000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061881065 |\n",
      "|    clip_fraction        | 0.0232      |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.761      |\n",
      "|    explained_variance   | 0.648       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.3        |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.00751    |\n",
      "|    value_loss           | 454         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 181      |\n",
      "|    ep_rew_mean     | 60.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 156      |\n",
      "|    iterations      | 125      |\n",
      "|    time_elapsed    | 1637     |\n",
      "|    total_timesteps | 256000   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 181         |\n",
      "|    ep_rew_mean          | 53.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 126         |\n",
      "|    time_elapsed         | 1647        |\n",
      "|    total_timesteps      | 258048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.102066234 |\n",
      "|    clip_fraction        | 0.00381     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -0.67       |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 174         |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 214         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 52\u001b[0m\n\u001b[1;32m     40\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     41\u001b[0m             env\u001b[38;5;241m=\u001b[39mtrain_env,\n\u001b[1;32m     42\u001b[0m             n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m             clip_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.999\u001b[39m,\n\u001b[1;32m     46\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# 3 Layers with 64 neurons each policy network both\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Train the model and use the evaluation callback to save the best model.\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:218\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 218\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:207\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 59\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m~/Documents/pi-optimal/pi_optimal/utils/gym_wrapper/model_based_env.py:106\u001b[0m, in \u001b[0;36mModelBasedEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mdataset_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestep_column\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mdataset_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestep_column\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Create new dataset\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m inf_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTimeseriesDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_processors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_inference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m state_input, action_input, _, _ \u001b[38;5;241m=\u001b[39m inf_dataset[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    109\u001b[0m state_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(state_input, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/pi-optimal/pi_optimal/datasets/timeseries_dataset.py:99\u001b[0m, in \u001b[0;36mTimeseriesDataset.__init__\u001b[0;34m(self, df, dataset_config, unit_index, timestep_column, reward_column, state_columns, action_columns, lookback_timesteps, forecast_timesteps, train_processors, is_inference, noise_intensity_on_past_states, verbose, use_padding)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransforming features...\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROCESS\u001b[39m\u001b[38;5;124m\"\u001b[39m, indent_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)    \n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_features(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_processors)\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_processors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_start_index)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_episode_length \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(np\u001b[38;5;241m.\u001b[39mdiff(np\u001b[38;5;241m.\u001b[39mr_[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_end_index]))\n",
      "File \u001b[0;32m~/Documents/pi-optimal/pi_optimal/datasets/timeseries_dataset.py:229\u001b[0m, in \u001b[0;36mTimeseriesDataset.transform_features\u001b[0;34m(self, feature_type, train_processors)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_config[feature_type]:\n\u001b[1;32m    228\u001b[0m     feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_config[feature_type][feature_idx]\n\u001b[0;32m--> 229\u001b[0m     transformed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform_single_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_processors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m transformed\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    231\u001b[0m         num_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/pi-optimal/pi_optimal/datasets/timeseries_dataset.py:291\u001b[0m, in \u001b[0;36mTimeseriesDataset._transform_single_feature\u001b[0;34m(self, feature, train_processor)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_processor \u001b[38;5;129;01mand\u001b[39;00m processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     processor\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[name]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    290\u001b[0m transformed \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m processor\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[name]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    294\u001b[0m )\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformed, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# For sparse matrices\u001b[39;00m\n\u001b[1;32m    297\u001b[0m     transformed \u001b[38;5;241m=\u001b[39m transformed\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:1023\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;124;03m    Transform X using one-hot encoding.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;124;03m        returned.\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1023\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     transform_output \u001b[38;5;241m=\u001b[39m _get_output_config(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdense\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m transform_output \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse_output:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:1751\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[0;32m-> 1751\u001b[0m tags \u001b[38;5;241m=\u001b[39m \u001b[43mget_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tags\u001b[38;5;241m.\u001b[39mrequires_fit \u001b[38;5;129;01mand\u001b[39;00m attributes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/sklearn/utils/_tags.py:393\u001b[0m, in \u001b[0;36mget_tags\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_tags\u001b[39m(estimator) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tags:\n\u001b[1;32m    368\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get estimator tags.\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m    :class:`~sklearn.BaseEstimator` provides the estimator tags machinery.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m        The estimator tags.\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m     tag_provider \u001b[38;5;241m=\u001b[39m \u001b[43m_find_tags_provider\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tag_provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_tags__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;66;03m# TODO(1.7): turn the warning into an error\u001b[39;00m\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/sklearn/utils/_tags.py:325\u001b[0m, in \u001b[0;36m_find_tags_provider\u001b[0;34m(estimator, warn)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m klass \u001b[38;5;129;01min\u001b[39;00m mro_model:\n\u001b[1;32m    324\u001b[0m     tags_provider \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_more_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mklass\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    326\u001b[0m         tags_provider\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_more_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_get_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import gymnasium\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "# Monkey-patch gym to include a __version__ attribute if it's missing.\n",
    "\n",
    "# Set up log folder for monitoring\n",
    "log_dir = \"./logs_dir/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create the training environment and wrap it with a Monitor to log rewards.\n",
    "train_env = sim_env\n",
    "train_env = Monitor(train_env, log_dir)\n",
    "\n",
    "# Create a separate evaluation environment.\n",
    "eval_env = gymnasium.make(\"LunarLander-v3\")\n",
    "eval_env = Monitor(eval_env, log_dir)\n",
    "\n",
    "# Set up the evaluation callback. This will evaluate the model every 5000 timesteps,\n",
    "# and save the model if it achieves a new best mean reward.\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=os.path.join(log_dir, 'best_model'),\n",
    "    log_path=log_dir,\n",
    "    eval_freq=5000,\n",
    "    n_eval_episodes=50,\n",
    "    deterministic=False,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "model = PPO(\"MlpPolicy\",\n",
    "            env=train_env,\n",
    "            n_steps=2048,\n",
    "            gamma=0.99,\n",
    "            n_epochs=10,\n",
    "            clip_range=0.999,\n",
    "            verbose=1)\n",
    "\n",
    "# 3 Layers with 64 neurons each policy network both\n",
    "# \n",
    "\n",
    "# Train the model and use the evaluation callback to save the best model.\n",
    "model.learn(total_timesteps=300000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = PPO.load(os.path.join(log_dir, 'best_model/best_model.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:44:08.856 Python[7548:401244] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-28 16:44:08.856 Python[7548:401244] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 276.47738249932047\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the trained agent by running one episode and rendering it.\n",
    "env = gymnasium.make(\"LunarLander-v3\", render_mode=\"human\")   \n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "terminated = False\n",
    "total_reward = 0\n",
    "while not (done or terminated):\n",
    "    # Predict the next action using the trained policy.\n",
    "    action, _ = best_model.predict(obs, deterministic=False)\n",
    "    obs, reward, done, terminated ,_ = env.step(action)\n",
    "    total_reward += reward\n",
    "    env.render()\n",
    "\n",
    "env.close()\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 274.01828930464137\n",
      "Total reward: 23.67992769688901\n",
      "Total reward: 16.005903790004222\n",
      "Total reward: 271.4731485612306\n",
      "Total reward: 222.98293610548535\n",
      "Total reward: 271.04334930186394\n",
      "Total reward: 59.97551668656732\n",
      "Total reward: 234.0408235007612\n",
      "Total reward: 290.0745296086543\n",
      "Total reward: 226.18771030630424\n",
      "Total reward: 101.59305895282387\n",
      "Total reward: 289.3743976757225\n",
      "Total reward: 40.37370258956648\n",
      "Total reward: 258.8458143657387\n",
      "Total reward: 45.0012761598185\n",
      "Total reward: 74.40971576479845\n",
      "Total reward: 0.6525354259194933\n",
      "Total reward: 258.19833158768483\n",
      "Total reward: 70.02118823812222\n",
      "Total reward: 272.85355399447565\n",
      "Total reward: 17.713849126552276\n",
      "Total reward: 219.66683689786572\n",
      "Total reward: 279.9356451213092\n",
      "Total reward: 281.39704523955686\n",
      "Total reward: -148.46578093843607\n",
      "Total reward: 38.370883120157835\n",
      "Total reward: 48.535443095708345\n",
      "Total reward: 239.2377544720301\n",
      "Total reward: 60.58952525979842\n",
      "Total reward: 19.006608262262063\n",
      "Total reward: 43.10003950841022\n",
      "Total reward: 79.11468663241322\n",
      "Total reward: -0.2689778357324144\n",
      "Total reward: 310.4552711930779\n",
      "Total reward: 265.16405931808123\n",
      "Total reward: 127.15315702331402\n",
      "Total reward: 232.5890950813274\n",
      "Total reward: 1.8355731942899691\n",
      "Total reward: 306.6454892625423\n",
      "Total reward: 257.5156319982865\n",
      "Total reward: 252.80500425536917\n",
      "Total reward: 251.84636797913004\n",
      "Total reward: 240.6235771750109\n",
      "Total reward: 245.17804410019087\n",
      "Total reward: 22.770386471750772\n",
      "Total reward: 63.52649868287128\n",
      "Total reward: 294.87626413997407\n",
      "Total reward: 286.88842006749593\n",
      "Total reward: 285.7295144588668\n",
      "Total reward: 16.23438934957261\n",
      "Total reward: 124.12321154623655\n",
      "Total reward: 246.755505165776\n",
      "Total reward: -133.35155403326968\n",
      "Total reward: 243.22137960690716\n",
      "Total reward: 100.05627232436434\n",
      "Total reward: 228.07522464664382\n",
      "Total reward: 231.01537393436047\n",
      "Total reward: 272.7440521458758\n",
      "Total reward: 305.4857803955945\n",
      "Total reward: 245.63984395406692\n",
      "Total reward: 247.4116964431832\n",
      "Total reward: 114.7463020866053\n",
      "Total reward: 118.17914130248937\n",
      "Total reward: 126.65466201110458\n",
      "Total reward: 266.7882130572174\n",
      "Total reward: 255.72291988121842\n",
      "Total reward: 229.4166725906202\n",
      "Total reward: 27.601191381302712\n",
      "Total reward: 261.1090159519548\n",
      "Total reward: 280.67439144865915\n",
      "Total reward: 61.306970346925624\n",
      "Total reward: 62.02639152543628\n",
      "Total reward: 240.71144725052704\n",
      "Total reward: -17.977519461380275\n",
      "Total reward: 253.35150767281127\n",
      "Total reward: 284.58828869372223\n",
      "Total reward: 258.469052618276\n",
      "Total reward: 5.259152327028218\n",
      "Total reward: 230.2664175306312\n",
      "Total reward: 295.7844454554114\n",
      "Total reward: 262.19539444494285\n",
      "Total reward: 212.87250450507526\n",
      "Total reward: 273.8580989959263\n",
      "Total reward: 245.46650728229713\n",
      "Total reward: 293.5198867122043\n",
      "Total reward: 257.5735561653855\n",
      "Total reward: 40.27430634222785\n",
      "Total reward: 219.69850606242062\n",
      "Total reward: 293.2596251670021\n",
      "Total reward: 298.63688830531555\n",
      "Total reward: 258.18969676568497\n",
      "Total reward: 216.83041089519435\n",
      "Total reward: 236.9610141517984\n",
      "Total reward: 10.481418308485829\n",
      "Total reward: 273.49869472090415\n",
      "Total reward: 44.79113093730354\n",
      "Total reward: 85.30037300607749\n",
      "Total reward: 13.43429743918615\n",
      "Total reward: 140.315233665877\n",
      "Total reward: 245.27028208196077\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the trained agent by running one episode and rendering it.\n",
    "env = gymnasium.make(\"LunarLander-v3\", render_mode=\"rgb_array\")  \n",
    "all_rewards = []\n",
    "for i in range(100):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    terminated = False\n",
    "    total_reward = 0\n",
    "    while not (done or terminated):\n",
    "        # Predict the next action using the trained policy.\n",
    "        action, _ = best_model.predict(obs, deterministic=False)\n",
    "        obs, reward, done, terminated ,_ = env.step(action)\n",
    "        total_reward += reward\n",
    "        env.render()\n",
    "    all_rewards.append(total_reward)\n",
    "    print(f\"Total reward: {total_reward}\")\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173.28859287085692"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 1 model\n",
    "np.mean(all_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Level Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id='logger-container'>\n",
       "            <div class=\"logger-entry\" data-indent=\"0\" style=\"margin-left: 0px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">üìù</span>\n",
       "                <span class=\"logger-message\">No eval environment provided, will not create eval callback</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"0\" style=\"margin-left: 0px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">üìù</span>\n",
       "                <span class=\"logger-message\">Creating PPO model</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"0\" style=\"margin-left: 0px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">üìù</span>\n",
       "                <span class=\"logger-message\">Training model</span>\n",
       "            </div>\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            /* Logger Container */\n",
       "            #logger-container {\n",
       "                max-height: 600px;\n",
       "                overflow-y: auto;\n",
       "                font-family: 'Segoe UI Emoji', 'Segoe UI Symbol', monospace;\n",
       "                padding: 5px;\n",
       "                background-color: #ffffff;\n",
       "                font-size: 0.9em; /* Increased font size */\n",
       "                position: relative;\n",
       "            }\n",
       "\n",
       "            /* Individual Log Entry */\n",
       "            .logger-entry {\n",
       "                display: flex;\n",
       "                align-items: center;\n",
       "                padding: 1px 5px;\n",
       "                margin-bottom: 4px;\n",
       "                background-color: #ffffff;\n",
       "                transition: background-color 0.2s, box-shadow 0.2s;\n",
       "                white-space: pre-wrap; /* Preserve whitespace for symbols */\n",
       "                position: relative;\n",
       "            }\n",
       "\n",
       "            .logger-entry:hover {\n",
       "                background-color: #f1f3f5;\n",
       "                box-shadow: 0 2px 6px rgba(0,0,0,0.1);\n",
       "            }\n",
       "\n",
       "        \n",
       "\n",
       "            /* Connector Line styling based on indent */\n",
       "            .logger-entry[data-indent=\"1\"]::before {\n",
       "                left: 0;\n",
       "            }\n",
       "            .logger-entry[data-indent=\"2\"]::before {\n",
       "                left: 0;\n",
       "            }\n",
       "            /* Add more as needed for higher indent levels */\n",
       "\n",
       "            /* Emoji */\n",
       "            .logger-emoji {\n",
       "                margin-right: 10px;\n",
       "                font-size: 1em; /* Larger emoji size */\n",
       "                flex-shrink: 0;\n",
       "                color: inherit; /* Inherit color from parent */\n",
       "            }\n",
       "\n",
       "            /* Message */\n",
       "            .logger-message {\n",
       "                flex-grow: 1;\n",
       "                color: #212529;\n",
       "                word-break: break-word;\n",
       "            }\n",
       "        </style>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 147       |\n",
      "|    ep_rew_mean     | -2.01e+06 |\n",
      "| time/              |           |\n",
      "|    fps             | 177       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 11        |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 161           |\n",
      "|    ep_rew_mean          | -1.2e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 167           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 24            |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2118835e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.3e+11       |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | 6.9e-06       |\n",
      "|    value_loss           | 1.17e+12      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 165           |\n",
      "|    ep_rew_mean          | -1.07e+07     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 168           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 36            |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4334801e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | -5.11e-05     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.73e+09      |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.000102     |\n",
      "|    value_loss           | 6.09e+09      |\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pi_optimal.planners.online_planner import OnlinePlanner\n",
    "from pi_optimal.utils.gym_wrapper.model_based_env import ModelBasedEnv\n",
    "\n",
    "sim_env = ModelBasedEnv(models=[nn_model, nn_model2, nn_model3], dataset=dataset_train, max_episode_steps=200)\n",
    "eval_env = gymnasium.make(\"LunarLander-v3\")\n",
    "\n",
    "online_planner = OnlinePlanner(env=sim_env, eval_env=None, train_params={\"total_timesteps\": 6000}, eval_params={\"n_eval_episodes\": 50, \"eval_freq\": 5000})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing Inference from a dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For details  look in to the predict function of the planer. It takes the last observation and uses it as observation. Ensure that the dataset is set to **is_inference == True** ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 5438.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id='logger-container'>\n",
       "            <div class=\"logger-entry\" data-indent=\"0\" style=\"margin-left: 0px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚öôÔ∏è</span>\n",
       "                <span class=\"logger-message\">Initializing new dataset...</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">üìù</span>\n",
       "                <span class=\"logger-message\">Dataset has 50 rows and 13 columns.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">üìù</span>\n",
       "                <span class=\"logger-message\">Dataset has 1 episodes.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">üìù</span>\n",
       "                <span class=\"logger-message\">Dataset has 10 state features and 1 actions.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">üìù</span>\n",
       "                <span class=\"logger-message\">Using processors provided in the dataset_configuration.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚öôÔ∏è</span>\n",
       "                <span class=\"logger-message\">Transforming features...</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_0' using preprocessor 'StandardScaler() with mean 0.11 and std 0.33'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_1' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_2' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_3' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_4' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_5' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_6' using preprocessor 'None '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_7' using preprocessor 'None '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'done' using preprocessor 'None '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'reward' using preprocessor 'PowerTransformer() '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚úÖ</span>\n",
       "                <span class=\"logger-message\">Transformed actions feature 'action_0' using preprocessor 'OneHotEncoder(sparse_output=False) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"0\" style=\"margin-left: 0px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">‚ú®</span>\n",
       "                <span class=\"logger-message\">Dataset was created successfully!</span>\n",
       "            </div>\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            /* Logger Container */\n",
       "            #logger-container {\n",
       "                max-height: 600px;\n",
       "                overflow-y: auto;\n",
       "                font-family: 'Segoe UI Emoji', 'Segoe UI Symbol', monospace;\n",
       "                padding: 5px;\n",
       "                background-color: #ffffff;\n",
       "                font-size: 0.9em; /* Increased font size */\n",
       "                position: relative;\n",
       "            }\n",
       "\n",
       "            /* Individual Log Entry */\n",
       "            .logger-entry {\n",
       "                display: flex;\n",
       "                align-items: center;\n",
       "                padding: 1px 5px;\n",
       "                margin-bottom: 4px;\n",
       "                background-color: #ffffff;\n",
       "                transition: background-color 0.2s, box-shadow 0.2s;\n",
       "                white-space: pre-wrap; /* Preserve whitespace for symbols */\n",
       "                position: relative;\n",
       "            }\n",
       "\n",
       "            .logger-entry:hover {\n",
       "                background-color: #f1f3f5;\n",
       "                box-shadow: 0 2px 6px rgba(0,0,0,0.1);\n",
       "            }\n",
       "\n",
       "        \n",
       "\n",
       "            /* Connector Line styling based on indent */\n",
       "            .logger-entry[data-indent=\"1\"]::before {\n",
       "                left: 0;\n",
       "            }\n",
       "            .logger-entry[data-indent=\"2\"]::before {\n",
       "                left: 0;\n",
       "            }\n",
       "            /* Add more as needed for higher indent levels */\n",
       "\n",
       "            /* Emoji */\n",
       "            .logger-emoji {\n",
       "                margin-right: 10px;\n",
       "                font-size: 1em; /* Larger emoji size */\n",
       "                flex-shrink: 0;\n",
       "                color: inherit; /* Inherit color from parent */\n",
       "            }\n",
       "\n",
       "            /* Message */\n",
       "            .logger-message {\n",
       "                flex-grow: 1;\n",
       "                color: #212529;\n",
       "                word-break: break-word;\n",
       "            }\n",
       "        </style>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_inf = data_collector.collect(n_steps=50, max_steps_per_episode=200, env_seed=None, action_seed=None)\n",
    "\n",
    "dataset_inf = dataset_test = TimeseriesDataset(\n",
    "    df=df_inf,\n",
    "    dataset_config=dataset_config,\n",
    "    train_processors=False,\n",
    "    is_inference=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_planner.plan(dataset_inf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pi-optimal-RKvx2dB5-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
