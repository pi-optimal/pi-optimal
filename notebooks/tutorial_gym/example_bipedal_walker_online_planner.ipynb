{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment and Dependencies\n",
    "Import required libraries including NumPy, sklearn with Intel extension, and pi_optimal utilities. Configure warning suppression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Environment and Dependencies\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "#from sklearnex import patch_sklearn\n",
    "import warnings\n",
    "\n",
    "# Change directory to the parent directory\n",
    "os.chdir(\"..\")\n",
    "\n",
    "# Apply Intel extension to sklearn\n",
    "#patch_sklearn()\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import pi_optimal utilities\n",
    "from pi_optimal.utils.data_generators.gym_data_generator import GymDataGenerator\n",
    "from pi_optimal.datasets.timeseries_dataset import TimeseriesDataset\n",
    "from pi_optimal.models.sklearn.random_forest_model import RandomForest\n",
    "from pi_optimal.models.sklearn.mlp import NeuralNetwork\n",
    "from pi_optimal.evaluators.base_evaluator import BaseEvaluator\n",
    "from pi_optimal.evaluators.plotting import plot_n_step_evaluation, plot_n_step_episode_rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Gym Data Generator\n",
    "Initialize GymDataGenerator with LunarLander environment and collect training and test data with specified parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting steps: 100%|██████████| 10000/10000 [00:00<00:00, 11255.87it/s]\n",
      "Collecting steps: 100%|██████████| 5000/5000 [00:00<00:00, 11329.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create Gym Data Generator\n",
    "\n",
    "# Initialize GymDataGenerator with BipedalWalker environment\n",
    "data_collector = GymDataGenerator(env_name=\"BipedalWalker\")\n",
    "\n",
    "# Collect training data\n",
    "df_train = data_collector.collect(n_steps=10000, max_steps_per_episode=200, env_seed=None, action_seed=None)\n",
    "df_test = data_collector.collect(n_steps=5000, max_steps_per_episode=200, env_seed=None, action_seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>step</th>\n",
       "      <th>reward</th>\n",
       "      <th>done</th>\n",
       "      <th>state_0</th>\n",
       "      <th>state_1</th>\n",
       "      <th>state_2</th>\n",
       "      <th>state_3</th>\n",
       "      <th>state_4</th>\n",
       "      <th>state_5</th>\n",
       "      <th>...</th>\n",
       "      <th>state_18</th>\n",
       "      <th>state_19</th>\n",
       "      <th>state_20</th>\n",
       "      <th>state_21</th>\n",
       "      <th>state_22</th>\n",
       "      <th>state_23</th>\n",
       "      <th>action_0</th>\n",
       "      <th>action_1</th>\n",
       "      <th>action_2</th>\n",
       "      <th>action_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>-0.016000</td>\n",
       "      <td>0.091956</td>\n",
       "      <td>-0.001459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534103</td>\n",
       "      <td>0.602461</td>\n",
       "      <td>0.709149</td>\n",
       "      <td>0.885932</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.362195</td>\n",
       "      <td>0.817553</td>\n",
       "      <td>0.204113</td>\n",
       "      <td>0.879735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.030488</td>\n",
       "      <td>False</td>\n",
       "      <td>0.008967</td>\n",
       "      <td>-0.014899</td>\n",
       "      <td>0.007804</td>\n",
       "      <td>0.027074</td>\n",
       "      <td>-0.279903</td>\n",
       "      <td>-0.417461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552951</td>\n",
       "      <td>0.623721</td>\n",
       "      <td>0.734174</td>\n",
       "      <td>0.917196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.913504</td>\n",
       "      <td>0.399155</td>\n",
       "      <td>-0.606546</td>\n",
       "      <td>-0.324907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.172784</td>\n",
       "      <td>False</td>\n",
       "      <td>0.021874</td>\n",
       "      <td>0.036472</td>\n",
       "      <td>0.020428</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>-0.062662</td>\n",
       "      <td>-0.830744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.567385</td>\n",
       "      <td>0.640003</td>\n",
       "      <td>0.753339</td>\n",
       "      <td>0.941138</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.827086</td>\n",
       "      <td>0.525922</td>\n",
       "      <td>0.971741</td>\n",
       "      <td>-0.676759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.159519</td>\n",
       "      <td>False</td>\n",
       "      <td>0.034931</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>-0.006921</td>\n",
       "      <td>0.052227</td>\n",
       "      <td>-0.131982</td>\n",
       "      <td>-1.019528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571541</td>\n",
       "      <td>0.644691</td>\n",
       "      <td>0.758857</td>\n",
       "      <td>0.948032</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.598664</td>\n",
       "      <td>-0.950631</td>\n",
       "      <td>0.050628</td>\n",
       "      <td>0.476462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.081774</td>\n",
       "      <td>False</td>\n",
       "      <td>0.036350</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>-0.021020</td>\n",
       "      <td>0.033742</td>\n",
       "      <td>-0.161395</td>\n",
       "      <td>-0.056679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.574264</td>\n",
       "      <td>0.647762</td>\n",
       "      <td>0.762473</td>\n",
       "      <td>0.952549</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.657584</td>\n",
       "      <td>0.088714</td>\n",
       "      <td>-0.202445</td>\n",
       "      <td>0.023098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>77</td>\n",
       "      <td>36</td>\n",
       "      <td>-0.218375</td>\n",
       "      <td>False</td>\n",
       "      <td>0.426909</td>\n",
       "      <td>0.060835</td>\n",
       "      <td>-0.021596</td>\n",
       "      <td>-0.138653</td>\n",
       "      <td>0.262896</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.492241</td>\n",
       "      <td>0.555242</td>\n",
       "      <td>0.653568</td>\n",
       "      <td>0.816495</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.065793</td>\n",
       "      <td>0.203441</td>\n",
       "      <td>0.036619</td>\n",
       "      <td>-0.305542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>77</td>\n",
       "      <td>37</td>\n",
       "      <td>-0.171305</td>\n",
       "      <td>False</td>\n",
       "      <td>0.454696</td>\n",
       "      <td>0.055480</td>\n",
       "      <td>-0.019627</td>\n",
       "      <td>-0.169166</td>\n",
       "      <td>0.188369</td>\n",
       "      <td>-0.915699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482711</td>\n",
       "      <td>0.544492</td>\n",
       "      <td>0.640914</td>\n",
       "      <td>0.800687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.056706</td>\n",
       "      <td>-0.819509</td>\n",
       "      <td>0.335117</td>\n",
       "      <td>-0.472653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>77</td>\n",
       "      <td>38</td>\n",
       "      <td>-0.175264</td>\n",
       "      <td>False</td>\n",
       "      <td>0.475991</td>\n",
       "      <td>0.043072</td>\n",
       "      <td>-0.026802</td>\n",
       "      <td>-0.196466</td>\n",
       "      <td>0.243394</td>\n",
       "      <td>0.082573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.471564</td>\n",
       "      <td>0.531918</td>\n",
       "      <td>0.626114</td>\n",
       "      <td>0.782197</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.510836</td>\n",
       "      <td>-0.663696</td>\n",
       "      <td>0.301046</td>\n",
       "      <td>-0.918718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>77</td>\n",
       "      <td>39</td>\n",
       "      <td>-0.209152</td>\n",
       "      <td>False</td>\n",
       "      <td>0.501708</td>\n",
       "      <td>0.051331</td>\n",
       "      <td>-0.017314</td>\n",
       "      <td>-0.214434</td>\n",
       "      <td>0.210570</td>\n",
       "      <td>-0.349046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.459457</td>\n",
       "      <td>0.518261</td>\n",
       "      <td>0.610039</td>\n",
       "      <td>0.762114</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.253496</td>\n",
       "      <td>0.237579</td>\n",
       "      <td>-0.054549</td>\n",
       "      <td>-0.943965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>77</td>\n",
       "      <td>40</td>\n",
       "      <td>-0.170055</td>\n",
       "      <td>False</td>\n",
       "      <td>0.524511</td>\n",
       "      <td>0.045387</td>\n",
       "      <td>-0.019175</td>\n",
       "      <td>-0.226349</td>\n",
       "      <td>0.180713</td>\n",
       "      <td>-0.381288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446665</td>\n",
       "      <td>0.503832</td>\n",
       "      <td>0.593054</td>\n",
       "      <td>0.740896</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.339751</td>\n",
       "      <td>-0.684865</td>\n",
       "      <td>-0.204841</td>\n",
       "      <td>-0.951826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      episode  step    reward   done   state_0   state_1   state_2   state_3  \\\n",
       "0           0     0  0.000000  False  0.002747 -0.000014  0.001106 -0.016000   \n",
       "1           0     1 -0.030488  False  0.008967 -0.014899  0.007804  0.027074   \n",
       "2           0     2 -0.172784  False  0.021874  0.036472  0.020428  0.002459   \n",
       "3           0     3 -0.159519  False  0.034931  0.026900 -0.006921  0.052227   \n",
       "4           0     4 -0.081774  False  0.036350  0.002703 -0.021020  0.033742   \n",
       "...       ...   ...       ...    ...       ...       ...       ...       ...   \n",
       "9995       77    36 -0.218375  False  0.426909  0.060835 -0.021596 -0.138653   \n",
       "9996       77    37 -0.171305  False  0.454696  0.055480 -0.019627 -0.169166   \n",
       "9997       77    38 -0.175264  False  0.475991  0.043072 -0.026802 -0.196466   \n",
       "9998       77    39 -0.209152  False  0.501708  0.051331 -0.017314 -0.214434   \n",
       "9999       77    40 -0.170055  False  0.524511  0.045387 -0.019175 -0.226349   \n",
       "\n",
       "       state_4   state_5  ...  state_18  state_19  state_20  state_21  \\\n",
       "0     0.091956 -0.001459  ...  0.534103  0.602461  0.709149  0.885932   \n",
       "1    -0.279903 -0.417461  ...  0.552951  0.623721  0.734174  0.917196   \n",
       "2    -0.062662 -0.830744  ...  0.567385  0.640003  0.753339  0.941138   \n",
       "3    -0.131982 -1.019528  ...  0.571541  0.644691  0.758857  0.948032   \n",
       "4    -0.161395 -0.056679  ...  0.574264  0.647762  0.762473  0.952549   \n",
       "...        ...       ...  ...       ...       ...       ...       ...   \n",
       "9995  0.262896 -1.000000  ...  0.492241  0.555242  0.653568  0.816495   \n",
       "9996  0.188369 -0.915699  ...  0.482711  0.544492  0.640914  0.800687   \n",
       "9997  0.243394  0.082573  ...  0.471564  0.531918  0.626114  0.782197   \n",
       "9998  0.210570 -0.349046  ...  0.459457  0.518261  0.610039  0.762114   \n",
       "9999  0.180713 -0.381288  ...  0.446665  0.503832  0.593054  0.740896   \n",
       "\n",
       "      state_22  state_23  action_0  action_1  action_2  action_3  \n",
       "0          1.0       1.0  0.362195  0.817553  0.204113  0.879735  \n",
       "1          1.0       1.0 -0.913504  0.399155 -0.606546 -0.324907  \n",
       "2          1.0       1.0 -0.827086  0.525922  0.971741 -0.676759  \n",
       "3          1.0       1.0  0.598664 -0.950631  0.050628  0.476462  \n",
       "4          1.0       1.0  0.657584  0.088714 -0.202445  0.023098  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "9995       1.0       1.0  0.065793  0.203441  0.036619 -0.305542  \n",
       "9996       1.0       1.0 -0.056706 -0.819509  0.335117 -0.472653  \n",
       "9997       1.0       1.0 -0.510836 -0.663696  0.301046 -0.918718  \n",
       "9998       1.0       1.0  0.253496  0.237579 -0.054549 -0.943965  \n",
       "9999       1.0       1.0 -0.339751 -0.684865 -0.204841 -0.951826  \n",
       "\n",
       "[10000 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Dataset Parameters\n",
    "Set up dataset configuration dictionary defining features, processors, and evaluation metrics for states, actions, and rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Dataset Parameters\n",
    "\n",
    "# Define dataset configuration dictionary\n",
    "dataset_config = {\n",
    "    \"episode_column\": \"episode\",\n",
    "    \"timestep_column\": \"step\",\n",
    "    \"states\": {\n",
    "        0: {\"name\": \"state_0\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        1: {\"name\": \"state_1\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        2: {\"name\": \"state_2\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        3: {\"name\": \"state_3\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        4: {\"name\": \"state_4\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        5: {\"name\": \"state_5\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        6: {\"name\": \"state_6\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        7: {\"name\": \"state_7\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        8: {\"name\": \"state_8\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        9: {\"name\": \"state_9\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        10: {\"name\": \"state_10\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        11: {\"name\": \"state_11\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        12: {\"name\": \"state_12\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        13: {\"name\": \"state_13\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        14: {\"name\": \"state_14\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        15: {\"name\": \"state_15\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        16: {\"name\": \"state_16\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        17: {\"name\": \"state_17\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        18: {\"name\": \"state_18\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        19: {\"name\": \"state_19\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        20: {\"name\": \"state_20\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        21: {\"name\": \"state_21\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        22: {\"name\": \"state_22\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        23: {\"name\": \"state_23\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        24: {\"name\": \"done\", \"type\": \"binary\", \"processor\": None, \"evaluation_metric\": \"f1_binary\"},\n",
    "        25: {\"name\": \"reward\", \"type\": \"numerical\", \"processor\": {\"name\": \"RobustScaler\", \"params\": {\"quantile_range\": (5.0, 95.0)}}, \"evaluation_metric\": \"mae\"},\n",
    "    \n",
    "        },\n",
    "    \"actions\": {\n",
    "        0: {\"name\": \"action_0\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        1: {\"name\": \"action_1\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        2: {\"name\": \"action_2\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "        3: {\"name\": \"action_3\", \"type\": \"numerical\", \"processor\": {\"name\": \"StandardScaler\"}, \"evaluation_metric\": \"mae\"},\n",
    "    },\n",
    "    \"reward_feature_idx\": 9,\n",
    "    \"reward_vector_idx\": 9,\n",
    "    \"reward_column\": \"reward\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training and Test Datasets\n",
    "Initialize TimeseriesDataset objects with collected data, applying the configuration and setting lookback/forecast windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id='logger-container'>\n",
       "            <div class=\"logger-entry\" data-indent=\"0\" style=\"margin-left: 0px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">⚙️</span>\n",
       "                <span class=\"logger-message\">Initializing new dataset...</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">📝</span>\n",
       "                <span class=\"logger-message\">Dataset has 10000 rows and 32 columns.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">📝</span>\n",
       "                <span class=\"logger-message\">Dataset has 78 episodes.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">📝</span>\n",
       "                <span class=\"logger-message\">Dataset has 26 state features and 4 actions.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">⚙️</span>\n",
       "                <span class=\"logger-message\">Fitting feature processors...</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Processors created and fitted</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">⚙️</span>\n",
       "                <span class=\"logger-message\">Transforming features...</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_0' using preprocessor 'StandardScaler() with mean 0.29 and std 0.48'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_1' using preprocessor 'StandardScaler() with mean 0.0 and std 0.05'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_2' using preprocessor 'StandardScaler() with mean 0.03 and std 0.09'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_3' using preprocessor 'StandardScaler() with mean -0.04 and std 0.07'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_4' using preprocessor 'StandardScaler() with mean -0.03 and std 0.73'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_5' using preprocessor 'StandardScaler() with mean -0.03 and std 0.58'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_6' using preprocessor 'StandardScaler() with mean -0.06 and std 0.54'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_7' using preprocessor 'StandardScaler() with mean -0.07 and std 0.76'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_8' using preprocessor 'StandardScaler() with mean 0.45 and std 0.5'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_9' using preprocessor 'StandardScaler() with mean 0.27 and std 0.75'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_10' using preprocessor 'StandardScaler() with mean 0.0 and std 0.58'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_11' using preprocessor 'StandardScaler() with mean 0.05 and std 0.6'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_12' using preprocessor 'StandardScaler() with mean -0.04 and std 0.74'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_13' using preprocessor 'StandardScaler() with mean 0.47 and std 0.5'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_14' using preprocessor 'StandardScaler() with mean 0.31 and std 0.08'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_15' using preprocessor 'StandardScaler() with mean 0.32 and std 0.08'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_16' using preprocessor 'StandardScaler() with mean 0.33 and std 0.08'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_17' using preprocessor 'StandardScaler() with mean 0.35 and std 0.09'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_18' using preprocessor 'StandardScaler() with mean 0.38 and std 0.1'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_19' using preprocessor 'StandardScaler() with mean 0.43 and std 0.11'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_20' using preprocessor 'StandardScaler() with mean 0.5 and std 0.13'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_21' using preprocessor 'StandardScaler() with mean 0.63 and std 0.16'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_22' using preprocessor 'StandardScaler() with mean 0.81 and std 0.15'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_23' using preprocessor 'StandardScaler() with mean 1.0 and std 0.02'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'done' using preprocessor 'None with mean 1.0 and std 0.02'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'reward' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed actions feature 'action_0' using preprocessor 'StandardScaler() with mean 0.01 and std 0.58'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed actions feature 'action_1' using preprocessor 'StandardScaler() with mean 0.0 and std 0.58'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed actions feature 'action_2' using preprocessor 'StandardScaler() with mean 0.0 and std 0.58'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed actions feature 'action_3' using preprocessor 'StandardScaler() with mean 0.02 and std 0.58'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"0\" style=\"margin-left: 0px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✨</span>\n",
       "                <span class=\"logger-message\">Dataset was created successfully!</span>\n",
       "            </div>\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            /* Logger Container */\n",
       "            #logger-container {\n",
       "                max-height: 600px;\n",
       "                overflow-y: auto;\n",
       "                font-family: 'Segoe UI Emoji', 'Segoe UI Symbol', monospace;\n",
       "                padding: 5px;\n",
       "                background-color: #ffffff;\n",
       "                font-size: 0.9em; /* Increased font size */\n",
       "                position: relative;\n",
       "            }\n",
       "\n",
       "            /* Individual Log Entry */\n",
       "            .logger-entry {\n",
       "                display: flex;\n",
       "                align-items: center;\n",
       "                padding: 1px 5px;\n",
       "                margin-bottom: 4px;\n",
       "                background-color: #ffffff;\n",
       "                transition: background-color 0.2s, box-shadow 0.2s;\n",
       "                white-space: pre-wrap; /* Preserve whitespace for symbols */\n",
       "                position: relative;\n",
       "            }\n",
       "\n",
       "            .logger-entry:hover {\n",
       "                background-color: #f1f3f5;\n",
       "                box-shadow: 0 2px 6px rgba(0,0,0,0.1);\n",
       "            }\n",
       "\n",
       "        \n",
       "\n",
       "            /* Connector Line styling based on indent */\n",
       "            .logger-entry[data-indent=\"1\"]::before {\n",
       "                left: 0;\n",
       "            }\n",
       "            .logger-entry[data-indent=\"2\"]::before {\n",
       "                left: 0;\n",
       "            }\n",
       "            /* Add more as needed for higher indent levels */\n",
       "\n",
       "            /* Emoji */\n",
       "            .logger-emoji {\n",
       "                margin-right: 10px;\n",
       "                font-size: 1em; /* Larger emoji size */\n",
       "                flex-shrink: 0;\n",
       "                color: inherit; /* Inherit color from parent */\n",
       "            }\n",
       "\n",
       "            /* Message */\n",
       "            .logger-message {\n",
       "                flex-grow: 1;\n",
       "                color: #212529;\n",
       "                word-break: break-word;\n",
       "            }\n",
       "        </style>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id='logger-container'>\n",
       "            <div class=\"logger-entry\" data-indent=\"0\" style=\"margin-left: 0px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">⚙️</span>\n",
       "                <span class=\"logger-message\">Initializing new dataset...</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">📝</span>\n",
       "                <span class=\"logger-message\">Dataset has 5000 rows and 32 columns.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">📝</span>\n",
       "                <span class=\"logger-message\">Dataset has 44 episodes.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">📝</span>\n",
       "                <span class=\"logger-message\">Dataset has 26 state features and 4 actions.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">📝</span>\n",
       "                <span class=\"logger-message\">Using processors provided in the dataset_configuration.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">⚙️</span>\n",
       "                <span class=\"logger-message\">Transforming features...</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_0' using preprocessor 'StandardScaler() with mean 0.29 and std 0.48'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_1' using preprocessor 'StandardScaler() with mean 0.0 and std 0.05'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_2' using preprocessor 'StandardScaler() with mean 0.03 and std 0.09'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_3' using preprocessor 'StandardScaler() with mean -0.04 and std 0.07'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_4' using preprocessor 'StandardScaler() with mean -0.03 and std 0.73'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_5' using preprocessor 'StandardScaler() with mean -0.03 and std 0.58'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_6' using preprocessor 'StandardScaler() with mean -0.06 and std 0.54'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_7' using preprocessor 'StandardScaler() with mean -0.07 and std 0.76'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_8' using preprocessor 'StandardScaler() with mean 0.45 and std 0.5'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_9' using preprocessor 'StandardScaler() with mean 0.27 and std 0.75'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_10' using preprocessor 'StandardScaler() with mean 0.0 and std 0.58'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_11' using preprocessor 'StandardScaler() with mean 0.05 and std 0.6'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_12' using preprocessor 'StandardScaler() with mean -0.04 and std 0.74'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_13' using preprocessor 'StandardScaler() with mean 0.47 and std 0.5'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_14' using preprocessor 'StandardScaler() with mean 0.31 and std 0.08'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_15' using preprocessor 'StandardScaler() with mean 0.32 and std 0.08'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_16' using preprocessor 'StandardScaler() with mean 0.33 and std 0.08'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_17' using preprocessor 'StandardScaler() with mean 0.35 and std 0.09'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_18' using preprocessor 'StandardScaler() with mean 0.38 and std 0.1'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_19' using preprocessor 'StandardScaler() with mean 0.43 and std 0.11'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_20' using preprocessor 'StandardScaler() with mean 0.5 and std 0.13'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_21' using preprocessor 'StandardScaler() with mean 0.63 and std 0.16'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_22' using preprocessor 'StandardScaler() with mean 0.81 and std 0.15'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_23' using preprocessor 'StandardScaler() with mean 1.0 and std 0.02'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'done' using preprocessor 'None with mean 1.0 and std 0.02'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'reward' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed actions feature 'action_0' using preprocessor 'StandardScaler() with mean 0.01 and std 0.58'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed actions feature 'action_1' using preprocessor 'StandardScaler() with mean 0.0 and std 0.58'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed actions feature 'action_2' using preprocessor 'StandardScaler() with mean 0.0 and std 0.58'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed actions feature 'action_3' using preprocessor 'StandardScaler() with mean 0.02 and std 0.58'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"0\" style=\"margin-left: 0px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✨</span>\n",
       "                <span class=\"logger-message\">Dataset was created successfully!</span>\n",
       "            </div>\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            /* Logger Container */\n",
       "            #logger-container {\n",
       "                max-height: 600px;\n",
       "                overflow-y: auto;\n",
       "                font-family: 'Segoe UI Emoji', 'Segoe UI Symbol', monospace;\n",
       "                padding: 5px;\n",
       "                background-color: #ffffff;\n",
       "                font-size: 0.9em; /* Increased font size */\n",
       "                position: relative;\n",
       "            }\n",
       "\n",
       "            /* Individual Log Entry */\n",
       "            .logger-entry {\n",
       "                display: flex;\n",
       "                align-items: center;\n",
       "                padding: 1px 5px;\n",
       "                margin-bottom: 4px;\n",
       "                background-color: #ffffff;\n",
       "                transition: background-color 0.2s, box-shadow 0.2s;\n",
       "                white-space: pre-wrap; /* Preserve whitespace for symbols */\n",
       "                position: relative;\n",
       "            }\n",
       "\n",
       "            .logger-entry:hover {\n",
       "                background-color: #f1f3f5;\n",
       "                box-shadow: 0 2px 6px rgba(0,0,0,0.1);\n",
       "            }\n",
       "\n",
       "        \n",
       "\n",
       "            /* Connector Line styling based on indent */\n",
       "            .logger-entry[data-indent=\"1\"]::before {\n",
       "                left: 0;\n",
       "            }\n",
       "            .logger-entry[data-indent=\"2\"]::before {\n",
       "                left: 0;\n",
       "            }\n",
       "            /* Add more as needed for higher indent levels */\n",
       "\n",
       "            /* Emoji */\n",
       "            .logger-emoji {\n",
       "                margin-right: 10px;\n",
       "                font-size: 1em; /* Larger emoji size */\n",
       "                flex-shrink: 0;\n",
       "                color: inherit; /* Inherit color from parent */\n",
       "            }\n",
       "\n",
       "            /* Message */\n",
       "            .logger-message {\n",
       "                flex-grow: 1;\n",
       "                color: #212529;\n",
       "                word-break: break-word;\n",
       "            }\n",
       "        </style>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Training and Test Datasets\n",
    "\n",
    "# Define lookback and forecast timesteps\n",
    "LOOKBACK_TIMESTEPS = 5\n",
    "FORECAST_TIMESTEPS = 1\n",
    "\n",
    "# Initialize TimeseriesDataset objects for training and test data\n",
    "dataset_train = TimeseriesDataset(\n",
    "    df=df_train,\n",
    "    dataset_config=dataset_config,\n",
    "    lookback_timesteps=LOOKBACK_TIMESTEPS,\n",
    "    forecast_timesteps=FORECAST_TIMESTEPS,\n",
    "    train_processors=True\n",
    ")\n",
    "\n",
    "\n",
    "dataset_test = TimeseriesDataset(\n",
    "    df=df_test,\n",
    "    dataset_config=dataset_config,\n",
    "    lookback_timesteps=LOOKBACK_TIMESTEPS,\n",
    "    forecast_timesteps=FORECAST_TIMESTEPS,\n",
    "    train_processors=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Network Model\n",
    "Create and train a Neural Network model with specified hyperparameters on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90206a1ee71742a0aa8b6f7c6d0218a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training models...:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Initialize Neural Network model with specified hyperparameters\n",
    "nn_model1 = NeuralNetwork(params={\n",
    "    \"hidden_layer_sizes\": (128, 128),\n",
    "    \"alpha\": 0.01,\n",
    "    \"learning_rate_init\": 0.001,}\n",
    ")\n",
    "    \n",
    "# Train the Neural Network model on the first training dataset\n",
    "nn_model1.fit(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3236c41f62bc4d91b034d00004a941a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training models...:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Initialize Neural Network model with specified hyperparameters\n",
    "nn_model2 = NeuralNetwork(params={\n",
    "    \"hidden_layer_sizes\": (128, 128),\n",
    "    \"alpha\": 0.01,\n",
    "    \"learning_rate_init\": 0.001}\n",
    ")\n",
    "    \n",
    "# Train the Neural Network model on the first training dataset\n",
    "nn_model2.fit(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca78d09189942ee9675ecedcd49ef92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training models...:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Initialize Neural Network model with specified hyperparameters\n",
    "nn_model3 = NeuralNetwork(params={\n",
    "    \"hidden_layer_sizes\": (128, 128),\n",
    "    \"alpha\": 0.01,\n",
    "    \"learning_rate_init\": 0.001}\n",
    ")\n",
    "    \n",
    "# Train the Neural Network model on the first training dataset\n",
    "nn_model3.fit(dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low Level Workflow\n",
    "\n",
    "Here you could see how it works under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pi_optimal.utils.gym_wrapper.model_based_env import ModelBasedEnv\n",
    "\n",
    "sim_env = ModelBasedEnv(models=[nn_model1, nn_model2, nn_model3], dataset=dataset_train, max_episode_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 110, State history: [ 0.10917307 -0.03028735  0.11007358 -0.00603868 -0.85159158 -0.42067238\n",
      "  0.12535425  1.10235948  0.57490481  1.10159591 -0.28652132  0.91063872\n",
      "  0.63494533  0.04704826  0.22945851  0.23521662  0.24439466  0.25533637\n",
      "  0.27977127  0.31497341  0.37040988  0.46278045  0.63376983  1.00105393]\n",
      "Step: 111, State history: [ 0.0772852  -0.03288959  0.07016263 -0.01197309 -0.8281429   0.31645677\n",
      "  0.12015212  0.01790893  0.53604227  1.14413022  0.46617253  0.85588927\n",
      " -0.43038876 -0.32862685  0.22913386  0.23283266  0.24489528  0.2534031\n",
      "  0.27912322  0.31416133  0.36514968  0.46010266  0.63365245  1.00069178]\n",
      "Step: 112, State history: [ 0.09343471 -0.00163625  0.06434112 -0.00664197 -0.80073906  0.12971076\n",
      " -0.04222848 -0.99856567  0.11055739  1.10078093 -0.43983124  1.01253641\n",
      "  1.01877128  0.24373973  0.22993612  0.23355617  0.24148511  0.25181005\n",
      "  0.2776507   0.31425796  0.37289747  0.45769046  0.63058997  0.99921568]\n",
      "Step: 113, State history: [ 0.08184601 -0.01621496  0.05602576 -0.0242975  -0.81896287 -0.59153534\n",
      "  0.17205141  0.64929309  0.46547418  1.1232055  -0.10750651  0.90325887\n",
      "  0.78225669  0.06354606  0.22952862  0.23132751  0.24173577  0.25318744\n",
      "  0.27843041  0.31208098  0.36492595  0.45736734  0.62803081  1.00073106]\n",
      "Step: 114, State history: [ 0.10569723  0.0056297   0.04969855 -0.03048159 -0.83071649  0.12528436\n",
      " -0.05085687 -1.41052342  0.4071463   1.21972948  1.51013753  0.80157287\n",
      " -1.70841267 -0.38385072  0.22634002  0.22988514  0.24040069  0.24915936\n",
      "  0.27604541  0.31096598  0.36371154  0.46082766  0.62786608  1.00122246]\n",
      "Step: 115, State history: [ 0.12244872  0.06528725  0.02377533 -0.0373406  -0.80457398 -0.47009533\n",
      " -0.19987431 -1.22990209  0.5568136   1.17039067 -0.36208984  0.91276998\n",
      "  1.02980723 -0.20560152  0.23292616  0.23338061  0.23795873  0.25588172\n",
      "  0.27707335  0.31151092  0.36967352  0.46086901  0.62552507  0.99854894]\n",
      "Step: 116, State history: [ 0.15389041  0.0453848   0.02123531  0.00678123 -0.79352177  0.34760244\n",
      " -0.36404192 -0.39249086  0.66375899  1.14276003 -0.15857141  0.98357294\n",
      "  0.82136874 -0.11510832  0.22743341  0.23279353  0.23883176  0.2478786\n",
      "  0.27553395  0.31451405  0.36665236  0.45735001  0.62781524  0.99862581]\n",
      "Step: 117, State history: [ 0.20044535  0.05870912  0.0348682  -0.02055822 -0.79936921  0.10702236\n",
      " -0.44114473 -0.65361423  0.88752538  1.1615357   0.39552719  1.04523885\n",
      "  0.61193599 -0.60005102  0.22896101  0.23068069  0.24105595  0.25284762\n",
      "  0.27692506  0.31307559  0.3677699   0.4600303   0.62360344  0.99779284]\n",
      "Step: 118, State history: [ 0.19652199  0.01816858  0.02246914  0.02367086 -0.79320119 -0.25153325\n",
      " -0.27123042  0.72920487  1.30158636  1.1045942  -0.7298486   0.9543481\n",
      "  0.34809884 -0.32619024  0.22990395  0.23189374  0.2388513   0.24899991\n",
      "  0.2796254   0.31300899  0.36824929  0.45888376  0.62789396  0.99937409]\n",
      "Step: 119, State history: [ 0.203903    0.01462541  0.03545911 -0.0311017  -0.83768555 -0.27852391\n",
      " -0.47531654 -1.72683447  1.30651813  1.1720848   0.23573956  1.05693409\n",
      " -0.02642905 -0.44599429  0.23386381  0.22211039  0.23863272  0.25448683\n",
      "  0.27511171  0.30868421  0.36677151  0.45902005  0.63515357  0.99979818]\n",
      "Step: 120, State history: [ 0.20032802 -0.01308613  0.05229777 -0.03485934 -0.90077722 -0.90672982\n",
      " -0.47058592 -0.54070482  0.80894439  1.1199462  -0.20352934  1.0152732\n",
      "  0.35791359 -0.53725901  0.22780452  0.23396773  0.23956413  0.25423909\n",
      "  0.27311854  0.31056514  0.36832961  0.45854888  0.62426396  1.00023461]\n",
      "Step: 121, State history: [ 0.17641518 -0.01461208  0.03751669 -0.03610014 -0.88997644 -0.43155621\n",
      " -0.61054814 -0.83834415 -0.77443182  1.15460629  0.10735736  0.92947661\n",
      " -0.73323947 -0.14676014  0.22412322  0.22860192  0.23845363  0.25120418\n",
      "  0.27245951  0.30625119  0.36212505  0.45193758  0.62092912  1.000798  ]\n",
      "Step: 122, State history: [ 0.18124318  0.02866424  0.00865557 -0.01392255 -0.88728216  0.26561195\n",
      " -0.67270333 -0.40463449 -0.962791    1.00647963 -1.6103827   1.07259076\n",
      "  1.15647886  0.92785814  0.22651495  0.22871868  0.23587616  0.25036186\n",
      "  0.27281421  0.30680542  0.36582749  0.45311271  0.62198317  1.00094238]\n",
      "Step: 123, State history: [ 0.2135418  -0.00592724  0.01127804 -0.06032598 -0.80721245  0.61999854\n",
      " -0.61935773 -0.23964832 -0.14578275  0.99339132 -0.28472393  1.02843872\n",
      "  1.57253421  1.42847023  0.22691386  0.22546007  0.23545547  0.25199777\n",
      "  0.27170709  0.30375119  0.35783716  0.45198514  0.62018082  0.99857106]\n",
      "Step: 124, State history: [ 0.18816261  0.05163564 -0.01357695 -0.01738109 -0.88788416 -0.20218004\n",
      " -0.77146382 -0.23738878  0.5219853   1.0345219  -0.18312511  1.06974992\n",
      " -0.97016782  0.72378389  0.22368109  0.22421869  0.23359523  0.24579681\n",
      "  0.26968775  0.3019989   0.3549171   0.45433196  0.60791517  0.99983432]\n",
      "Step: 125, State history: [ 0.20952077  0.05357412  0.00912513  0.03067478 -0.93123765 -0.18637287\n",
      " -0.61049067  0.87214803  0.53084428  1.03819638  0.34692027  0.95069919\n",
      " -1.84794289  0.89644785  0.22191121  0.22805245  0.23259057  0.24941685\n",
      "  0.26659283  0.30456276  0.36258444  0.44708579  0.60885751  1.00218121]\n",
      "Step: 126, State history: [ 0.26193636  0.04597369  0.01621092  0.02891109 -0.929191   -0.1306406\n",
      " -0.54957618  0.37671445  0.57652835  1.10725031  0.31316598  0.68670227\n",
      " -0.63571524  1.28589189  0.22700589  0.22828013  0.23077514  0.24912256\n",
      "  0.27163888  0.30975912  0.362354    0.46080924  0.62885354  1.00234941]\n",
      "Step: 127, State history: [ 0.23771975 -0.03689764 -0.01703592  0.04816272 -0.90035489  0.63129207\n",
      " -0.43371893  0.37796588  0.84774133  1.15173263  0.80336669  0.76970204\n",
      "  0.72202567  0.26329653  0.22363307  0.22888243  0.23447127  0.24794678\n",
      "  0.27000879  0.31451656  0.36611239  0.45362625  0.62209377  1.00052578]\n",
      "Step: 128, State history: [ 2.37257077e-01 -1.52496274e-02 -9.92782867e-03  3.85998224e-02\n",
      " -8.86904485e-01 -9.03963085e-02 -3.28832590e-01  1.68439837e+00\n",
      "  8.45354923e-01  1.14689719e+00  6.38846499e-04  9.12400461e-01\n",
      "  3.21763021e-01  1.88508709e-01  2.30861868e-01  2.31431553e-01\n",
      "  2.39028604e-01  2.55479765e-01  2.75638011e-01  3.13735778e-01\n",
      "  3.66182190e-01  4.60035987e-01  6.20904537e-01  9.99709529e-01]\n",
      "Step: 129, State history: [ 0.19405123 -0.03718013 -0.00385519  0.05355936 -0.88199135  0.25593447\n",
      " -0.17915205  0.1811804   1.11078368  1.1648741   0.26089657  0.62209481\n",
      " -1.58846281  0.5912497   0.23020417  0.23415241  0.24072426  0.25547896\n",
      "  0.27654873  0.3144744   0.37040217  0.4644466   0.63209743  0.99981867]\n",
      "Step: 130, State history: [ 0.18306337 -0.05336171 -0.0113314   0.07088543 -0.85467349  0.3979005\n",
      " -0.11012754  0.69812115  0.41186681  1.1464633  -0.27679034  0.80411315\n",
      "  1.37465707  1.20381403  0.23696074  0.23318948  0.24293096  0.2606328\n",
      "  0.28286666  0.31734109  0.37902467  0.46903909  0.64663443  0.99987581]\n",
      "Step: 131, State history: [ 0.18450598 -0.02166655 -0.03408885  0.0116107  -0.85885084 -0.16376305\n",
      " -0.10478086  0.15000439  0.06178786  1.0938648  -0.37890537  0.92586311\n",
      "  1.72470155  0.48816565  0.23533906  0.23890285  0.24747739  0.25994338\n",
      "  0.28450255  0.32202531  0.37761251  0.47551926  0.64590348  1.00287718]\n",
      "Step: 132, State history: [ 0.14240957 -0.06100343 -0.01524349 -0.01802516 -0.85547403  0.12361529\n",
      "  0.17593456  0.95077949  0.73686758  1.10147707 -0.08645475  0.9166421\n",
      " -0.60721295  0.00186965  0.23354013  0.23750165  0.24821515  0.2578904\n",
      "  0.28631339  0.32062081  0.37683762  0.47518196  0.65210287  1.00222724]\n",
      "Step: 133, State history: [ 0.13092504 -0.05210609  0.0055968   0.00621683 -0.85072017 -0.04141058\n",
      "  0.22801638 -0.21597197  0.34643309  1.11865228  0.30159248  0.89968725\n",
      " -0.19149324  0.4875958   0.23459655  0.23555032  0.24760788  0.2609539\n",
      "  0.2858635   0.32040139  0.3804028   0.46424522  0.64439443  1.00204897]\n",
      "Step: 134, State history: [ 0.10003765 -0.0631374   0.00549542  0.01193895 -0.85890453 -0.36015594\n",
      "  0.14566461  0.22415673 -0.28328207  1.11461402 -0.0277468   0.90238718\n",
      "  0.61580725  1.07207463  0.23529587  0.24074042  0.24942248  0.25726431\n",
      "  0.28592676  0.3188551   0.38127066  0.46661781  0.64606123  1.00245171]\n",
      "Step: 135, State history: [ 0.10195963 -0.05434455 -0.04286382  0.03171597 -0.79988724  1.00113381\n",
      " -0.00723959 -1.17496468 -0.38232254  1.13795269  0.25401492  0.8462412\n",
      " -0.35013044  1.32285455  0.23533534  0.23788145  0.24747085  0.26397661\n",
      "  0.28888786  0.32651058  0.37905582  0.47371694  0.64948132  1.00041029]\n",
      "Step: 136, State history: [ 0.07527862 -0.04862302 -0.03613738  0.03475227 -0.68809279  0.52582501\n",
      " -0.24479563 -1.40048592  0.49769004  1.08486856 -0.56239274  0.90231681\n",
      "  0.39321319  1.39791903  0.23840402  0.24073289  0.25229891  0.26416678\n",
      "  0.287613    0.32759807  0.38539436  0.47872106  0.66156716  1.00155278]\n",
      "Step: 137, State history: [ 0.08290412  0.02837498  0.00421808  0.02739143 -0.77029321 -1.6644485\n",
      " -0.01703961  0.85430334  0.51071064  1.0912076   0.07404384  0.68883155\n",
      " -1.17310542  0.87775681  0.24295807  0.24574723  0.25770219  0.26882698\n",
      "  0.29520922  0.32848759  0.39623167  0.49755486  0.6642827   0.99883402]\n",
      "Step: 138, State history: [ 0.08677856 -0.03448984 -0.00281279  0.02607292 -0.81552105 -0.60377906\n",
      "  0.21470608  0.45598858 -0.46584204  1.03774338 -0.84959598  0.78826106\n",
      "  1.52345247  1.28505516  0.24635329  0.24873947  0.25559802  0.2716587\n",
      "  0.29865811  0.34172353  0.39607767  0.50009705  0.67335472  1.0013916 ]\n",
      "Step: 139, State history: [ 0.06750915 -0.05236152 -0.01185763  0.02267889 -0.79043987  0.30893035\n",
      "  0.22085187 -0.3151411   0.29078461  1.06049927  0.40490096  0.9566078\n",
      "  0.07611379 -0.31054018  0.25266183  0.25069456  0.26118556  0.27318961\n",
      "  0.29940277  0.33528561  0.40052281  0.50654739  0.67884841  1.00027184]\n",
      "Step: 140, State history: [ 0.05212316 -0.09434321 -0.0094284   0.00548669 -0.80366281  0.81879563\n",
      "  0.23731563  0.58818483  0.56432488  1.16818173  1.05450942  0.81574875\n",
      " -0.06234302 -0.59494462  0.24938209  0.25227509  0.26045885  0.2808454\n",
      "  0.29969898  0.33953211  0.39987617  0.49640486  0.68159928  1.00112591]\n",
      "Step: 141, State history: [-0.07345355 -0.09943314  0.0054748  -0.01082707 -0.63372048  0.65301828\n",
      "  0.30448973  0.47999847  0.70225055  1.19027576  0.49354798  0.93936856\n",
      "  1.05944417  0.06501103  0.25409332  0.24727329  0.26141082  0.27771136\n",
      "  0.29880532  0.34158647  0.40291994  0.50066291  0.68816072  0.99978315]\n",
      "Step: 142, State history: [-0.07979569 -0.02776543  0.02767982  0.04577383 -0.61470227  0.76753048\n",
      "  0.20639677 -0.41045162 -0.37130997  1.14009692  0.80787338  0.76239097\n",
      " -0.92004789 -0.00189115  0.2512437   0.24966445  0.25580195  0.27422773\n",
      "  0.30221779  0.33845231  0.40110662  0.49856743  0.67924874  0.99993237]\n",
      "Step: 143, State history: [ 0.04317534  0.05918477 -0.10849315  0.08900393 -0.29008663  1.14725792\n",
      "  0.42081876  0.31629415 -1.18801956  1.22173899  0.81433088  0.66133071\n",
      " -2.13868498  1.4944735   0.25233089  0.27052647  0.28372799  0.28531078\n",
      "  0.34394822  0.3663248   0.40942825  0.5550978   0.72091789  0.99487663]\n",
      "Step: 144, State history: [ 0.21631337  0.09815199 -0.0698129   0.18400388 -0.37295187  0.13266653\n",
      "  0.33614967  2.32388971  1.69983084  1.21239534 -1.13386557  0.35079452\n",
      "  1.27077226 -1.14593761  0.2842263   0.3035465   0.31387113  0.33817244\n",
      "  0.32978884  0.40258071  0.42325045  0.54002396  0.75626082  0.99755565]\n",
      "Step: 145, State history: [ 0.15636582  0.08638312 -0.0215212   0.17902513 -0.10403532 -0.67030381\n",
      "  0.18744278 -1.26841794  2.23348568  0.96075879 -1.17386737  0.47581236\n",
      "  0.2421035   0.08039286  0.29053505  0.30733967  0.31094787  0.33408929\n",
      "  0.3434835   0.3995459   0.45618587  0.60005835  0.82568627  1.00610875]\n",
      "Step: 146, State history: [ 0.24416415  0.03689101 -0.05986499  0.17660991 -0.28901922 -1.49289882\n",
      " -0.06648715 -1.49529281  2.91257197  0.93295689 -1.49898993  0.5995121\n",
      " -0.90464614 -1.29692469  0.32062495  0.32350091  0.31305526  0.35308681\n",
      "  0.35437797  0.42035527  0.50001181  0.62443249  0.82425558  1.00318886]\n",
      "Step: 147, State history: [ 0.22368279  0.0348509   0.01434138  0.10206245 -0.16741231 -3.0811271\n",
      "  0.00744233 -1.55237371  0.56196563  0.64974018 -0.58469444  0.74151593\n",
      "  1.4575276  -0.25751171  0.32716048  0.31720605  0.33592379  0.37340481\n",
      "  0.39606575  0.4751245   0.52055387  0.68192874  0.87076876  1.00203409]\n",
      "Step: 148, State history: [ 0.28573374  0.04132155  0.01987913  0.02448314 -0.55140637 -2.46562614\n",
      "  0.01601781 -1.30625665 -0.37148838  0.77260489  0.06699704  0.88463988\n",
      " -0.06874021  0.25400575  0.31031338  0.3403829   0.3378569   0.35591266\n",
      "  0.38912916  0.45705431  0.5121052   0.65757834  0.87077819  1.00006866]\n",
      "Step: 149, State history: [ 0.33583184  0.07577637  0.03064979  0.02232525 -0.64428586 -1.20271891\n",
      "  0.18933882 -0.94914162 -1.07585752  0.70733028 -0.892466    0.48686392\n",
      " -1.11792515  3.05448873  0.32389276  0.3426506   0.34082383  0.38249866\n",
      "  0.39606708  0.43172019  0.53163966  0.63669099  0.87157079  0.99960086]\n",
      "Step: 150, State history: [ 0.38020898  0.09134662 -0.02616773 -0.03157386 -0.58572474 -1.20890912\n",
      "  0.06788232  0.52753348  0.1251137   0.56951808 -0.79266189  0.43161444\n",
      "  0.77537887  2.48506921  0.32455078  0.33504526  0.33852938  0.35938786\n",
      "  0.40437691  0.44925492  0.53400706  0.67602814  0.871196    1.00202101]\n",
      "Step: 151, State history: [ 0.37658901  0.0319321  -0.01958951 -0.10431676 -0.64570018 -0.76883634\n",
      " -0.20793354 -2.06583033 -0.5364057   0.65876523  0.5920059   0.55643224\n",
      " -1.1918145  -0.56667784  0.32869846  0.33614455  0.33395727  0.37455146\n",
      "  0.39328537  0.45166654  0.54417585  0.65373667  0.87946744  1.00093155]\n",
      "Step: 152, State history: [ 0.44447572  0.06906742 -0.01687681 -0.07983477 -0.65029497 -0.9635578\n",
      " -0.34552179 -1.2767644  -0.52321246  0.62731859 -0.08816787  0.73851022\n",
      "  1.16057171  0.86835619  0.32483472  0.33969513  0.33899828  0.36162652\n",
      "  0.39428755  0.44935651  0.52897617  0.6552271   0.86141503  1.0003739 ]\n",
      "Step: 153, State history: [ 0.51161818  0.0942731  -0.00496432 -0.15056558 -0.75747135 -1.36515089\n",
      " -0.50433768 -0.19005837 -0.11018309  0.61055582 -0.24841924  0.59600199\n",
      " -0.07476284  0.54162909  0.31789666  0.3254636   0.33700324  0.36317694\n",
      "  0.39464278  0.44273786  0.51341933  0.63340568  0.86756386  1.000408  ]\n",
      "Step: 154, State history: [ 0.44666261  0.04778674 -0.01310595 -0.18633213 -0.81963537 -0.97891463\n",
      "  0.04398302  2.23088214  0.52803151  0.6919319   1.22213341  0.63700389\n",
      "  0.86977584  0.42752364  0.30507792  0.31461359  0.31988738  0.34727491\n",
      "  0.3725501   0.43536179  0.50255001  0.62410917  0.83726252  0.99965361]\n",
      "Step: 155, State history: [ 0.52128549  0.04717277 -0.03225443 -0.21393088 -0.80994759 -0.68389843\n",
      " -0.37439642 -1.34243643  0.32219632  0.81819307  1.36510292  0.68751368\n",
      " -0.74874544 -0.48498702  0.2944308   0.29824385  0.31617577  0.33147925\n",
      "  0.35819969  0.41842665  0.47926931  0.60490275  0.8343403   0.99750411]\n",
      "Step: 156, State history: [ 0.54275001  0.05962518  0.03940124 -0.18291996 -0.92691851 -0.94005491\n",
      " -0.44602534 -1.19996867 -0.28268284  0.95418792  1.65565895  0.38508137\n",
      " -2.06294747 -0.18249946  0.28850093  0.29474094  0.30009958  0.31351018\n",
      "  0.35019349  0.39660295  0.46795973  0.59094669  0.79572812  1.00036557]\n",
      "Step: 157, State history: [ 0.49526074  0.04813056  0.01605852 -0.13740968 -0.93022697 -0.44290904\n",
      " -0.38348323  0.34956632  0.01312626  0.99719882  0.78976532  0.45317446\n",
      " -1.1069715   0.24542746  0.2840414   0.28865097  0.29195824  0.31206489\n",
      "  0.34533808  0.38580828  0.45427271  0.56380884  0.77540442  1.00238718]\n",
      "Step: 158, State history: [ 0.51110637  0.0059837   0.02374863 -0.11237263 -0.91553272 -0.01367767\n",
      " -0.32201456  0.19014973  0.16524207  1.00576433  0.49329938  0.44995978\n",
      "  0.02223239 -0.21676252  0.28274578  0.28142871  0.29084555  0.30876905\n",
      "  0.33747683  0.37612435  0.45138421  0.5582363   0.78155424  1.00128409]\n",
      "Step: 159, State history: [ 0.54768531 -0.03573252  0.0774197  -0.11750292 -0.9347061  -0.47214439\n",
      " -0.34973808  0.44181859 -0.52843391  1.04043098  0.19851818  0.66107767\n",
      "  2.54315304  0.22357042  0.26974185  0.27616067  0.28326393  0.30462832\n",
      "  0.33206742  0.37632273  0.44239463  0.54560949  0.74756187  1.00225037]\n",
      "-17.39538612521439\n"
     ]
    }
   ],
   "source": [
    "obs, _ = sim_env.reset()\n",
    "total_reward = 0\n",
    "for _ in range(200):\n",
    "    action = sim_env.action_space.sample()\n",
    "    obs, reward, done, done, info = sim_env.step(action)\n",
    "    total_reward += reward\n",
    "    sim_env.render(\"human\")\n",
    "    if done:\n",
    "        break\n",
    "sim_env.close()\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -67.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 106      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 49.6       |\n",
      "|    ep_rew_mean          | -47.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 105        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 38         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07046617 |\n",
      "|    clip_fraction        | 0.00425    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -5.67      |\n",
      "|    explained_variance   | -0.00175   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.09e+03   |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0218    |\n",
      "|    std                  | 0.997      |\n",
      "|    value_loss           | 4.98e+03   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-116.12 +/- 25.42\n",
      "Episode length: 499.38 +/- 669.81\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 499        |\n",
      "|    mean_reward          | -116       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10195261 |\n",
      "|    clip_fraction        | 0.0199     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -5.68      |\n",
      "|    explained_variance   | 0.0117     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 104        |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0338    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 445        |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.6     |\n",
      "|    ep_rew_mean     | -24.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 98       |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 49.7        |\n",
      "|    ep_rew_mean          | -26.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.085145846 |\n",
      "|    clip_fraction        | 0.0147      |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -5.67       |\n",
      "|    explained_variance   | 0.0623      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 61.3        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 272         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-117.69 +/- 27.71\n",
      "Episode length: 440.86 +/- 605.68\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 441        |\n",
      "|    mean_reward          | -118       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06295036 |\n",
      "|    clip_fraction        | 0.00903    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -5.64      |\n",
      "|    explained_variance   | 0.0486     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 110        |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0221    |\n",
      "|    std                  | 0.988      |\n",
      "|    value_loss           | 667        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.3     |\n",
      "|    ep_rew_mean     | -26.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 96       |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 106      |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 49.5       |\n",
      "|    ep_rew_mean          | -34.6      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 96         |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 127        |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09163659 |\n",
      "|    clip_fraction        | 0.0276     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -5.61      |\n",
      "|    explained_variance   | 0.114      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 49.5       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0253    |\n",
      "|    std                  | 0.981      |\n",
      "|    value_loss           | 409        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | -33.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 149         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043841075 |\n",
      "|    clip_fraction        | 0.00181     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -5.6        |\n",
      "|    explained_variance   | 0.0577      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 332         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    std                  | 0.982       |\n",
      "|    value_loss           | 2.79e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-122.50 +/- 25.45\n",
      "Episode length: 666.68 +/- 708.48\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 667        |\n",
      "|    mean_reward          | -122       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 15000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09281102 |\n",
      "|    clip_fraction        | 0.0149     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -5.6       |\n",
      "|    explained_variance   | 0.217      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 89.2       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0223    |\n",
      "|    std                  | 0.976      |\n",
      "|    value_loss           | 167        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.1     |\n",
      "|    ep_rew_mean     | -24.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 93       |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 175      |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 49.1       |\n",
      "|    ep_rew_mean          | -15.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 94         |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 195        |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12817463 |\n",
      "|    clip_fraction        | 0.0258     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -5.56      |\n",
      "|    explained_variance   | 0.239      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 180        |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0387    |\n",
      "|    std                  | 0.963      |\n",
      "|    value_loss           | 330        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-165.60 +/- 18.18\n",
      "Episode length: 573.62 +/- 197.83\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 574        |\n",
      "|    mean_reward          | -166       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.52877367 |\n",
      "|    clip_fraction        | 0.0673     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -5.47      |\n",
      "|    explained_variance   | 0.418      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.01       |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0558    |\n",
      "|    std                  | 0.933      |\n",
      "|    value_loss           | 15.2       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.1     |\n",
      "|    ep_rew_mean     | -12.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 92       |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 220      |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 50         |\n",
      "|    ep_rew_mean          | -13.2      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 93         |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 240        |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18199992 |\n",
      "|    clip_fraction        | 0.0419     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -5.35      |\n",
      "|    explained_variance   | 0.185      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 107        |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0423    |\n",
      "|    std                  | 0.911      |\n",
      "|    value_loss           | 102        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | -20.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 260         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104684584 |\n",
      "|    clip_fraction        | 0.0222      |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -5.27       |\n",
      "|    explained_variance   | 0.205       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.35e+03    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    std                  | 0.898       |\n",
      "|    value_loss           | 683         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-189.97 +/- 18.99\n",
      "Episode length: 851.76 +/- 237.68\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 852        |\n",
      "|    mean_reward          | -190       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 25000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02656349 |\n",
      "|    clip_fraction        | 0.00151    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -5.24      |\n",
      "|    explained_variance   | 0.0847     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 14.4       |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    std                  | 0.897      |\n",
      "|    value_loss           | 2.19e+03   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49       |\n",
      "|    ep_rew_mean     | -24.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 92       |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 288      |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 48.3       |\n",
      "|    ep_rew_mean          | -11.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 92         |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 309        |\n",
      "|    total_timesteps      | 28672      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07899065 |\n",
      "|    clip_fraction        | 0.0119     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -5.24      |\n",
      "|    explained_variance   | 0.392      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 12.8       |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0177    |\n",
      "|    std                  | 0.896      |\n",
      "|    value_loss           | 166        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-180.17 +/- 14.41\n",
      "Episode length: 676.14 +/- 167.48\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 676        |\n",
      "|    mean_reward          | -180       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 30000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12925121 |\n",
      "|    clip_fraction        | 0.0413     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -5.2       |\n",
      "|    explained_variance   | 0.147      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.861      |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0463    |\n",
      "|    std                  | 0.874      |\n",
      "|    value_loss           | 145        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.7     |\n",
      "|    ep_rew_mean     | -17.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 91       |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 337      |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 49.2       |\n",
      "|    ep_rew_mean          | -11.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 91         |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 358        |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10056882 |\n",
      "|    clip_fraction        | 0.01       |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -5.13      |\n",
      "|    explained_variance   | 0.251      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 824        |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.871      |\n",
      "|    value_loss           | 900        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 50         |\n",
      "|    ep_rew_mean          | -10.1      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 91         |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 378        |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17169738 |\n",
      "|    clip_fraction        | 0.0347     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -5.09      |\n",
      "|    explained_variance   | 0.27       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.449      |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0503    |\n",
      "|    std                  | 0.857      |\n",
      "|    value_loss           | 7.61       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-221.39 +/- 41.14\n",
      "Episode length: 1289.80 +/- 334.76\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.29e+03   |\n",
      "|    mean_reward          | -221       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 35000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15335387 |\n",
      "|    clip_fraction        | 0.048      |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -4.97      |\n",
      "|    explained_variance   | 0.19       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.382      |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0305    |\n",
      "|    std                  | 0.824      |\n",
      "|    value_loss           | 19.7       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -6.39    |\n",
      "| time/              |          |\n",
      "|    fps             | 89       |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 412      |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 50         |\n",
      "|    ep_rew_mean          | -6.27      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 89         |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 434        |\n",
      "|    total_timesteps      | 38912      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02790216 |\n",
      "|    clip_fraction        | 0.00195    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -4.9       |\n",
      "|    explained_variance   | 0.333      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 15.3       |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    std                  | 0.824      |\n",
      "|    value_loss           | 93.5       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-170.00 +/- 29.27\n",
      "Episode length: 1444.10 +/- 443.62\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.44e+03   |\n",
      "|    mean_reward          | -170       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 40000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.21239135 |\n",
      "|    clip_fraction        | 0.0459     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -4.86      |\n",
      "|    explained_variance   | -0.721     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.409      |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0386    |\n",
      "|    std                  | 0.806      |\n",
      "|    value_loss           | 1.19       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -2.45    |\n",
      "| time/              |          |\n",
      "|    fps             | 87       |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 468      |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 50         |\n",
      "|    ep_rew_mean          | -3.44      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 489        |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06454109 |\n",
      "|    clip_fraction        | 0.0147     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -4.8       |\n",
      "|    explained_variance   | 0.193      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.615      |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0204    |\n",
      "|    std                  | 0.802      |\n",
      "|    value_loss           | 52.3       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-122.50 +/- 27.81\n",
      "Episode length: 269.98 +/- 458.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 270         |\n",
      "|    mean_reward          | -123        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027807793 |\n",
      "|    clip_fraction        | 0.000781    |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -4.79       |\n",
      "|    explained_variance   | 0.327       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 78.1        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00778    |\n",
      "|    std                  | 0.803       |\n",
      "|    value_loss           | 67          |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.1     |\n",
      "|    ep_rew_mean     | -5.66    |\n",
      "| time/              |          |\n",
      "|    fps             | 87       |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 512      |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 49.1       |\n",
      "|    ep_rew_mean          | -2.98      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 88         |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 533        |\n",
      "|    total_timesteps      | 47104      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.21370162 |\n",
      "|    clip_fraction        | 0.0427     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -4.76      |\n",
      "|    explained_variance   | 0.559      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.628      |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0294    |\n",
      "|    std                  | 0.789      |\n",
      "|    value_loss           | 17.1       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 49.6       |\n",
      "|    ep_rew_mean          | -0.289     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 88         |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 554        |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.24809119 |\n",
      "|    clip_fraction        | 0.0428     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -4.67      |\n",
      "|    explained_variance   | -0.112     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.367      |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0453    |\n",
      "|    std                  | 0.764      |\n",
      "|    value_loss           | 1.24       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-134.02 +/- 22.57\n",
      "Episode length: 812.34 +/- 757.58\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 812       |\n",
      "|    mean_reward          | -134      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 50000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1312512 |\n",
      "|    clip_fraction        | 0.0305    |\n",
      "|    clip_range           | 0.999     |\n",
      "|    entropy_loss         | -4.56     |\n",
      "|    explained_variance   | 0.138     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.47      |\n",
      "|    n_updates            | 240       |\n",
      "|    policy_gradient_loss | -0.0329   |\n",
      "|    std                  | 0.751     |\n",
      "|    value_loss           | 4.31      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -0.945   |\n",
      "| time/              |          |\n",
      "|    fps             | 88       |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 581      |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | -0.117      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 602         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060692593 |\n",
      "|    clip_fraction        | 0.00562     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -4.53       |\n",
      "|    explained_variance   | 0.241       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.65        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    std                  | 0.751       |\n",
      "|    value_loss           | 184         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-164.22 +/- 8.23\n",
      "Episode length: 1569.06 +/- 216.58\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.57e+03   |\n",
      "|    mean_reward          | -164       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 55000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18835777 |\n",
      "|    clip_fraction        | 0.0427     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -4.5       |\n",
      "|    explained_variance   | 0.225      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.591      |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0408    |\n",
      "|    std                  | 0.74       |\n",
      "|    value_loss           | 1.49       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -6.62    |\n",
      "| time/              |          |\n",
      "|    fps             | 86       |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 636      |\n",
      "|    total_timesteps | 55296    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 49.5      |\n",
      "|    ep_rew_mean          | -7.53     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 87        |\n",
      "|    iterations           | 28        |\n",
      "|    time_elapsed         | 658       |\n",
      "|    total_timesteps      | 57344     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0873775 |\n",
      "|    clip_fraction        | 0.0124    |\n",
      "|    clip_range           | 0.999     |\n",
      "|    entropy_loss         | -4.46     |\n",
      "|    explained_variance   | 0.137     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.18      |\n",
      "|    n_updates            | 270       |\n",
      "|    policy_gradient_loss | -0.00805  |\n",
      "|    std                  | 0.737     |\n",
      "|    value_loss           | 1.04e+03  |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 49         |\n",
      "|    ep_rew_mean          | -19.6      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 29         |\n",
      "|    time_elapsed         | 681        |\n",
      "|    total_timesteps      | 59392      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13860361 |\n",
      "|    clip_fraction        | 0.0424     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -4.45      |\n",
      "|    explained_variance   | 0.263      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.02       |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0386    |\n",
      "|    std                  | 0.732      |\n",
      "|    value_loss           | 12.4       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-168.28 +/- 8.65\n",
      "Episode length: 1568.98 +/- 217.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.57e+03     |\n",
      "|    mean_reward          | -168         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029424059 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.999        |\n",
      "|    entropy_loss         | -4.43        |\n",
      "|    explained_variance   | 0.235        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.52e+03     |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00487     |\n",
      "|    std                  | 0.733        |\n",
      "|    value_loss           | 1.77e+03     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49       |\n",
      "|    ep_rew_mean     | -10.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 85       |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 715      |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 50        |\n",
      "|    ep_rew_mean          | -10       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 85        |\n",
      "|    iterations           | 31        |\n",
      "|    time_elapsed         | 740       |\n",
      "|    total_timesteps      | 63488     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5505862 |\n",
      "|    clip_fraction        | 0.067     |\n",
      "|    clip_range           | 0.999     |\n",
      "|    entropy_loss         | -4.41     |\n",
      "|    explained_variance   | -0.893    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.11      |\n",
      "|    n_updates            | 300       |\n",
      "|    policy_gradient_loss | -0.0508   |\n",
      "|    std                  | 0.723     |\n",
      "|    value_loss           | 3.18      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-196.37 +/- 40.86\n",
      "Episode length: 1571.44 +/- 69.52\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.57e+03   |\n",
      "|    mean_reward          | -196       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 65000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08987057 |\n",
      "|    clip_fraction        | 0.0138     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -4.38      |\n",
      "|    explained_variance   | 0.239      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 227        |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | -0.018     |\n",
      "|    std                  | 0.724      |\n",
      "|    value_loss           | 2.11e+03   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -5.71    |\n",
      "| time/              |          |\n",
      "|    fps             | 84       |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 775      |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 50         |\n",
      "|    ep_rew_mean          | -33        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 33         |\n",
      "|    time_elapsed         | 798        |\n",
      "|    total_timesteps      | 67584      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18945229 |\n",
      "|    clip_fraction        | 0.0336     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -4.35      |\n",
      "|    explained_variance   | -0.173     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.982      |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.0339    |\n",
      "|    std                  | 0.712      |\n",
      "|    value_loss           | 3.96       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 50         |\n",
      "|    ep_rew_mean          | -39.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 34         |\n",
      "|    time_elapsed         | 819        |\n",
      "|    total_timesteps      | 69632      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08316544 |\n",
      "|    clip_fraction        | 0.0135     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -4.32      |\n",
      "|    explained_variance   | 0.0335     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.3        |\n",
      "|    n_updates            | 330        |\n",
      "|    policy_gradient_loss | -0.00316   |\n",
      "|    std                  | 0.712      |\n",
      "|    value_loss           | 4.6e+04    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-171.25 +/- 2.71\n",
      "Episode length: 1600.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.6e+03     |\n",
      "|    mean_reward          | -171        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040312108 |\n",
      "|    clip_fraction        | 0.00498     |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -4.33       |\n",
      "|    explained_variance   | 0.337       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 985         |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    std                  | 0.715       |\n",
      "|    value_loss           | 1.15e+03    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -32.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 84       |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 852      |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | -33.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 871         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008467391 |\n",
      "|    clip_fraction        | 0.000781    |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -4.33       |\n",
      "|    explained_variance   | 0.139       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.51e+04    |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    std                  | 0.715       |\n",
      "|    value_loss           | 1.54e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-175.52 +/- 13.43\n",
      "Episode length: 1596.70 +/- 23.10\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.6e+03    |\n",
      "|    mean_reward          | -176       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 75000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15590727 |\n",
      "|    clip_fraction        | 0.0162     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -4.32      |\n",
      "|    explained_variance   | -0.213     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 60.5       |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.0309    |\n",
      "|    std                  | 0.711      |\n",
      "|    value_loss           | 158        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -23.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 83       |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 905      |\n",
      "|    total_timesteps | 75776    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 4.61        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 926         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.082039036 |\n",
      "|    clip_fraction        | 0.0184      |\n",
      "|    clip_range           | 0.999       |\n",
      "|    entropy_loss         | -4.3        |\n",
      "|    explained_variance   | -0.463      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0364     |\n",
      "|    std                  | 0.706       |\n",
      "|    value_loss           | 16.4        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 50         |\n",
      "|    ep_rew_mean          | 5.33       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 39         |\n",
      "|    time_elapsed         | 946        |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09824288 |\n",
      "|    clip_fraction        | 0.0317     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -4.25      |\n",
      "|    explained_variance   | -0.128     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.3        |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.049     |\n",
      "|    std                  | 0.691      |\n",
      "|    value_loss           | 6.16       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-169.75 +/- 1.92\n",
      "Episode length: 1600.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.6e+03    |\n",
      "|    mean_reward          | -170       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 80000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18521218 |\n",
      "|    clip_fraction        | 0.0401     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -4.16      |\n",
      "|    explained_variance   | 0.387      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.22       |\n",
      "|    n_updates            | 390        |\n",
      "|    policy_gradient_loss | -0.042     |\n",
      "|    std                  | 0.68       |\n",
      "|    value_loss           | 1.8        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 4.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 83       |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 979      |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 50         |\n",
      "|    ep_rew_mean          | 5.78       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 83         |\n",
      "|    iterations           | 41         |\n",
      "|    time_elapsed         | 1000       |\n",
      "|    total_timesteps      | 83968      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.22250612 |\n",
      "|    clip_fraction        | 0.0374     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -4.12      |\n",
      "|    explained_variance   | 0.336      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.8        |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0435    |\n",
      "|    std                  | 0.675      |\n",
      "|    value_loss           | 8.77       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-166.78 +/- 1.19\n",
      "Episode length: 1600.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.6e+03   |\n",
      "|    mean_reward          | -167      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 85000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2018319 |\n",
      "|    clip_fraction        | 0.0455    |\n",
      "|    clip_range           | 0.999     |\n",
      "|    entropy_loss         | -4.05     |\n",
      "|    explained_variance   | 0.152     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.15      |\n",
      "|    n_updates            | 410       |\n",
      "|    policy_gradient_loss | -0.0553   |\n",
      "|    std                  | 0.655     |\n",
      "|    value_loss           | 3.54      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.8     |\n",
      "|    ep_rew_mean     | 2.35     |\n",
      "| time/              |          |\n",
      "|    fps             | 83       |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 1034     |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 49.8       |\n",
      "|    ep_rew_mean          | 3.32       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 83         |\n",
      "|    iterations           | 43         |\n",
      "|    time_elapsed         | 1054       |\n",
      "|    total_timesteps      | 88064      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12769566 |\n",
      "|    clip_fraction        | 0.0314     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -3.97      |\n",
      "|    explained_variance   | 0.397      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 80.2       |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0337    |\n",
      "|    std                  | 0.652      |\n",
      "|    value_loss           | 161        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-167.14 +/- 1.19\n",
      "Episode length: 1600.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.6e+03    |\n",
      "|    mean_reward          | -167       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 90000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18638375 |\n",
      "|    clip_fraction        | 0.0502     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -3.92      |\n",
      "|    explained_variance   | 0.417      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.19       |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.0519    |\n",
      "|    std                  | 0.634      |\n",
      "|    value_loss           | 2.24       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.4     |\n",
      "|    ep_rew_mean     | 2.6      |\n",
      "| time/              |          |\n",
      "|    fps             | 82       |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 1087     |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 49.5       |\n",
      "|    ep_rew_mean          | 6.21       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 83         |\n",
      "|    iterations           | 45         |\n",
      "|    time_elapsed         | 1107       |\n",
      "|    total_timesteps      | 92160      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.19810611 |\n",
      "|    clip_fraction        | 0.0342     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -3.82      |\n",
      "|    explained_variance   | 0.381      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.05       |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.0364    |\n",
      "|    std                  | 0.624      |\n",
      "|    value_loss           | 10.5       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 49.5       |\n",
      "|    ep_rew_mean          | 6.44       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 83         |\n",
      "|    iterations           | 46         |\n",
      "|    time_elapsed         | 1126       |\n",
      "|    total_timesteps      | 94208      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18973759 |\n",
      "|    clip_fraction        | 0.0469     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -3.76      |\n",
      "|    explained_variance   | 0.573      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.16       |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | -0.0331    |\n",
      "|    std                  | 0.614      |\n",
      "|    value_loss           | 2.16       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=-164.63 +/- 1.65\n",
      "Episode length: 1600.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.6e+03    |\n",
      "|    mean_reward          | -165       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 95000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17147025 |\n",
      "|    clip_fraction        | 0.0389     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -3.68      |\n",
      "|    explained_variance   | 0.542      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.899      |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0378    |\n",
      "|    std                  | 0.601      |\n",
      "|    value_loss           | 2.33       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 7.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 83       |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 1159     |\n",
      "|    total_timesteps | 96256    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 50        |\n",
      "|    ep_rew_mean          | -6.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 83        |\n",
      "|    iterations           | 48        |\n",
      "|    time_elapsed         | 1180      |\n",
      "|    total_timesteps      | 98304     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2345202 |\n",
      "|    clip_fraction        | 0.035     |\n",
      "|    clip_range           | 0.999     |\n",
      "|    entropy_loss         | -3.61     |\n",
      "|    explained_variance   | 0.401     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.745     |\n",
      "|    n_updates            | 470       |\n",
      "|    policy_gradient_loss | -0.0488   |\n",
      "|    std                  | 0.59      |\n",
      "|    value_loss           | 2.26      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-167.89 +/- 1.56\n",
      "Episode length: 1600.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.6e+03    |\n",
      "|    mean_reward          | -168       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 100000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08542891 |\n",
      "|    clip_fraction        | 0.0295     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -3.56      |\n",
      "|    explained_variance   | 0.243      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.4e+03    |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.0178    |\n",
      "|    std                  | 0.589      |\n",
      "|    value_loss           | 2.98e+03   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -5.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 82       |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 1215     |\n",
      "|    total_timesteps | 100352   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 50        |\n",
      "|    ep_rew_mean          | -10.5     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 82        |\n",
      "|    iterations           | 50        |\n",
      "|    time_elapsed         | 1234      |\n",
      "|    total_timesteps      | 102400    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1801703 |\n",
      "|    clip_fraction        | 0.035     |\n",
      "|    clip_range           | 0.999     |\n",
      "|    entropy_loss         | -3.54     |\n",
      "|    explained_variance   | 0.503     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.53      |\n",
      "|    n_updates            | 490       |\n",
      "|    policy_gradient_loss | -0.0303   |\n",
      "|    std                  | 0.582     |\n",
      "|    value_loss           | 3.63      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 49.5       |\n",
      "|    ep_rew_mean          | 1.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 83         |\n",
      "|    iterations           | 51         |\n",
      "|    time_elapsed         | 1254       |\n",
      "|    total_timesteps      | 104448     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06917401 |\n",
      "|    clip_fraction        | 0.00723    |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -3.5       |\n",
      "|    explained_variance   | 0.341      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 276        |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0215    |\n",
      "|    std                  | 0.581      |\n",
      "|    value_loss           | 260        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=-168.67 +/- 3.09\n",
      "Episode length: 1600.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.6e+03   |\n",
      "|    mean_reward          | -169      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 105000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1784891 |\n",
      "|    clip_fraction        | 0.0448    |\n",
      "|    clip_range           | 0.999     |\n",
      "|    entropy_loss         | -3.45     |\n",
      "|    explained_variance   | 0.147     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.28      |\n",
      "|    n_updates            | 510       |\n",
      "|    policy_gradient_loss | -0.0425   |\n",
      "|    std                  | 0.566     |\n",
      "|    value_loss           | 3.54      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.5     |\n",
      "|    ep_rew_mean     | 6.51     |\n",
      "| time/              |          |\n",
      "|    fps             | 82       |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 1290     |\n",
      "|    total_timesteps | 106496   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 50         |\n",
      "|    ep_rew_mean          | 6.75       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 82         |\n",
      "|    iterations           | 53         |\n",
      "|    time_elapsed         | 1314       |\n",
      "|    total_timesteps      | 108544     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11085284 |\n",
      "|    clip_fraction        | 0.0368     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -3.38      |\n",
      "|    explained_variance   | 0.392      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.737      |\n",
      "|    n_updates            | 520        |\n",
      "|    policy_gradient_loss | -0.0354    |\n",
      "|    std                  | 0.562      |\n",
      "|    value_loss           | 10.7       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-166.59 +/- 2.10\n",
      "Episode length: 1600.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.6e+03    |\n",
      "|    mean_reward          | -167       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 110000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.25206408 |\n",
      "|    clip_fraction        | 0.0482     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -3.33      |\n",
      "|    explained_variance   | 0.428      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.69       |\n",
      "|    n_updates            | 530        |\n",
      "|    policy_gradient_loss | -0.0592    |\n",
      "|    std                  | 0.55       |\n",
      "|    value_loss           | 2.81       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 6.92     |\n",
      "| time/              |          |\n",
      "|    fps             | 81       |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 1348     |\n",
      "|    total_timesteps | 110592   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 49.5       |\n",
      "|    ep_rew_mean          | 7.67       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 82         |\n",
      "|    iterations           | 55         |\n",
      "|    time_elapsed         | 1371       |\n",
      "|    total_timesteps      | 112640     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16915141 |\n",
      "|    clip_fraction        | 0.0446     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -3.22      |\n",
      "|    explained_variance   | 0.558      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.942      |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0493    |\n",
      "|    std                  | 0.536      |\n",
      "|    value_loss           | 2.36       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 49.5       |\n",
      "|    ep_rew_mean          | 7.53       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 82         |\n",
      "|    iterations           | 56         |\n",
      "|    time_elapsed         | 1394       |\n",
      "|    total_timesteps      | 114688     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17238359 |\n",
      "|    clip_fraction        | 0.0424     |\n",
      "|    clip_range           | 0.999      |\n",
      "|    entropy_loss         | -3.15      |\n",
      "|    explained_variance   | 0.532      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.04       |\n",
      "|    n_updates            | 550        |\n",
      "|    policy_gradient_loss | -0.046     |\n",
      "|    std                  | 0.528      |\n",
      "|    value_loss           | 4.13       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 50\u001b[0m\n\u001b[1;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     42\u001b[0m             env\u001b[38;5;241m=\u001b[39mtrain_env,\n\u001b[1;32m     43\u001b[0m             n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m             clip_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.999\u001b[39m,\n\u001b[1;32m     47\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Train the model and use the evaluation callback to save the best model.\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:218\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 218\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:207\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 59\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m~/Documents/pi-optimal/pi_optimal/utils/gym_wrapper/model_based_env.py:114\u001b[0m, in \u001b[0;36mModelBasedEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Use a randomly selected model to predict the next full state.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m model_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels))\n\u001b[0;32m--> 114\u001b[0m next_state_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m next_state_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39minverse_transform_features(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m\"\u001b[39m, next_state_pred)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    116\u001b[0m reward \u001b[38;5;241m=\u001b[39m next_state_pred[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_idx]\n",
      "File \u001b[0;32m~/Documents/pi-optimal/pi_optimal/models/sklearn/base_sklearn_model.py:57\u001b[0m, in \u001b[0;36mBaseSklearnModel.forward\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action):\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input_data(state, action)\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/pi-optimal/pi_optimal/models/sklearn/base_sklearn_model.py:35\u001b[0m, in \u001b[0;36mBaseSklearnModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward_feature_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 35\u001b[0m         feature_next_state \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m         next_state_pred\u001b[38;5;241m.\u001b[39mappend(feature_next_state)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Convert predictions into a (n_samples, n_features_without_reward) array\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1626\u001b[0m, in \u001b[0;36mMLPRegressor.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1613\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict using the multi-layer perceptron model.\u001b[39;00m\n\u001b[1;32m   1614\u001b[0m \n\u001b[1;32m   1615\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;124;03m    The predicted values.\u001b[39;00m\n\u001b[1;32m   1624\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1625\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1626\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1630\u001b[0m, in \u001b[0;36mMLPRegressor._predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Private predict method with optional input validation\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1630\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_pass_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m y_pred\u001b[38;5;241m.\u001b[39mravel()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:212\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._forward_pass_fast\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    210\u001b[0m hidden_activation \u001b[38;5;241m=\u001b[39m ACTIVATIONS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation]\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers_ \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 212\u001b[0m     activation \u001b[38;5;241m=\u001b[39m \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoefs_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     activation \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercepts_[i]\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers_ \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/sklearn/utils/extmath.py:206\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     ret \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m@\u001b[39m b\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m--> 206\u001b[0m     \u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43missparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    210\u001b[0m ):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/scipy/sparse/_base.py:1400\u001b[0m, in \u001b[0;36missparse\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A namespace class to separate sparray from spmatrix\"\"\"\u001b[39;00m\n\u001b[1;32m   1397\u001b[0m sparray\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m _spbase\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\n\u001b[0;32m-> 1400\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21missparse\u001b[39m(x):\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Is `x` of a sparse array or sparse matrix type?\u001b[39;00m\n\u001b[1;32m   1402\u001b[0m \n\u001b[1;32m   1403\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _spbase)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import gymnasium\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "# Monkey-patch gym to include a __version__ attribute if it's missing.\n",
    "\n",
    "# Set up log folder for monitoring\n",
    "log_dir = \"./logs_dir/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create the training environment and wrap it with a Monitor to log rewards.\n",
    "train_env = sim_env\n",
    "train_env = Monitor(train_env, log_dir)\n",
    "\n",
    "# Create a separate evaluation environment.\n",
    "eval_env = gymnasium.make(\"BipedalWalker-v3\")\n",
    "eval_env = Monitor(eval_env, log_dir)\n",
    "\n",
    "# Set up the evaluation callback. This will evaluate the model every 5000 timesteps,\n",
    "# and save the model if it achieves a new best mean reward.\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=os.path.join(log_dir, 'best_model'),\n",
    "    log_path=log_dir,\n",
    "    eval_freq=5000,\n",
    "    n_eval_episodes=50,\n",
    "    deterministic=False,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "\n",
    "model = PPO(\"MlpPolicy\",\n",
    "            env=train_env,\n",
    "            n_steps=2048,\n",
    "            gamma=0.99,\n",
    "            n_epochs=10,\n",
    "            clip_range=0.999,\n",
    "            verbose=1)\n",
    "\n",
    "# Train the model and use the evaluation callback to save the best model.\n",
    "model.learn(total_timesteps=300000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = PPO.load(os.path.join(log_dir, 'best_model/best_model.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     13\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 14\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/gymnasium/core.py:332\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/gymnasium/wrappers/common.py:409\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is an intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     )\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/gymnasium/core.py:332\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/gymnasium/wrappers/common.py:303\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/gymnasium/envs/box2d/bipedal_walker.py:759\u001b[0m, in \u001b[0;36mBipedalWalker.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscroll \u001b[38;5;241m*\u001b[39m SCALE, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    758\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 759\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the trained agent by running one episode and rendering it.\n",
    "env = gymnasium.make(\"BipedalWalker\", render_mode=\"human\")   \n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "terminated = False\n",
    "total_reward = 0\n",
    "step = 0 \n",
    "while not (done or terminated):\n",
    "    # Predict the next action using the trained policy.\n",
    "    action, _ = best_model.predict(obs, deterministic=False)\n",
    "    obs, reward, done, terminated ,_ = env.step(action)\n",
    "    total_reward += reward\n",
    "    step += 1\n",
    "    env.render()\n",
    "\n",
    "env.close()\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: -164.47833408422713\n",
      "Total reward: -163.4879424148611\n",
      "Total reward: -162.96492318751058\n",
      "Total reward: -162.48233613970763\n",
      "Total reward: -164.40617770835726\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     obs, reward, done, terminated ,_ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     13\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m---> 14\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m all_rewards\u001b[38;5;241m.\u001b[39mappend(total_reward)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/gymnasium/core.py:332\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/gymnasium/wrappers/common.py:409\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is an intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     )\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/gymnasium/core.py:332\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/gymnasium/wrappers/common.py:303\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pi-optimal-RKvx2dB5-py3.10/lib/python3.10/site-packages/gymnasium/envs/box2d/bipedal_walker.py:763\u001b[0m, in \u001b[0;36mBipedalWalker.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[0;32m--> 763\u001b[0m         \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurfarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpixels3d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    764\u001b[0m     )[:, \u001b[38;5;241m-\u001b[39mVIEWPORT_W:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the trained agent by running one episode and rendering it.\n",
    "env = gymnasium.make(\"BipedalWalker\", render_mode=\"rgb_array\")  \n",
    "all_rewards = []\n",
    "for i in range(100):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    terminated = False\n",
    "    total_reward = 0\n",
    "    while not (done or terminated):\n",
    "        # Predict the next action using the trained policy.\n",
    "        action, _ = best_model.predict(obs, deterministic=False)\n",
    "        obs, reward, done, terminated ,_ = env.step(action)\n",
    "        total_reward += reward\n",
    "        env.render()\n",
    "    all_rewards.append(total_reward)\n",
    "    print(f\"Total reward: {total_reward}\")\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.18399900038778"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 1 model\n",
    "np.mean(all_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Level Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id='logger-container'>\n",
       "            <div class=\"logger-entry\" data-indent=\"0\" style=\"margin-left: 0px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">📝</span>\n",
       "                <span class=\"logger-message\">No eval environment provided, will not create eval callback</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"0\" style=\"margin-left: 0px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">📝</span>\n",
       "                <span class=\"logger-message\">Creating PPO model</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"0\" style=\"margin-left: 0px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">📝</span>\n",
       "                <span class=\"logger-message\">Training model</span>\n",
       "            </div>\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            /* Logger Container */\n",
       "            #logger-container {\n",
       "                max-height: 600px;\n",
       "                overflow-y: auto;\n",
       "                font-family: 'Segoe UI Emoji', 'Segoe UI Symbol', monospace;\n",
       "                padding: 5px;\n",
       "                background-color: #ffffff;\n",
       "                font-size: 0.9em; /* Increased font size */\n",
       "                position: relative;\n",
       "            }\n",
       "\n",
       "            /* Individual Log Entry */\n",
       "            .logger-entry {\n",
       "                display: flex;\n",
       "                align-items: center;\n",
       "                padding: 1px 5px;\n",
       "                margin-bottom: 4px;\n",
       "                background-color: #ffffff;\n",
       "                transition: background-color 0.2s, box-shadow 0.2s;\n",
       "                white-space: pre-wrap; /* Preserve whitespace for symbols */\n",
       "                position: relative;\n",
       "            }\n",
       "\n",
       "            .logger-entry:hover {\n",
       "                background-color: #f1f3f5;\n",
       "                box-shadow: 0 2px 6px rgba(0,0,0,0.1);\n",
       "            }\n",
       "\n",
       "        \n",
       "\n",
       "            /* Connector Line styling based on indent */\n",
       "            .logger-entry[data-indent=\"1\"]::before {\n",
       "                left: 0;\n",
       "            }\n",
       "            .logger-entry[data-indent=\"2\"]::before {\n",
       "                left: 0;\n",
       "            }\n",
       "            /* Add more as needed for higher indent levels */\n",
       "\n",
       "            /* Emoji */\n",
       "            .logger-emoji {\n",
       "                margin-right: 10px;\n",
       "                font-size: 1em; /* Larger emoji size */\n",
       "                flex-shrink: 0;\n",
       "                color: inherit; /* Inherit color from parent */\n",
       "            }\n",
       "\n",
       "            /* Message */\n",
       "            .logger-message {\n",
       "                flex-grow: 1;\n",
       "                color: #212529;\n",
       "                word-break: break-word;\n",
       "            }\n",
       "        </style>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 147       |\n",
      "|    ep_rew_mean     | -2.01e+06 |\n",
      "| time/              |           |\n",
      "|    fps             | 177       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 11        |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 161           |\n",
      "|    ep_rew_mean          | -1.2e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 167           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 24            |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2118835e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.3e+11       |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | 6.9e-06       |\n",
      "|    value_loss           | 1.17e+12      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 165           |\n",
      "|    ep_rew_mean          | -1.07e+07     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 168           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 36            |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4334801e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | -5.11e-05     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.73e+09      |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.000102     |\n",
      "|    value_loss           | 6.09e+09      |\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pi_optimal.planners.online_planner import OnlinePlanner\n",
    "from pi_optimal.utils.gym_wrapper.model_based_env import ModelBasedEnv\n",
    "\n",
    "sim_env = ModelBasedEnv(models=[nn_model, nn_model2, nn_model3], dataset=dataset_train, max_episode_steps=200)\n",
    "eval_env = gymnasium.make(\"LunarLander-v3\")\n",
    "\n",
    "online_planner = OnlinePlanner(env=sim_env, eval_env=None, train_params={\"total_timesteps\": 6000}, eval_params={\"n_eval_episodes\": 50, \"eval_freq\": 5000})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing Inference from a dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For details  look in to the predict function of the planer. It takes the last observation and uses it as observation. Ensure that the dataset is set to **is_inference == True** ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting steps: 100%|██████████| 50/50 [00:00<00:00, 5438.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id='logger-container'>\n",
       "            <div class=\"logger-entry\" data-indent=\"0\" style=\"margin-left: 0px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">⚙️</span>\n",
       "                <span class=\"logger-message\">Initializing new dataset...</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">📝</span>\n",
       "                <span class=\"logger-message\">Dataset has 50 rows and 13 columns.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">📝</span>\n",
       "                <span class=\"logger-message\">Dataset has 1 episodes.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">📝</span>\n",
       "                <span class=\"logger-message\">Dataset has 10 state features and 1 actions.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">📝</span>\n",
       "                <span class=\"logger-message\">Using processors provided in the dataset_configuration.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"1\" style=\"margin-left: 20px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">⚙️</span>\n",
       "                <span class=\"logger-message\">Transforming features...</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_0' using preprocessor 'StandardScaler() with mean 0.11 and std 0.33'.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_1' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_2' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_3' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_4' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_5' using preprocessor 'RobustScaler(quantile_range=(5.0, 95.0)) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_6' using preprocessor 'None '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'state_7' using preprocessor 'None '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'done' using preprocessor 'None '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed states feature 'reward' using preprocessor 'PowerTransformer() '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"2\" style=\"margin-left: 40px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✅</span>\n",
       "                <span class=\"logger-message\">Transformed actions feature 'action_0' using preprocessor 'OneHotEncoder(sparse_output=False) '.</span>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"logger-entry\" data-indent=\"0\" style=\"margin-left: 0px;\">\n",
       "                <span class=\"logger-emoji\" style=\"color: #0d6efd;\">✨</span>\n",
       "                <span class=\"logger-message\">Dataset was created successfully!</span>\n",
       "            </div>\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            /* Logger Container */\n",
       "            #logger-container {\n",
       "                max-height: 600px;\n",
       "                overflow-y: auto;\n",
       "                font-family: 'Segoe UI Emoji', 'Segoe UI Symbol', monospace;\n",
       "                padding: 5px;\n",
       "                background-color: #ffffff;\n",
       "                font-size: 0.9em; /* Increased font size */\n",
       "                position: relative;\n",
       "            }\n",
       "\n",
       "            /* Individual Log Entry */\n",
       "            .logger-entry {\n",
       "                display: flex;\n",
       "                align-items: center;\n",
       "                padding: 1px 5px;\n",
       "                margin-bottom: 4px;\n",
       "                background-color: #ffffff;\n",
       "                transition: background-color 0.2s, box-shadow 0.2s;\n",
       "                white-space: pre-wrap; /* Preserve whitespace for symbols */\n",
       "                position: relative;\n",
       "            }\n",
       "\n",
       "            .logger-entry:hover {\n",
       "                background-color: #f1f3f5;\n",
       "                box-shadow: 0 2px 6px rgba(0,0,0,0.1);\n",
       "            }\n",
       "\n",
       "        \n",
       "\n",
       "            /* Connector Line styling based on indent */\n",
       "            .logger-entry[data-indent=\"1\"]::before {\n",
       "                left: 0;\n",
       "            }\n",
       "            .logger-entry[data-indent=\"2\"]::before {\n",
       "                left: 0;\n",
       "            }\n",
       "            /* Add more as needed for higher indent levels */\n",
       "\n",
       "            /* Emoji */\n",
       "            .logger-emoji {\n",
       "                margin-right: 10px;\n",
       "                font-size: 1em; /* Larger emoji size */\n",
       "                flex-shrink: 0;\n",
       "                color: inherit; /* Inherit color from parent */\n",
       "            }\n",
       "\n",
       "            /* Message */\n",
       "            .logger-message {\n",
       "                flex-grow: 1;\n",
       "                color: #212529;\n",
       "                word-break: break-word;\n",
       "            }\n",
       "        </style>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_inf = data_collector.collect(n_steps=50, max_steps_per_episode=200, env_seed=None, action_seed=None)\n",
    "\n",
    "dataset_inf = dataset_test = TimeseriesDataset(\n",
    "    df=df_inf,\n",
    "    dataset_config=dataset_config,\n",
    "    train_processors=False,\n",
    "    is_inference=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_planner.plan(dataset_inf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pi-optimal-RKvx2dB5-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
