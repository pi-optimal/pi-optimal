{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Building Temperature Control with `pi_optimal`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Efficient energy management is a key component of sustainable building operations, helping to reduce costs and minimize environmental impact. By leveraging **Reinforcement Learning (RL)**, we can optimize energy usage while maintaining occupant comfort. This notebook demonstrates the use of `pi_optimal` to train an RL agent for **temperature control optimization** in buildings, utilizing a dataset of historical energy consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Table of Contents\n",
    "\n",
    "1. [Optimizing Building Temperature Control with `pi_optimal`](#optimizing-building-temperature-control-with-pi_optimal)\n",
    "2. [Introduction](#introduction)\n",
    "3. [Problem Statement](#problem-statement)\n",
    "4. [Dataset](#dataset)\n",
    "    - [Dataset Features](#dataset-features)\n",
    "5. [Defining the Reward Function](#defining-the-reward-function)\n",
    "    - [Implementation](#implementation)\n",
    "    - [Apply the reward function to the dataset](#apply-the-reward-function-to-the-dataset)\n",
    "6. [Dataset Preparation](#dataset-preparation)\n",
    "    - [Configuration](#configuration)\n",
    "7. [Training the Agent](#training-the-agent)\n",
    "    - [Agent Initialization](#agent-initialization)\n",
    "    - [Training the Agent](#training-the-agent-1)\n",
    "8. [Evaluating and Predicting Actions](#evaluating-and-predicting-actions)\n",
    "    - [Load Current Data](#load-current-data)\n",
    "    - [Create Current Dataset](#create-current-dataset)\n",
    "    - [Predict Optimal Actions](#predict-optimal-actions)\n",
    "9. [Interpreting the Results](#interpreting-the-results)\n",
    "    - [Multi-Step Planning](#multi-step-planning)\n",
    "    - [Decision-Making Options](#decision-making-options)\n",
    "10. [Visualization](#visualization)\n",
    "11. [Conclusion](#conclusion)\n",
    "    - [Key Highlights](#key-highlights)\n",
    "12. [Next Steps](#next-steps)\n",
    "13. [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Our objective is to train an RL agent using `pi_optimal` to:\n",
    "\n",
    "- **Minimize energy consumption**: Optimize cooling intensity to reduce electricity usage.\n",
    "- **Maintain comfort**: Keep indoor temperature close to a desired level.\n",
    "\n",
    "The agent will learn from historical data to adjust cooling intensity, balancing these goals effectively.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset contains **energy consumption data** from multiple buildings, structured as follows:\n",
    "\n",
    "### Dataset Features\n",
    "\n",
    "1. **Simulation Details**:\n",
    "   - `episode`: Unique identifier for each simulation run.\n",
    "   - `step`: Time step during data collection.\n",
    "\n",
    "2. **Time Variables**:\n",
    "   - `hour`: Hour of the day.\n",
    "   - `day_type`: Day of the week (e.g., weekday or weekend).\n",
    "\n",
    "3. **Environmental Context**:\n",
    "   - `outdoor_dry_bulb_temperature`: Outdoor temperature (°C).\n",
    "   - `occupant_count`: Number of occupants in the building.\n",
    "\n",
    "4. **System State**:\n",
    "   - `indoor_dry_bulb_temperature`: Current indoor temperature (°C).\n",
    "   - `indoor_dry_bulb_temperature_cooling_set_point`: Desired indoor temperature (°C).\n",
    "\n",
    "5. **Control Action**:\n",
    "   - `cooling_device`: Intensity of the cooling system (0 = low, 1 = high).\n",
    "\n",
    "6. **Energy Metrics**:\n",
    "   - `net_electricity_consumption`: Electricity usage (kWh)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>step</th>\n",
       "      <th>day_type</th>\n",
       "      <th>hour</th>\n",
       "      <th>outdoor_dry_bulb_temperature</th>\n",
       "      <th>indoor_dry_bulb_temperature</th>\n",
       "      <th>indoor_dry_bulb_temperature_cooling_set_point</th>\n",
       "      <th>cooling_device</th>\n",
       "      <th>net_electricity_consumption</th>\n",
       "      <th>occupant_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>24.66</td>\n",
       "      <td>23.098652</td>\n",
       "      <td>23.222221</td>\n",
       "      <td>0.276068</td>\n",
       "      <td>0.677881</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>24.07</td>\n",
       "      <td>22.234743</td>\n",
       "      <td>22.222221</td>\n",
       "      <td>0.301041</td>\n",
       "      <td>0.846281</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>23.90</td>\n",
       "      <td>22.223060</td>\n",
       "      <td>22.222221</td>\n",
       "      <td>0.741433</td>\n",
       "      <td>5.384543</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>23.87</td>\n",
       "      <td>22.222250</td>\n",
       "      <td>22.222221</td>\n",
       "      <td>0.034795</td>\n",
       "      <td>1.809869</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>23.83</td>\n",
       "      <td>22.222237</td>\n",
       "      <td>22.222221</td>\n",
       "      <td>0.982480</td>\n",
       "      <td>-0.319520</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode  step  day_type  hour  outdoor_dry_bulb_temperature  \\\n",
       "0        0     0         5     1                         24.66   \n",
       "1        0     1         5     2                         24.07   \n",
       "2        0     2         5     3                         23.90   \n",
       "3        0     3         5     4                         23.87   \n",
       "4        0     4         5     5                         23.83   \n",
       "\n",
       "   indoor_dry_bulb_temperature  indoor_dry_bulb_temperature_cooling_set_point  \\\n",
       "0                    23.098652                                      23.222221   \n",
       "1                    22.234743                                      22.222221   \n",
       "2                    22.223060                                      22.222221   \n",
       "3                    22.222250                                      22.222221   \n",
       "4                    22.222237                                      22.222221   \n",
       "\n",
       "   cooling_device  net_electricity_consumption  occupant_count  \n",
       "0        0.276068                     0.677881             3.0  \n",
       "1        0.301041                     0.846281             3.0  \n",
       "2        0.741433                     5.384543             3.0  \n",
       "3        0.034795                     1.809869             3.0  \n",
       "4        0.982480                    -0.319520             3.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_historical_building_energy_consumption = pd.read_csv('data/historical_temperature_control.csv')\n",
    "df_historical_building_energy_consumption.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines for faster training and inference if you have sklearnex installed and are using an Intel CPU\n",
    "\n",
    "#import numpy as np\n",
    "#from sklearnex import patch_sklearn\n",
    "#patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the root path to the sys path to load pi_optimal from the parent directory\n",
    "import sys\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Defining the Reward Function\n",
    "\n",
    "The reward function balances two objectives:\n",
    "\n",
    "1. **Comfort**: Penalizing deviations from the desired temperature.\n",
    "2. **Cost**: Penalizing excessive energy consumption.\n",
    "\n",
    "$\\text{Reward} = - \\left( (\\text{Indoor Temperature} - \\text{Desired Temperature})^2 + \\text{Energy Consumption} \\cdot 0.001 \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired indoor temperature\n",
    "DESIRED_TEMP = 22  # Celsius\n",
    "\n",
    "# Function to calculate reward\n",
    "def calculate_reward(row):\n",
    "    # Temperature comfort penalty\n",
    "    temp_penalty = (row['indoor_dry_bulb_temperature'] - DESIRED_TEMP) ** 2\n",
    "    # Energy cost\n",
    "    energy_cost = row['net_electricity_consumption'] * 0.001\n",
    "    # Total penalty\n",
    "    total_penalty = temp_penalty + energy_cost\n",
    "    # Reward is the negative of the total penalty\n",
    "    reward = -total_penalty\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the reward function to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>step</th>\n",
       "      <th>day_type</th>\n",
       "      <th>hour</th>\n",
       "      <th>outdoor_dry_bulb_temperature</th>\n",
       "      <th>indoor_dry_bulb_temperature</th>\n",
       "      <th>indoor_dry_bulb_temperature_cooling_set_point</th>\n",
       "      <th>cooling_device</th>\n",
       "      <th>net_electricity_consumption</th>\n",
       "      <th>occupant_count</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>24.66</td>\n",
       "      <td>23.098652</td>\n",
       "      <td>23.222221</td>\n",
       "      <td>0.276068</td>\n",
       "      <td>0.677881</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.207714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>24.07</td>\n",
       "      <td>22.234743</td>\n",
       "      <td>22.222221</td>\n",
       "      <td>0.301041</td>\n",
       "      <td>0.846281</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.055951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>23.90</td>\n",
       "      <td>22.223060</td>\n",
       "      <td>22.222221</td>\n",
       "      <td>0.741433</td>\n",
       "      <td>5.384543</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.055140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>23.87</td>\n",
       "      <td>22.222250</td>\n",
       "      <td>22.222221</td>\n",
       "      <td>0.034795</td>\n",
       "      <td>1.809869</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.051205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>23.83</td>\n",
       "      <td>22.222237</td>\n",
       "      <td>22.222221</td>\n",
       "      <td>0.982480</td>\n",
       "      <td>-0.319520</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.049070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode  step  day_type  hour  outdoor_dry_bulb_temperature  \\\n",
       "0        0     0         5     1                         24.66   \n",
       "1        0     1         5     2                         24.07   \n",
       "2        0     2         5     3                         23.90   \n",
       "3        0     3         5     4                         23.87   \n",
       "4        0     4         5     5                         23.83   \n",
       "\n",
       "   indoor_dry_bulb_temperature  indoor_dry_bulb_temperature_cooling_set_point  \\\n",
       "0                    23.098652                                      23.222221   \n",
       "1                    22.234743                                      22.222221   \n",
       "2                    22.223060                                      22.222221   \n",
       "3                    22.222250                                      22.222221   \n",
       "4                    22.222237                                      22.222221   \n",
       "\n",
       "   cooling_device  net_electricity_consumption  occupant_count    reward  \n",
       "0        0.276068                     0.677881             3.0 -1.207714  \n",
       "1        0.301041                     0.846281             3.0 -0.055951  \n",
       "2        0.741433                     5.384543             3.0 -0.055140  \n",
       "3        0.034795                     1.809869             3.0 -0.051205  \n",
       "4        0.982480                    -0.319520             3.0 -0.049070  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the reward calculation\n",
    "df_historical_building_energy_consumption['reward'] = df_historical_building_energy_consumption.apply(calculate_reward, axis=1)\n",
    "df_historical_building_energy_consumption.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset Preparation\n",
    "\n",
    "To train a `pi_optimal` RL agent, we must first load and preprocess the building energy dataset. The `pi_optimal` package provides a custom dataset class that streamlines the preprocessing pipeline. Below are the key parameters that need to be defined during this process:\n",
    "\n",
    "- **Unit Index**:  \n",
    "   This parameter, `unit_index`, identifies distinct units in the dataset. In our case, each unit corresponds to a unique building (`episode` column).\n",
    "\n",
    "- **Time Column**:  \n",
    "   The time column (`timestep_column`) establishes the temporal sequence of data points, enabling the model to learn from historical trends. For instance, the RL agent can consider the previous 12 hours of data (set by the `lookback_timesteps` parameter) to make informed decisions.\n",
    "\n",
    "- **Reward Column**:  \n",
    "   The `reward_column` specifies the target that the agent seeks to optimize. Here, the dataset already includes a precomputed `reward` column, which reflects the balance between energy efficiency and occupant comfort.\n",
    "\n",
    "- **State Columns**:  \n",
    "   The state columns capture the system's current status, including variables that influence energy consumption and comfort levels. Relevant examples include:  \n",
    "   - `outdoor_dry_bulb_temperature`  \n",
    "   - `occupant_count`  \n",
    "   - `day_type`  \n",
    "\n",
    "   These features help the agent assess the current environment and predict outcomes effectively.\n",
    "\n",
    "- **Action Columns**:  \n",
    "   The action columns represent controllable variables, such as the intensity of the cooling device (`cooling_device`). While this example focuses on a single action, `pi_optimal` supports multiple simultaneous actions if needed.\n",
    "\n",
    "By carefully defining these parameters, we ensure that the RL agent can interpret the dataset's structure, learn from past patterns, and make optimized decisions to reduce energy usage while maintaining comfort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pi_optimal as po\n",
    "\n",
    "LOOKBACK_TIMESTEPS = 8\n",
    "historical_dataset = po.datasets.timeseries_dataset.TimeseriesDataset(df=df_historical_building_energy_consumption,\n",
    "                                                                        lookback_timesteps=LOOKBACK_TIMESTEPS,\n",
    "                                                                        unit_index='episode',\n",
    "                                                                        timestep_column='step',\n",
    "                                                                        reward_column='reward',\n",
    "                                                                        state_columns=['day_type', 'hour', 'outdoor_dry_bulb_temperature', 'indoor_dry_bulb_temperature','occupant_count', 'net_electricity_consumption', 'indoor_dry_bulb_temperature_cooling_set_point'],\n",
    "                                                                        action_columns=['cooling_device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training the Agent\n",
    "\n",
    "With the dataset prepared, we can initialize and train the RL agent using `pi_optimal`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pi_optimal.agents.agent import Agent\n",
    "\n",
    "# Initialize the agent with a name\n",
    "agent = Agent(name = 'temperature_control',              \n",
    "                type=\"mpc-continuous\") # MPC horizon of 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:02<00:00,  3.50it/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  4.29it/s]\n"
     ]
    }
   ],
   "source": [
    "agent.train(dataset=historical_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent for future use\n",
    "agent.save(path='agent_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Agent directory not found at: agent_dir/temperature",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example of loading the agent\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43magent_dir/temperature\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Stellwerk3GmbH/Work/pi_optimal/pi-optimal-api/pi-optimal/pi_optimal/agents/agent.py:182\u001b[0m, in \u001b[0;36mAgent.load\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    180\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load an agent from saved configuration.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m     \u001b[43mvalidate_agent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/agent_config.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    185\u001b[0m         config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Stellwerk3GmbH/Work/pi_optimal/pi-optimal-api/pi-optimal/pi_optimal/utils/validation.py:13\u001b[0m, in \u001b[0;36mvalidate_agent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Agent directory validation utilities\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent directory not found at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/agent_config.json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent configuration file not found at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/agent_config.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Agent directory not found at: agent_dir/temperature"
     ]
    }
   ],
   "source": [
    "# Example of loading the agent\n",
    "agent = agent.load(path='agent_dir/temperature_control')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluating and Predicting Actions\n",
    "\n",
    "After training the Reinforcement Learning (RL) agent, the next step is to evaluate its performance on new, unseen data. This involves loading the current building energy consumption data, preparing it using the same preprocessing pipeline as the historical dataset, and then using the trained agent to predict the optimal actions (i.e. in our case cooling intensity) to maximize energy savings and maintain an desiried temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Current Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pi_optimal as po\n",
    "\n",
    "# Load the current building energy consumption data\n",
    "df_current_building_energy_consumption = pd.read_csv('data/current_temperature_control.csv')\n",
    "\n",
    "# Apply the reward calculation\n",
    "df_current_building_energy_consumption[\"reward\"] = df_current_building_energy_consumption.apply(calculate_reward, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Current Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Episode column does not start from 0\n"
     ]
    }
   ],
   "source": [
    "current_dataset = po.datasets.timeseries_dataset.TimeseriesDataset(df=df_current_building_energy_consumption,\n",
    "                                                                   lookback_timesteps=LOOKBACK_TIMESTEPS,\n",
    "                                                                    dataset_config=agent.dataset_config,\n",
    "                                                                    train_processors=False,\n",
    "                                                                    is_inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Optimal Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:05<00:00,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, Top-100 Cost: 0.2477 (Cost: 0.5984, Uncertainty: 0.4016)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_actions = agent.predict(current_dataset, \n",
    "                             inverse_transform=True, \n",
    "                             horizon=24,\n",
    "                             n_iter=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interpreting the Results\n",
    "\n",
    "The agent provides a sequence of optimal actions (cooling intensities) for the time horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 0:\n",
      "Cooling device strength: 0.48\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 1:\n",
      "Cooling device strength: 0.56\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 2:\n",
      "Cooling device strength: 0.49\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 3:\n",
      "Cooling device strength: 0.51\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 4:\n",
      "Cooling device strength: 0.49\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 5:\n",
      "Cooling device strength: 0.47\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 6:\n",
      "Cooling device strength: 0.52\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 7:\n",
      "Cooling device strength: 0.43\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 8:\n",
      "Cooling device strength: 0.43\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 9:\n",
      "Cooling device strength: 0.37\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 10:\n",
      "Cooling device strength: 0.36\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 11:\n",
      "Cooling device strength: 0.42\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 12:\n",
      "Cooling device strength: 0.44\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 13:\n",
      "Cooling device strength: 0.37\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 14:\n",
      "Cooling device strength: 0.37\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 15:\n",
      "Cooling device strength: 0.43\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 16:\n",
      "Cooling device strength: 0.39\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 17:\n",
      "Cooling device strength: 0.4\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 18:\n",
      "Cooling device strength: 0.4\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 19:\n",
      "Cooling device strength: 0.4\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 20:\n",
      "Cooling device strength: 0.45\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 21:\n",
      "Cooling device strength: 0.43\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 22:\n",
      "Cooling device strength: 0.44\n",
      "\n",
      "--------------------\n",
      "\n",
      "Timestep 23:\n",
      "Cooling device strength: 0.49\n",
      "\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(best_actions)):\n",
    "    print(f\"Timestep {i}:\")\n",
    "    print(\"Cooling device strength:\", round(best_actions[i][0], 2))\n",
    "    print()\n",
    "    print(\"--------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Step Planning\n",
    "\n",
    "The agent optimizes actions by considering future outcomes over a multi-step horizon. This allows for efficient and forward-thinking decision-making.\n",
    "\n",
    "### Decision-Making Options\n",
    "\n",
    "1. **Full Application of Recommended Actions**: We could choose to apply all recommended actions immediately, adjusting the cooling intensity according to the agent's suggestions for the entire time horizon (e.g., the next 4 hours). This approach allows the building control system to operate based on the agent’s full plan.\n",
    "\n",
    "2. **Incremental Application**: Alternatively, we might apply only the first action in the sequence for the next hour and then the next hour re-run the agent to generate updated recommendations. This method provides flexibility by allowing adjustments based on real-time conditions, while still leveraging the agent’s ability to look multiple steps ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Visualization\n",
    "\n",
    "`pi_optimal` includes a **trajectory visualizer** for the simulated optimal trajectory. This tool allows you to explore the agent's recommendations and analyze their effects on energy consumption and indoor temperature over time. It provides valuable insights into the agent's behavior and helps evaluate its performance across various scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>step</th>\n",
       "      <th>day_type</th>\n",
       "      <th>hour</th>\n",
       "      <th>outdoor_dry_bulb_temperature</th>\n",
       "      <th>indoor_dry_bulb_temperature</th>\n",
       "      <th>indoor_dry_bulb_temperature_cooling_set_point</th>\n",
       "      <th>cooling_device</th>\n",
       "      <th>net_electricity_consumption</th>\n",
       "      <th>occupant_count</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>2</td>\n",
       "      <td>295</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>27.16</td>\n",
       "      <td>21.041910</td>\n",
       "      <td>24.444445</td>\n",
       "      <td>0.023414</td>\n",
       "      <td>1.580120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.919517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>2</td>\n",
       "      <td>296</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>28.80</td>\n",
       "      <td>22.458672</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.535330</td>\n",
       "      <td>-0.242441</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.210138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>2</td>\n",
       "      <td>297</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>30.34</td>\n",
       "      <td>20.361000</td>\n",
       "      <td>23.888890</td>\n",
       "      <td>0.708212</td>\n",
       "      <td>0.816046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.687137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>2</td>\n",
       "      <td>298</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>31.77</td>\n",
       "      <td>20.154018</td>\n",
       "      <td>23.888890</td>\n",
       "      <td>0.481191</td>\n",
       "      <td>1.811166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.409461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>2</td>\n",
       "      <td>299</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>33.12</td>\n",
       "      <td>20.939377</td>\n",
       "      <td>23.888890</td>\n",
       "      <td>0.180023</td>\n",
       "      <td>1.666699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.126588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     episode  step  day_type  hour  outdoor_dry_bulb_temperature  \\\n",
       "295        2   295         3     8                         27.16   \n",
       "296        2   296         3     9                         28.80   \n",
       "297        2   297         3    10                         30.34   \n",
       "298        2   298         3    11                         31.77   \n",
       "299        2   299         3    12                         33.12   \n",
       "\n",
       "     indoor_dry_bulb_temperature  \\\n",
       "295                    21.041910   \n",
       "296                    22.458672   \n",
       "297                    20.361000   \n",
       "298                    20.154018   \n",
       "299                    20.939377   \n",
       "\n",
       "     indoor_dry_bulb_temperature_cooling_set_point  cooling_device  \\\n",
       "295                                      24.444445        0.023414   \n",
       "296                                      24.000000        0.535330   \n",
       "297                                      23.888890        0.708212   \n",
       "298                                      23.888890        0.481191   \n",
       "299                                      23.888890        0.180023   \n",
       "\n",
       "     net_electricity_consumption  occupant_count    reward  \n",
       "295                     1.580120             0.0 -0.919517  \n",
       "296                    -0.242441             0.0 -0.210138  \n",
       "297                     0.816046             0.0 -2.687137  \n",
       "298                     1.811166             0.0 -3.409461  \n",
       "299                     1.666699             0.0 -1.126588  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_current_building_energy_consumption.iloc[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c027770beb3d4d55af35ba9a97a5e80a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Sheet(cells=(Cell(column_end=0, column_start=0, numeric_format='0.00', row_end=0, row_start=0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pi_optimal.utils.trajectory_visualizer import TrajectoryVisualizer\n",
    "\n",
    "trajectory_visualizer = TrajectoryVisualizer(agent, current_dataset, best_actions=best_actions)\n",
    "trajectory_visualizer.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates how `pi_optimal` can train an RL agent to optimize building energy consumption while maintaining indoor comfort. The agent efficiently balances **energy savings** and **occupant comfort**, making it a powerful tool for sustainable building management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Key Highlights\n",
    "\n",
    "- **Efficient Energy Management**: Significantly reduces electricity usage.\n",
    "- **Comfort Maintenance**: Keeps indoor temperatures close to desired levels.\n",
    "- **Scalable and Adaptive**: Can be applied to various buildings with minimal configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Fine-Tuning**:\n",
    "   - Adjust reward function weights.\n",
    "   - Experiment with different lookback horizons and agent types.\n",
    "\n",
    "2. **Deployment**:\n",
    "   - Integrate the RL agent into a real-time control system for live optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- `pi_optimal` Documentation: [GitHub](https://github.com/pi-optimal/pi_optimal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prophet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
